[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Human Element",
    "section": "",
    "text": "Preface\nThis is an initial placeholder for a 75,000 word book.\nCurrent word count = 15462\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The AI revolution has spawned two competing narratives, both fundamentally wrong. The doomsayers warn of widespread job displacement as artificial intelligence becomes increasingly capable of performing human tasks. The techno-utopians promise a future where AI solves humanity’s greatest challenges, freeing us from mundane work. But the reality emerging from actual AI implementations tells a different story – one where artificial intelligence enhances rather than replaces human capabilities.\nDrawing on our complementary backgrounds in investment analysis and technology implementation, we’ve observed a pattern across industries: the most successful AI applications are those that augment human judgment rather than attempt to replicate it. From financial trading desks to hospital diagnostic centers, from military command posts to creative studios, the winning formula consistently involves keeping humans “in the loop” while leveraging AI’s computational capabilities.\nThis insight shouldn’t surprise us. Previous technological revolutions followed similar paths. The ATM was supposed to eliminate bank tellers; instead, it changed their role and ultimately increased their numbers. Computer-aided design tools were expected to replace architects; instead, they enhanced architects’ ability to explore creative possibilities. The key difference today is the pace and scope of change that AI enables.\nWhat makes AI unique is its ability to process vast amounts of data and recognize patterns that humans might miss. But this capability, impressive as it is, remains fundamentally different from human intelligence. AI can analyze millions of medical images to flag potential anomalies, but it takes a doctor’s judgment to interpret these findings in the context of a patient’s overall health. AI can process thousands of financial data points per second, but it takes a human analyst to understand how changing geopolitical dynamics might affect market sentiment.\nThis book challenges both the fear-mongering and the hype around AI. Instead, we present a framework for understanding how AI can enhance human capabilities across industries. Drawing on real-world case studies and our own experience implementing AI solutions, we demonstrate why keeping humans central to decision-making leads to better outcomes than pursuing full automation. For business leaders, this book offers practical guidance on implementing AI in ways that augment rather than replace human workers. For investors, we provide frameworks for evaluating AI companies based on their approach to human-AI collaboration. For policymakers, we outline principles for governing AI development while preserving human agency and judgment.\nThe coming decades will see artificial intelligence transform every industry. But this transformation will not follow the simple pattern of automation and replacement that many predict. Instead, we are entering an era of enhancement, where human capabilities are amplified by AI rather than superseded by it. Understanding this distinction – and its implications for business strategy, investment decisions, and policy choices – will be crucial for navigating the AI revolution.\nThe future belongs not to those who try to replicate human intelligence, but to those who find ways to enhance it. This is the human element in the AI revolution.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Ch01.html",
    "href": "Ch01.html",
    "title": "1  The False Binary: Why AI Won’t Replace Human Work",
    "section": "",
    "text": "1.1 The Current Thing\nThe debate around artificial intelligence has fallen into a familiar trap: a false binary between utopian promises and dystopian warnings. On one side stand the techno-optimists, proclaiming an era where AI will solve humanity’s greatest challenges – from climate change to cancer. On the other, the doomsayers warn of widespread unemployment and social upheaval as machines surpass human capabilities. Both camps share a fundamental misconception about the nature of artificial intelligence and its relationship to human cognition.\nConsider the case of the automated teller machine (ATM). When banks began widespread deployment of ATMs in the 1970s, conventional wisdom held that bank teller jobs would soon disappear. The reality proved more complex. The number of bank tellers in the United States actually increased in the decades following ATM adoption. As routine cash transactions became automated, tellers evolved into relationship managers who could help customers with more complex financial needs. The ATM didn’t replace human workers – it enhanced their capabilities and transformed their roles.\nThis pattern has repeated across industries. Computer-aided design tools didn’t eliminate architects; they expanded the possibilities for architectural creativity. Spreadsheet software didn’t replace accountants; it freed them to focus on higher-level analysis and strategy. In each case, technology augmented rather than replaced human capabilities. Yet when we discuss artificial intelligence, we seem to forget these lessons.\nThe current wave of AI technology, particularly large language models like GPT-4, has reignited old debates about machine intelligence and human obsolescence. These models can engage in remarkably human-like conversation, generate creative content, and process vast amounts of information. Their capabilities have led some observers to predict the imminent automation of knowledge work, from legal research to financial analysis to creative writing. Others warn of the existential risks posed by increasingly capable AI systems.\nBut both perspectives miss a crucial insight: AI’s most impressive capabilities are fundamentally different from human intelligence. Consider an AI system trained to play chess. While it may achieve superhuman performance, it does so through brute-force calculation and pattern matching, not through the kind of strategic thinking that characterizes human chess masters. The AI doesn’t understand chess in any meaningful sense – it simply processes moves according to its training.\nThis distinction becomes even more apparent when we examine AI systems in real-world applications. Take the case of AI-assisted medical diagnosis. Modern AI systems can analyze medical images with remarkable accuracy, often spotting potential anomalies that human doctors might miss. Yet no responsible healthcare provider would suggest eliminating human doctors from the diagnostic process. The AI excels at pattern recognition, but it lacks the holistic understanding that allows doctors to interpret findings in the context of a patient’s overall health, lifestyle, and circumstances.\nThe key to understanding AI’s role lies in recognizing what makes human intelligence unique. Humans possess several capabilities that current AI systems cannot replicate: contextual understanding, common sense reasoning, and perhaps most importantly, the ability to ask meaningful questions. When a financial analyst evaluates a company, they don’t simply process historical data – they consider broader economic trends, assess management credibility, and imagine possible futures. These distinctly human capabilities remain essential even as AI tools become more sophisticated.\nMoreover, humans possess something that AI fundamentally lacks: agency and intentionality. We are beings-in-the-world, to borrow philosopher Martin Heidegger’s phrase, with our own goals, values, and desires. We don’t simply process information; we interpret it through the lens of our experiences and aspirations. This human element isn’t a bug to be eliminated – it’s a feature essential to meaningful decision-making.\nConsider the attempt to use AI to complete Beethoven’s unfinished 10th symphony. While the resulting composition contained all the technical elements of a Beethoven symphony, music critics found it lacking the spark of genius that characterizes the composer’s authentic works. The notes were correct, but the soul was missing. This illustrates a broader truth about AI: it can process and recombine existing patterns, but it cannot truly create in the way humans do.\nThe implications for business and society are profound. Rather than asking which jobs AI will eliminate, we should ask how AI can enhance human capabilities across industries. This requires moving beyond simplistic automation narratives to understand the unique strengths of both human and artificial intelligence.\nIn financial services, for example, AI systems can process vast amounts of market data and identify patterns that human traders might miss. But successful trading strategies still require human judgment to interpret these patterns in the context of broader economic and geopolitical developments. The most successful firms are those that have found ways to combine AI’s computational capabilities with human insight and intuition.\nSimilarly, in creative fields, AI tools can generate variations on existing patterns or help visualize concepts quickly. But compelling creative work still requires human vision and judgment. The rise of AI art tools hasn’t eliminated the need for human artists – it has given them new ways to express their creativity while highlighting the importance of authentic human expression.\nEven in fields where AI has made remarkable progress, such as language translation, the human element remains crucial. While AI can produce grammatically correct translations, human translators are still needed to capture nuance, cultural context, and intended meaning. The most effective approach combines AI’s speed and broad coverage with human judgment about what makes a translation truly effective.\nThis pattern – AI augmenting rather than replacing human capabilities – points to a future very different from both the utopian and dystopian visions currently dominating public discourse. Instead of a world where AI systems take over human tasks, we’re entering an era of enhancement, where human capabilities are amplified by artificial intelligence.\nThis future will require new ways of thinking about human-AI collaboration. Business leaders will need frameworks for identifying where AI can best enhance human capabilities. Workers will need to understand how to effectively partner with AI tools while developing the distinctly human skills that become more valuable as routine tasks are automated. Policymakers will need to govern AI development in ways that preserve human agency and judgment.\nThroughout this book, we’ll explore these themes through real-world examples and practical frameworks. We’ll examine successful implementations of AI across industries, always focusing on how they enhance rather than replace human capabilities. We’ll look at the technical limitations of current AI systems, not to minimize their impressive capabilities, but to understand where human judgment remains essential. And we’ll consider the broader implications for business strategy, investment decisions, and policy choices.\nThe central argument that will emerge is this: the future belongs not to artificial intelligence alone, but to enhanced human intelligence – the combination of AI’s computational capabilities with humanity’s unique capacity for judgment, creativity, and understanding. By keeping humans “in the loop” and focusing on enhancement rather than replacement, we can harness AI’s potential while preserving what makes us uniquely human.\nThis isn’t just a philosophical position – it’s a practical approach supported by evidence from actual AI implementations across industries. In the chapters that follow, we’ll explore this evidence in detail, providing concrete guidance for business leaders, investors, and policymakers navigating the AI revolution.\nThe rise of ChatGPT in late 2022 marked a watershed moment in public consciousness about artificial intelligence. Suddenly, everyone from CEOs to schoolchildren could experience AI’s capabilities firsthand. The technology felt magical – here was a computer that could write essays, debug code, and engage in seemingly intelligent conversation. Stock prices of AI-related companies soared, and predictions about AI’s impact became increasingly extreme. Some warned of mass unemployment as AI replaces human workers, while others promised a utopian future where artificial intelligence solves humanity’s greatest challenges.\nBut both narratives miss something fundamental about how AI actually works and how it’s being implemented in the real world. Consider what happened when a Fortune 500 consumer products company piloted Microsoft’s CoPilot, an AI assistant integrated into Office applications. The initial excitement was palpable – here was a tool that promised to automate email responses, summarize meetings, and help with presentations. Yet employees found themselves spending almost as much time editing and verifying the AI’s output as they would have spent writing from scratch. The AI’s responses were grammatically perfect but often missed crucial context or nuance that a human would naturally understand.\nThis pattern – AI as a powerful but ultimately limited assistant rather than a replacement – keeps repeating across industries. In finance, AI-powered research platforms emerged in 2023 promising to revolutionize investment analysis by automatically processing earnings reports, news flows, and market data. The output was impressive at first glance – comprehensive summaries, neat charts, and plausible-sounding recommendations. But experienced analysts quickly noticed something crucial: while the AI excelled at processing historical data, it struggled with the forward-looking analysis that gives investors an edge. It could tell you that a company’s margins had compressed but couldn’t meaningfully assess whether management’s turnaround strategy would work. It could flag that a competitor had entered a market but couldn’t evaluate the long-term competitive dynamics.\nThis limitation isn’t just a temporary technological hurdle – it’s fundamental to how current AI systems work. The large language models powering tools like ChatGPT excel at pattern recognition and synthesis of existing information. They can process and recombinate vast amounts of training data in sophisticated ways. But they lack “backtracking” capabilities – the ability to test hypotheses, revise assumptions, and iterate toward better solutions. When these systems generate text, they’re making a series of sophisticated statistical predictions about what words should come next, but they can’t “think ahead” or revise their approach based on where they’re going.\nThis technological constraint has profound implications for how AI will impact work. Rather than wholesale replacement of human workers, what we’re seeing is a shift in the nature of work itself. The key distinction is between knowing “how” to do something and knowing “what” to do in the first place. AI is becoming incredibly good at the “how” – the mechanical execution of tasks once you’ve specified what needs to be done. But humans remain essential for determining “what” needs to be done, why it matters, and whether the results make sense in a broader context.\nConsider software development. Tools like GitHub Copilot are remarkably good at generating code once you’ve specified what you want to build. But they can’t determine what features users actually need, how different components should work together, or whether a particular approach makes sense for the long-term maintainability of the system. This distinction between “what” and “how” helps explain why previous predictions about automation and job displacement have consistently been wrong. When ATMs were introduced, many predicted the end of bank tellers. Instead, banks opened more branches, and tellers shifted from counting cash to providing higher-value services like relationship management and problem-solving.\nThe investment implications of this pattern are significant. Many of today’s highest-flying AI companies are valued based on the assumption that they’ll eventually replace human workers entirely. But the biggest winners are likely to be companies that focus on augmenting human capabilities rather than replacing them. Take Starbucks, for example. Instead of trying to automate baristas out of existence, they’ve used AI to optimize store operations and inventory management, freeing up staff to focus on customer interaction and experience – the human elements that actually drive their business.\nThis pattern extends even to creative fields. When a team used AI to complete Beethoven’s unfinished 10th symphony, the technical achievement was impressive – the AI analyzed all of Beethoven’s previous works and generated music that superficially sounded similar. But music critics immediately noticed something was missing. The notes were there, but the spark of genius – the human element – was absent. As critic Jan Swafford noted, “We humans need to see the human doing it.” This applies not just to art but to business, healthcare, education, and virtually every field where AI is being deployed.\nThis brings us to the central insight of the enhancement revolution: AI’s impact on work will be determined not by what tasks it can technically perform, but by how it changes the value of different types of human capabilities. Tasks that primarily involve following predetermined procedures or processing large amounts of data will increasingly be handled by AI. But this will make uniquely human capabilities – judgment, creativity, emotional intelligence, and the ability to determine “what” needs to be done – more valuable, not less.\nThis insight has profound implications for how organizations should approach AI implementation. Rather than asking “What jobs can we automate?”, the better question is “How can we use AI to enhance our employees’ capabilities?” This shift in perspective leads to very different strategic choices and investment decisions.\nIn the following chapters, we’ll explore specific examples of successful enhancement-focused AI implementations across industries. We’ll examine why some approaches work better than others, and we’ll provide frameworks for business leaders and investors to evaluate AI opportunities through the enhancement lens. But the key takeaway is this: The AI revolution won’t be about replacement; it will be about enhancement. Understanding this distinction is crucial for anyone trying to navigate the profound changes that AI will bring to business and society.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The False Binary: Why AI Won't Replace Human Work</span>"
    ]
  },
  {
    "objectID": "Ch01.html#the-current-thing",
    "href": "Ch01.html#the-current-thing",
    "title": "1  The False Binary: Why AI Won’t Replace Human Work",
    "section": "",
    "text": "Abdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The False Binary: Why AI Won't Replace Human Work</span>"
    ]
  },
  {
    "objectID": "Ch02.html",
    "href": "Ch02.html",
    "title": "2  Inside the Black Box: Understanding What AI Actually Does",
    "section": "",
    "text": "2.1 About AI\nDemystifying AI’s real capabilities and limitations from an implementer’s perspective\nArtificial intelligence is a broad field which long-time researchers often jokingly define as “anything computers can’t do yet”. From early grammar checkers to chess to facial recognition, many features that are now routine were once considered AI. No doubt the same will eventually be said of the new generation of large language models (LLMs), the more precise term to describe the impressive new tools that include ChatGPT. Under the hood, LLMs are less magical and based on a straightforward application of an optimization algorithm called Generative Pre-trained Transformer (GPT) invented by Google researchers in 2017.\nYou can think of LLMs as a massively optimized and expanded version of the auto-complete feature your smartphone has featured for years. Instead of proposing the next word or two, LLMs can generate full sentences, paragraphs, books, and on and on without limit. Its power comes from the GPT optimization that lets it take advantage of the massively-parallel architecture of graphic processing units (GPUs). Just as a graphical image can be broken into smaller pixels, each manipulated in parallel, LLMs break text documents into characters (or “tokens”) that are processed simultaneously within the GPU.\nThe GPT algorithm has one critical limitation: once set in motion, it cannot backtrack. Humans plan ahead, weigh different scenarios, and can change their minds based on foreseen alternatives. GPTs can only fake this planning ability through their access to mountains of data where such alternatives have already been explored. GPTs cannot do Sudoku, or handle chess boards not covered in its training books. Similarly, although it may appear to evaluate potential investment scenarios, it is merely spitting out a long stream of text that it harvested from options that were already evaluated somewhere in the bowels of its (massive) training sets.\nIt’s important to keep this “one-way” fact in mind when using LLMs. Because they have no concept of imagining how a future situation might change current plans, it would be wise to take its predictions with caution.\nLLMs are models that compress all human knowledge – written, spoken, images, video – into a format that can generate similar-seeming content when given a starting prompt. Although the final models themselves are small enough to fit on a laptop or smartphone, they are created through a training process that consumes massive amounts of data — virtually everything on the public internet, plus collections of the text from millions of books, magazines, academic journals, patent filings, and anything else its creators can find.\nThanks to a clever, time-saving shortcut discovered in the 2017 GPT algorithm, key parts of the training happen in parallel, limited only by the number of GPUs available. It’s this optimization that explains the mad rush to buy GPUs, the chief beneficiary of which is Nvidia, thanks to its decades-long leadership in these fast processors. Although Nvidia chips were originally designed for fast graphics, their wide adoption means that many engineers are well-acquainted with CUDA, the low-level graphics programming software that powers Nvidia devices. When designing the various implementations of GPT, it was natural for developers to optimize for CUDA, further cementing Nvidia’s lead.\nOnce trained, the LLM is a statistical prediction engine that knows the most likely word, phrase, or paragraphs that follow any given input. It knows, for example, that the phrase “Mary had a little” is highly likely to be followed by “lamb” or even the entire phrase “Its fleece was white as snow”. It will apply the same statistical completion algorithm to any snippet of text, including those that look like questions, where the most likely “completion” is the answer to the question. For example, the statistically most likely way to complete the phrase “what is 1 + 1?” is “2”.\nThe final LLM consists of billions of “parameters”, finely-tuned statistical values created during the training process. But generating the response to your input requires similar levels of prodigious machine power. In fact, every character you type into the ChatGPT input box, as well as every character it types back, goes through many billions of computations. That slight delay you see as each character comes back at your terminal is not a clever UX effect intended to appear like a human is typing the answer. In fact, the characters come out slowly because of the untold levels of computing power required to generate each one of them. Multiply this by the many millions of simultaneous ChatGPT users and you can understand why state-of-the-art LLMs are phenomenally expensive to operate.\nAlthough these completions can be uncannily realistic, it’s important to keep in mind that it’s just auto-completion. Just as you would want to review an auto-complete suggestion before sending a reply on your smartphone, your ChatGPT answers require a similar level of skeptical scrutiny.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inside the Black Box: Understanding What AI Actually Does</span>"
    ]
  },
  {
    "objectID": "Ch02.html#how-does-an-llm-based-generative-system-work",
    "href": "Ch02.html#how-does-an-llm-based-generative-system-work",
    "title": "2  Inside the Black Box: Understanding What AI Actually Does",
    "section": "2.2 How does an LLM-based generative system work?",
    "text": "2.2 How does an LLM-based generative system work?\nImagine you have access to a zillion documents, preferably curated in some way reassures you about their quality and consistency. Wikipedia, for example, or maybe Reddit and other posts that have been sufficiently up-voted. Maybe you also have a corpus of published articles and books from trustworthy sources.\nIt would be straightforward to tag all words in these documents with labels like “noun”, “verb”, “proper noun”, etc. Of course there would be lots of tricky edge cases, but a generation of spelling and grammar-checkers makes the task doable.\nNow instead of organizing the dictionary by parts of speech, imagine your words are tagged semantically. A word like “queen”, for example, is broken into the labels “female” and “monarch”; change the label “female” to “male” and you have “king”. A word like “Starbucks” might include labels like “coffee”, as well as “retail store” or even “Fortune 500 business”. You can shift the meaning by changing the labels.\nGenerating a good semantic model like this would itself be a significant undertaking, but people have been working on this for a while, and various good “unsupervised” means have been developed that can do this fairly well. For example, one trick might be to assign labels based on the types of words nearby. The word “Starbucks” means “Fortune 500 Business” if you find it in a paragraph containing words like “earnings” or “CEO”; but it means “coffee” if you see words like “$4.95” or “latte”. This won’t be perfect, but you can imagine how it could get to be pretty good if you train on enough text.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inside the Black Box: Understanding What AI Actually Does</span>"
    ]
  },
  {
    "objectID": "Ch02.html#statistics-of-words",
    "href": "Ch02.html#statistics-of-words",
    "title": "2  Inside the Black Box: Understanding What AI Actually Does",
    "section": "2.3 Statistics of Words",
    "text": "2.3 Statistics of Words\nThis system works because words aren’t laid out randomly. Languages constrain the way words can follow one another. There are grammatical rules that determine word order, and there are additional semantic rules that further constrain which sentences make sense. “Colorless green ideas sleep furiously” is a grammatically valid sentence, but it makes no semantic sense and is extremely unlikely to occur. You and I know these rules because we’ve been living in the real world for many years. A computer can deduce most of these rules statistically as you give it more data.\nThe auto-completion feature, now ubiquitous in all word processors and mobile phones, is a simple application of the power of statistics combined with language. With any sized corpus, you’ll know with reasonable probability the likelihood that a particular word will follow another word. Interestingly, you can do this in any human language without even knowing about that language – the probabilities of word order come automatically from the sample sentences you have from that language.\nNow go a step above auto-completion and allow for completion at the sentence level, or even the paragraphs or chapters. Given a large enough corpus of quality sentences, you could probably guess with greater-than-chance probability the kinds of sentences and paragraphs that should follow a given set of sentences. Of course it won’t be perfect, but already you’d be getting an uncanny level of sophistication.\nPair this autocompletion capability with the work you’ve done with semantic labeling. And maybe go really big, and do this with even more meta-information you might have about each corpus. A Wikipedia entry, for example, knows that it’s about a person or a place. You know which entries link to one another. You know the same about Reddit, and about web pages. With enough training, you could probably get the computer to easily classify a given paragraph into various categories: this piece is fiction, that one is medical, here’s one that’s from a biography, etc., etc.\nOnce you have a model of relationships that can identify the type of content, you can go the other direction: given a few snippets of one known form of content (biography, medical, etc.), “auto-complete” with more content of the same kind.\nThis is an extremely simplified summary of what’s happening, but you can imagine how with some effort you could make this fairly sophisticated. In fact, at some level isn’t that what we humans are already doing. If your teacher or boss asks you to write a report about something, you are taking everything you’ve seen previously about the subject and generating more of it, preferably in a pattern that fits what the teacher or boss is expecting.\nSome people are very good at this: take what you heard from various other sources and summarize it into a new format.\n“List five things wrong with this business plan”, you don’t necessarily need to understand the contents. If you’re good enough at re-applying patterns you’ve seen from similar projects, you’ll instinctively throw out a few tropes that have worked for you in the past. “The plan doesn’t say enough about the competition”, “the sales projections don’t take X and Y into account”, “How can you be sure you’ll be able to hire the right people”. There are thousands, maybe hundreds of thousands of books and articles that include these patterns, so you can imagine that with a little tuning a computer could get to be pretty good too.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inside the Black Box: Understanding What AI Actually Does</span>"
    ]
  },
  {
    "objectID": "Ch02.html#fine-tuning-the-output",
    "href": "Ch02.html#fine-tuning-the-output",
    "title": "2  Inside the Black Box: Understanding What AI Actually Does",
    "section": "2.4 Fine-tuning the output",
    "text": "2.4 Fine-tuning the output\nSimple text-completion will only get you so far1. Usable systems need refinement to make them behave more in the way we expect.\nReinforcement learning works by applying a reward or penalty score to the output and then retraining recursively until the model improves to an acceptable level.\nReinforcement learning with human feedback (RLHF) takes this a step further by including humans in the reward formula. The system generates multiple versions of an answer and a human is asked to vote on the best one.\nReinforcement learning with AI feedback (RLAIF) tries to use the AI itself to provide the feedback\nsee Thomas Woodside and Helen Toner: How Developers Steer Language Model Outputs: Large Language Models Explained, Part 2 for a detailed but readable discussion.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inside the Black Box: Understanding What AI Actually Does</span>"
    ]
  },
  {
    "objectID": "Ch02.html#wisdom-of-the-crowds",
    "href": "Ch02.html#wisdom-of-the-crowds",
    "title": "2  Inside the Black Box: Understanding What AI Actually Does",
    "section": "2.5 Wisdom of the crowds",
    "text": "2.5 Wisdom of the crowds\nAn LLM is sampling from an unimaginably complex mathematical model of the distribution of human words – essentially a wisdom of crowds effect that distills the collective output of humanity in a statistical way.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inside the Black Box: Understanding What AI Actually Does</span>"
    ]
  },
  {
    "objectID": "Ch02.html#where-do-you-get-the-documents",
    "href": "Ch02.html#where-do-you-get-the-documents",
    "title": "2  Inside the Black Box: Understanding What AI Actually Does",
    "section": "2.6 Where do you get the documents",
    "text": "2.6 Where do you get the documents\nOpenAI gets its documents from more than 200 million documents, 93% of which are in English, that are selected to be representative of a broad space of human knowledge.\nOf course it starts with Wikipedia: almost 6 million articles.\nOne set of words comes from Common Crawl: a large, public-domain dataset of millions of web pages.\nAnother is a proprietary corpus called WebText2 of more than 8 million documents made by scraping particularly high-quality web documents, such as those that are highly-upranked on Reddit.\nTwo proprietary datasets, known as Books1 and Books2 contain tens of thousands of published books. These datasets include classic literature, such as works by Shakespeare, Jane Austen, and Charles Dickens, as well as modern works of fiction and non-fiction, such as the Harry Potter series, The Da Vinci Code, and The Hunger Games.2 There are also many other books on a variety of topics, including science, history, politics, and philosophy.\n\nAlso high on the list: b-ok.org No. 190, a notorious market for pirated e-books that has since been seized by the U.S. Justice Department. At least 27 other sites identified by the U.S. government as markets for piracy and counterfeits were present in the data set.\n\nWashington Post has an interactive graphic that digs into more detail. (Also discussed on HN)\nYes, they crawl me:\n\n\n\nblog.richardsprague.com tokens on Google’s C4 dataset\n\n\n\n\n\nrichardsprague.com tokens on Google’s C4 dataset\n\n\n\n\n\npsm.personalscience.com tokens on Google’s C4 dataset\n\n\nfrom NYTimes\n\n\n\nGPT-3 Data Sources",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inside the Black Box: Understanding What AI Actually Does</span>"
    ]
  },
  {
    "objectID": "Ch02.html#organizing-the-data",
    "href": "Ch02.html#organizing-the-data",
    "title": "2  Inside the Black Box: Understanding What AI Actually Does",
    "section": "2.7 Organizing the data",
    "text": "2.7 Organizing the data\nAt the core of a transformer model is the idea that many of the intellectual tasks we humans do involves taking one sequence of tokens – words, numbers, programming instructions, etc. – and converting them into another sequence. Translation from one language to another is the classic case, but the insight at the heart of ChatGPT is that question-answering is another example. My question is a sequence of words and symbols like punctuation or numbers. If you append my question to, say, all the words in that huge OpenAI dataset, then you can “answer” my question by rearranging it along with some of the words in the dataset.\nThe technique of rearranging one sequence into another is called Seq2Seq*.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inside the Black Box: Understanding What AI Actually Does</span>"
    ]
  },
  {
    "objectID": "Ch02.html#footnotes",
    "href": "Ch02.html#footnotes",
    "title": "2  Inside the Black Box: Understanding What AI Actually Does",
    "section": "",
    "text": "see Karpathy for examples↩︎\nsee Apr 2023 Chang et al. (2023a)↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Inside the Black Box: Understanding What AI Actually Does</span>"
    ]
  },
  {
    "objectID": "Ch03.html",
    "href": "Ch03.html",
    "title": "3  The Human Edge: What We Do That AI Can’t",
    "section": "",
    "text": "3.1 The Fundamental Divide\nIn the early days of the personal computer revolution, spreadsheet software transformed financial analysis. Critics warned that tools like VisiCalc and Lotus 1-2-3 would eliminate financial analysts by automating their calculations. Instead, these tools dramatically increased productivity while shifting analysts’ focus from mathematical computation to business insight. Today’s artificial intelligence is driving a similar transformation, but at a far greater scale and across virtually every knowledge-based profession.\nThe key to understanding AI’s impact on knowledge work lies in distinguishing between two fundamental aspects of any task: determining what needs to be done versus figuring out how to do it. This distinction, though simple, has profound implications for the future of work, business strategy, and investment.\nConsider a typical business task like creating a market analysis presentation. The “what” involves determining which market segments to analyze, which metrics matter most, and what strategic implications to draw from the data. The “how” involves gathering the data, creating charts, writing clear explanations, and formatting the presentation. Traditionally, both aspects required significant human effort. AI is rapidly changing this equation.\nLarge language models and other AI tools have become remarkably adept at the “how” - they can write coherent prose, generate professional visualizations, and format documents with impressive skill. What they cannot do is determine what analysis would be most valuable for a specific business situation. They cannot identify which insights would resonate with a particular audience or anticipate how findings might influence strategic decisions.\n[Data visualization suggestion: A 2x2 matrix showing tasks plotted on “What vs How” and “AI Capable vs Human Required” axes, with example tasks positioned in each quadrant]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Human Edge: What We Do That AI Can't</span>"
    ]
  },
  {
    "objectID": "Ch03.html#the-rise-of-what-skills",
    "href": "Ch03.html#the-rise-of-what-skills",
    "title": "3  The Human Edge: What We Do That AI Can’t",
    "section": "3.2 The Rise of “What” Skills",
    "text": "3.2 The Rise of “What” Skills\nThis fundamental divide is reshaping the value proposition of knowledge workers. Previously, career success often depended heavily on mastering “how” skills - becoming proficient with specific tools, frameworks, and methodologies. While these skills remain relevant, their relative importance is declining as AI becomes increasingly capable of handling implementation details.\nConsider three examples from different professions:\n\nInvestment Analysis: Traditional financial analysts spent countless hours gathering data, building spreadsheet models, and formatting reports. Today’s AI tools can handle much of this mechanical work. The key differentiator becomes the analyst’s ability to determine what to analyze - which metrics matter most for a particular investment thesis, which comparisons will be most illuminating, and what strategic implications to draw from the data.\nSoftware Development: As AI code generation tools become more sophisticated, the competitive advantage shifts from knowing how to write efficient code to knowing what to build. The critical skills become understanding user needs, identifying valuable features, and architecting systems that solve real business problems.\nMarketing: AI can now generate endless variations of ad copy, social media posts, and email campaigns. The key human contribution shifts from crafting individual messages to determining what message strategy will resonate with target audiences and align with business objectives.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Human Edge: What We Do That AI Can't</span>"
    ]
  },
  {
    "objectID": "Ch03.html#the-persistence-of-human-judgment",
    "href": "Ch03.html#the-persistence-of-human-judgment",
    "title": "3  The Human Edge: What We Do That AI Can’t",
    "section": "3.3 The Persistence of Human Judgment",
    "text": "3.3 The Persistence of Human Judgment\nThis shift toward “what” skills explains why AI enhances rather than replaces human knowledge workers. Determining what to do requires capabilities that remain uniquely human:\n\nContextual Understanding: Humans can interpret information within broader social, economic, and strategic contexts that AI systems struggle to grasp.\nStakeholder Empathy: We can anticipate how different audiences will respond emotionally and intellectually to various approaches.\nStrategic Synthesis: Humans excel at combining insights from disparate domains to identify novel opportunities and approaches.\nEthical Judgment: We can weigh complex moral considerations and social implications that AI systems cannot meaningfully evaluate.\n\n[Data visualization suggestion: A timeline showing the evolution of key professional skills from 1980-2030, with “how” skills declining in relative importance as AI capabilities increase]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Human Edge: What We Do That AI Can't</span>"
    ]
  },
  {
    "objectID": "Ch03.html#implications-for-organizations",
    "href": "Ch03.html#implications-for-organizations",
    "title": "3  The Human Edge: What We Do That AI Can’t",
    "section": "3.4 Implications for Organizations",
    "text": "3.4 Implications for Organizations\nThis what-how divide has profound implications for how organizations should approach AI implementation:\n\n3.4.1 Skill Development\nOrganizations need to shift training and development programs to emphasize “what” skills: - Strategic thinking and problem framing - Stakeholder needs analysis - Cross-domain synthesis - Ethical decision-making - AI prompt engineering and oversight\n\n\n3.4.2 Workflow Design\nBusiness processes should be redesigned to leverage the what-how divide: - Separate strategic decisions from implementation details - Create clear handoffs between human judgment and AI execution - Build feedback loops to validate and improve AI outputs - Maintain human oversight of critical decisions\n\n\n3.4.3 Team Structure\nTraditional hierarchies based on technical expertise may need to evolve: - Fewer pure technical specialists - More roles combining domain expertise with AI literacy - New positions focused on AI-human collaboration - Greater emphasis on cross-functional knowledge",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Human Edge: What We Do That AI Can't</span>"
    ]
  },
  {
    "objectID": "Ch03.html#investment-implications",
    "href": "Ch03.html#investment-implications",
    "title": "3  The Human Edge: What We Do That AI Can’t",
    "section": "3.5 Investment Implications",
    "text": "3.5 Investment Implications\nThe what-how divide offers a useful framework for evaluating AI-related investments:\n\n3.5.1 Winners\n\nCompanies that help humans become better at “what” decisions\nTools that seamlessly integrate human judgment with AI execution\nPlatforms that facilitate human-AI collaboration\nSolutions that augment rather than replace human expertise\n\n\n\n3.5.2 Losers\n\nCompanies focused solely on automating “how” tasks\nTools that attempt to eliminate human judgment\nSolutions that create black boxes resistant to human oversight\nPlatforms that fail to leverage unique human capabilities",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Human Edge: What We Do That AI Can't</span>"
    ]
  },
  {
    "objectID": "Ch03.html#looking-ahead",
    "href": "Ch03.html#looking-ahead",
    "title": "3  The Human Edge: What We Do That AI Can’t",
    "section": "3.6 Looking Ahead",
    "text": "3.6 Looking Ahead\nAs AI capabilities continue to advance, the what-how divide will likely deepen. More “how” tasks will become automated, increasing the premium on human judgment and strategic thinking. This shift has several important implications:\n\n3.6.1 Education\nTraditional education systems heavily emphasize “how” skills - teaching specific methodologies, tools, and techniques. These systems will need to evolve to place greater emphasis on developing students’ abilities to: - Frame problems effectively - Synthesize insights across domains - Make ethical judgments - Collaborate with AI systems\n\n\n3.6.2 Career Development\nIndividual knowledge workers will need to consciously shift their skill development toward “what” capabilities: - Deeper domain expertise - Broader cross-functional knowledge - Stronger strategic thinking - Better stakeholder understanding\n\n\n3.6.3 Business Strategy\nOrganizations will need to rethink their competitive advantages: - Less emphasis on operational excellence - Greater focus on strategic insight - Increased value of human judgment - New models of human-AI collaboration",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Human Edge: What We Do That AI Can't</span>"
    ]
  },
  {
    "objectID": "Ch03.html#conclusion",
    "href": "Ch03.html#conclusion",
    "title": "3  The Human Edge: What We Do That AI Can’t",
    "section": "3.7 Conclusion",
    "text": "3.7 Conclusion\nThe what-how divide offers a powerful framework for understanding AI’s impact on knowledge work. Rather than replacing human workers, AI is driving a shift in the nature of human contribution - from implementing solutions to determining what solutions are needed. Organizations and individuals that understand and adapt to this shift will be best positioned to thrive in an AI-enhanced future.\nThis transformation echoes previous technological revolutions, but with greater scope and speed. Just as spreadsheet software changed financial analysis without eliminating analysts, AI will transform knowledge work without eliminating knowledge workers. The key is understanding where human judgment truly adds value and designing systems that enhance rather than replace that judgment.\nThe future belongs not to those who master the “how” - AI will increasingly handle that - but to those who excel at determining “what” needs to be done. This shift represents both a challenge and an opportunity for organizations and individuals ready to adapt to a new paradigm of human-AI collaboration.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Human Edge: What We Do That AI Can't</span>"
    ]
  },
  {
    "objectID": "Ch04.html",
    "href": "Ch04.html",
    "title": "4  Beyond Computation: The Philosophy of Human Intelligence",
    "section": "",
    "text": "Previous chapters examined AI’s capabilities and limitations from technical and business perspectives. But to truly understand why human intelligence remains irreplaceable, we need to dig deeper into what makes human thinking unique. This takes us into philosophical territory that might seem abstract at first but has profound practical implications for business leaders and investors trying to navigate the AI revolution.\nWhile American and British philosophers have focused primarily on logic and language – which certainly matter for AI development – Continental philosophers, particularly Martin Heidegger, tackled more fundamental questions about what it means to think and exist. Their insights help explain why even our most advanced AI systems, despite impressive capabilities, still miss essential aspects of human intelligence.\nThe fundamental issue is that we’ve inherited a flawed model of human intelligence from Descartes and other early modern philosophers. They viewed humans as essentially thinking machines – hence “I think, therefore I am.” This same assumption underlies most AI development: if we can replicate human-like information processing, we’ll achieve human-like intelligence. But this gets things exactly backwards.\nWe’re not primarily thinking machines that sometimes act in the world. Instead, we’re fundamentally beings-in-the-world (Heidegger’s hyphenated term emphasizes this unity) who sometimes step back to think abstractly. This distinction has enormous implications for how we should think about AI and its limitations.\nConsider a skilled trader on a busy trading floor. When they’re “in the zone,” they’re not consciously thinking through each decision. They’re responding to market movements, news flows, and subtle signals from colleagues with an intuitive grasp that comes from years of embodied experience. Heidegger would say they’re exhibiting “ready-to-hand” engagement with their environment, not detached analytical thinking.\nThis is fundamentally different from how AI trading systems work. The AI processes data and applies algorithms, but it lacks what Heidegger calls “comportment” – that basic way of being oriented toward and engaged with the world that comes before any explicit thinking. This explains why pure algorithmic trading works well for certain types of high-frequency operations, but the most successful hedge funds still rely heavily on human judgment for their major positions. The humans aren’t necessarily “smarter” than the algorithms – they just engage with the market in a fundamentally different way.\nThis connects to another key Heideggerian insight: we’re temporal beings who inherently understand past, present, and future as a unified whole. When a skilled investor or business leader makes decisions, they’re not just processing current data – they’re drawing on their lived experience of the past and projecting possibilities into the future. AI systems, in contrast, can only process historical data and make statistical projections. They lack what Heidegger calls “temporality” – that basic human way of existing across time that makes genuine understanding possible.\nThis explains why many business leaders discover that their best human decision-makers aren’t just processing more data – they’re bringing something qualitatively different to the table. Humans don’t primarily understand things by building up from basic facts to complex conclusions. Instead, we always already have what Heidegger calls a “pre-understanding” – a practical grasp of how things work that comes from being embedded in a shared world of meaning.\nThink about how a seasoned executive “reads the room” in a crucial negotiation. They’re not just processing verbal statements and body language signals. They’re drawing on a lifetime of cultural and social understanding that no AI system can replicate because AIs lack what Heidegger calls “being-with” – that fundamental way humans share a meaningful world with others.\nThis explains something often observed in investment teams: Junior analysts may have impressive technical skills and can process more data than their senior colleagues. But the best senior investors have something that can’t be reduced to information processing – a kind of practical wisdom that comes from years of being immersed in markets and business.\nThese philosophical insights have practical implications for how businesses should implement AI. Consider three different business activities:\n\nProcessing insurance claims\nNegotiating a major acquisition\nDeveloping a new product strategy\n\nThe first task is mainly about following procedures and processing information – perfect for AI enhancement. The second requires deep cultural understanding and reading subtle human dynamics – AI can assist but human judgment remains essential. The third requires what Heidegger calls “projection” – understanding current possibilities in light of future potential. AI can provide data and analysis, but only humans can truly innovate because only humans exist temporally.\nThis pattern appears consistently in markets. Companies that try to completely automate complex human judgments often disappoint, while those that use AI to enhance human capabilities tend to succeed. It’s not about replacing human intelligence but augmenting it in ways that respect its unique character.\nThis suggests the current focus on making AI more “human-like” may be misguided. Instead of trying to replicate human intelligence, which is fundamentally embedded in being-in-the-world, we should focus on developing AI systems that complement human capabilities. Think about how a hammer extends human capabilities without trying to replicate the human arm. Similarly, AI should extend human intelligence without trying to replicate human understanding.\nFor investors, this means companies that understand these distinctions – between what AI can enhance and what remains irreducibly human – are more likely to successfully implement AI than those pursuing full automation of human judgment. It also suggests we need to rethink how we evaluate AI progress. Instead of asking whether AI can pass increasingly sophisticated Turing tests, we should ask how effectively it enhances distinctively human capabilities.\nThe goal shouldn’t be artificial general intelligence that replicates human thinking. Instead, we should aim for artificial specific intelligence that amplifies human judgment while respecting its unique character. This philosophical perspective helps explain why the most successful AI implementations are those that enhance rather than replace human judgment. They succeed not despite keeping humans in the loop, but because they maintain that crucial human element.\nThis brings us back to our core enhancement thesis. By understanding the fundamental differences between human and artificial intelligence, we can better appreciate why enhancement rather than replacement is the right goal. The future belongs not to pure AI systems, but to human-AI partnerships that respect and amplify what makes human intelligence unique – our being-in-the-world, our temporality, and our fundamental way of sharing meaning with others.\nThese insights have profound implications for how businesses should approach AI implementation, which we’ll explore in the following chapters. But the key takeaway is this: successful AI strategy requires understanding not just what computers can do, but what makes human intelligence irreplaceably unique.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Beyond Computation: The Philosophy of Human Intelligence</span>"
    ]
  },
  {
    "objectID": "Ch05.html",
    "href": "Ch05.html",
    "title": "5  The What vs. How Divide: AI’s Real Impact on Knowledge Work",
    "section": "",
    "text": "5.1 The Traditional “How” Advantage\nUntil recently, career success in knowledge work depended heavily on mastering “how” skills - knowing how to build a compelling PowerPoint, how to structure a financial model, or how to write efficient code. But as AI systems become more capable at these technical tasks, the competitive advantage is shifting dramatically toward people who know “what” needs to be done - those who can identify the right problems to solve and strategies to pursue.\nThis fundamental shift from “how” to “what” has profound implications for businesses, careers, and investment opportunities. Let’s explore why this transformation is happening and what it means for different stakeholders.\nTraditionally, organizations needed large teams of specialists who knew “how” to perform various technical tasks: - Financial analysts who knew how to build complex Excel models - Software engineers who knew how to write code in specific languages - Designers who knew how to use tools like Photoshop - Writers who knew how to craft clear technical documentation - Translators who knew how to convert text between languages\nThese specialists developed their skills through years of practice and training. Their expertise created both job security and earning power - companies were willing to pay premium salaries for people who could execute complex technical tasks effectively.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The What vs. How Divide: AI's Real Impact on Knowledge Work</span>"
    ]
  },
  {
    "objectID": "Ch05.html#ais-disruption-of-how",
    "href": "Ch05.html#ais-disruption-of-how",
    "title": "5  The What vs. How Divide: AI’s Real Impact on Knowledge Work",
    "section": "5.2 AI’s Disruption of “How”",
    "text": "5.2 AI’s Disruption of “How”\nLarge language models and other AI tools are rapidly getting better at many of these “how” tasks: - ChatGPT can write basic code in multiple languages - Midjourney can generate sophisticated images - Translation tools are approaching human-level quality - AI assistants can create presentations and documentation\nThis capability is expanding quickly. Tasks that seemed immune to automation just a few years ago are now being handled competently by AI systems. And unlike human specialists who may take years to master new skills, AI systems can be rapidly retrained or fine-tuned for new capabilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The What vs. How Divide: AI's Real Impact on Knowledge Work</span>"
    ]
  },
  {
    "objectID": "Ch05.html#the-rise-of-what-skills",
    "href": "Ch05.html#the-rise-of-what-skills",
    "title": "5  The What vs. How Divide: AI’s Real Impact on Knowledge Work",
    "section": "5.3 The Rise of “What” Skills",
    "text": "5.3 The Rise of “What” Skills\nAs AI handles more of the “how,” competitive advantage shifts to people who excel at determining “what” needs to be done: - What problems are worth solving? - What features should a product include? - What markets should a company enter? - What strategies will create sustainable advantages? - What metrics matter most for success?\nThese “what” decisions require capabilities that current AI systems fundamentally lack:\nPattern Recognition Across Domains Humans can notice subtle patterns and draw insights across seemingly unrelated fields. A business leader might see parallels between consumer behavior in fashion and trends in enterprise software, leading to novel strategic insights. Current AI systems, despite their broad training, struggle to make these creative connections in meaningful ways.\nJudgment Under Uncertainty Many crucial business decisions involve incomplete information and conflicting priorities. Experienced leaders develop judgment about which risks are worth taking and which tradeoffs make sense. This type of judgment emerges from years of seeing both successes and failures firsthand - something AI systems cannot truly replicate.\nUnderstanding Human Context Success in business ultimately depends on understanding human needs, motivations, and behaviors. While AI can process vast amounts of data about human behavior, it lacks the innate understanding that comes from being human and experiencing the full range of human emotions and social dynamics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The What vs. How Divide: AI's Real Impact on Knowledge Work</span>"
    ]
  },
  {
    "objectID": "Ch05.html#real-world-examples",
    "href": "Ch05.html#real-world-examples",
    "title": "5  The What vs. How Divide: AI’s Real Impact on Knowledge Work",
    "section": "5.4 Real-World Examples",
    "text": "5.4 Real-World Examples\nLet’s look at some specific examples of how this “what vs. how” divide plays out:\nSoftware Development - How: AI can now write basic code, debug problems, and even suggest optimizations - What: Humans still needed to determine which features will delight users, how different components should work together, and what technical debt is worth taking on\nInvestment Analysis - How: AI can process financial statements, generate comparison tables, and even write initial analysis - What: Humans required to identify which companies have sustainable advantages, which management teams are trustworthy, and which market opportunities are truly attractive\nHealthcare - How: AI increasingly capable at analyzing test results, suggesting diagnoses, and recommending treatments - What: Human doctors crucial for understanding patient context, weighing complex tradeoffs, and maintaining trust in treatment decisions",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The What vs. How Divide: AI's Real Impact on Knowledge Work</span>"
    ]
  },
  {
    "objectID": "Ch05.html#investment-implications",
    "href": "Ch05.html#investment-implications",
    "title": "5  The What vs. How Divide: AI’s Real Impact on Knowledge Work",
    "section": "5.5 Investment Implications",
    "text": "5.5 Investment Implications\nThis shift has important implications for investors:\nWinners: - Companies that help humans make better “what” decisions - Tools that augment human judgment rather than replace it - Platforms that combine AI capabilities with human insight - Businesses with strong human judgment at their core\nLosers: - Pure automation plays that don’t preserve human judgment - Companies selling commoditized “how” skills - Businesses that can’t articulate their human advantage",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The What vs. How Divide: AI's Real Impact on Knowledge Work</span>"
    ]
  },
  {
    "objectID": "Ch05.html#the-future-of-work",
    "href": "Ch05.html#the-future-of-work",
    "title": "5  The What vs. How Divide: AI’s Real Impact on Knowledge Work",
    "section": "5.6 The Future of Work",
    "text": "5.6 The Future of Work\nThis transition suggests several changes in how organizations will operate:\nNew Organizational Structures\n\nFlatter hierarchies as AI handles routine coordination\nSmaller, more senior teams focused on “what” decisions\nGreater emphasis on judgment and strategic thinking\n\nChanged Skill Requirements\n\nLess focus on technical tool proficiency\nMore emphasis on strategic thinking and judgment\nGreater value placed on cross-domain knowledge\n\nModified Training Approaches\n\nReduced time spent teaching technical “how” skills\nIncreased focus on judgment development\nMore emphasis on understanding human factors",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The What vs. How Divide: AI's Real Impact on Knowledge Work</span>"
    ]
  },
  {
    "objectID": "Ch05.html#preparing-for-the-transition",
    "href": "Ch05.html#preparing-for-the-transition",
    "title": "5  The What vs. How Divide: AI’s Real Impact on Knowledge Work",
    "section": "5.7 Preparing for the Transition",
    "text": "5.7 Preparing for the Transition\nFor individuals and organizations looking to succeed in this new environment, several approaches make sense:\nFor Individuals:\n\nFocus on developing judgment through varied experiences\nBuild broad knowledge across multiple domains\nPractice making and learning from strategic decisions\nGet comfortable with ambiguity and uncertainty\n\nFor Organizations:\n\nInvest in tools that augment human judgment\nDevelop processes that capture and share strategic insights\nCreate cultures that value and develop good judgment\nBuild teams with diverse perspectives and experiences\n\n\n5.7.1 The Human Element Remains Central\nIt’s crucial to remember that this shift doesn’t diminish the importance of human contribution - it actually elevates it. As AI handles more routine tasks, human judgment, creativity, and wisdom become more valuable, not less.\nConsider the example of chess: Despite AI systems being able to beat any human player, human chess hasn’t disappeared. Instead, it’s evolved. The most interesting matches now involve human-AI collaboration, where success depends on humans knowing what positions to play for and when to trust or override AI suggestions.\nThis pattern will likely repeat across many fields - the key to success will be understanding what humans do best and creating systems that augment these capabilities rather than try to replace them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The What vs. How Divide: AI's Real Impact on Knowledge Work</span>"
    ]
  },
  {
    "objectID": "Ch05.html#looking-ahead",
    "href": "Ch05.html#looking-ahead",
    "title": "5  The What vs. How Divide: AI’s Real Impact on Knowledge Work",
    "section": "5.8 Looking Ahead",
    "text": "5.8 Looking Ahead\nThe transition from “how” to “what” won’t happen overnight, but it’s already underway. Organizations and individuals that recognize and adapt to this shift will have significant advantages. Those that continue to focus primarily on “how” skills risk finding their capabilities increasingly commoditized by AI.\nThis shift also suggests we need to rethink education and training. Rather than focusing primarily on teaching technical skills that AI might soon handle, we should emphasize developing judgment, creativity, and strategic thinking - the fundamentally human capabilities that will become increasingly valuable.\nThe future belongs not to those who can execute tasks most efficiently, but to those who can best decide what tasks are worth doing in the first place.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The What vs. How Divide: AI's Real Impact on Knowledge Work</span>"
    ]
  },
  {
    "objectID": "Ch06.html",
    "href": "Ch06.html",
    "title": "6  Finding the Sweet Spot: Where AI and Humans Work Best Together",
    "section": "",
    "text": "6.1 The Enhancement Zone\nA friend who runs customer support at a Fortune 500 consumer products company recently faced a dilemma. Her team had been assigned to evaluate Microsoft’s CoPilot, an AI assistant meant to boost productivity. After weeks of testing, she discovered something surprising: while the AI could compose email replies and generate meeting summaries, employees were spending as much time editing the AI’s output as they would have spent writing from scratch. The AI’s responses, though grammatically perfect, lacked the human touch that customers expect. Her experience crystallizes a crucial insight about AI implementation: the goal isn’t to replace humans, but to enhance their capabilities in ways that create genuine value.\nThis chapter explores how organizations can identify the optimal balance between human judgment and AI capabilities. We’ll examine specific cases where AI enhances rather than replaces human work, drawing lessons that apply across industries. The key lies in understanding which aspects of work benefit from AI assistance and which require irreducible human judgment.\nConsider how pilots interact with modern aircraft systems. The autopilot handles routine flight operations, allowing human pilots to focus on higher-level decisions and emergency responses. This division of labor exemplifies what we call the “enhancement zone” – where AI handles detail-oriented tasks while humans manage strategic decisions. The pilot doesn’t need to know exactly how the autopilot calculates minor course corrections. Instead, they focus on what matters: safely getting passengers to their destination.\nThis same principle applies across knowledge work. Take the case of JPMorgan’s implementation of AI in its investment research division. Rather than replacing analysts, the AI serves as a research assistant, scanning thousands of documents to surface relevant information. The analysts’ role has evolved to focus more on synthesis and client relationship management – areas where human judgment remains irreplaceable. The AI handles the “how” of information gathering, while humans determine “what” insights matter to clients.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finding the Sweet Spot: Where AI and Humans Work Best Together</span>"
    ]
  },
  {
    "objectID": "Ch06.html#the-decisioning-framework",
    "href": "Ch06.html#the-decisioning-framework",
    "title": "6  Finding the Sweet Spot: Where AI and Humans Work Best Together",
    "section": "6.2 The Decisioning Framework",
    "text": "6.2 The Decisioning Framework\nThrough our research across industries, we’ve identified three key questions that help organizations find their enhancement sweet spot:\n\nWhat decisions require contextual understanding that AI cannot replicate?\nWhere can AI’s pattern recognition complement human insight?\nHow can workflow be restructured to leverage both human and AI strengths?\n\nThe answers vary by industry, but the framework remains consistent. At a leading radiology practice we studied, AI excels at flagging potential anomalies in medical images, but radiologists remain essential for interpreting these findings in the context of patient history and symptoms. The AI handles the “how” of image processing, while doctors focus on “what” the findings mean for patient care.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finding the Sweet Spot: Where AI and Humans Work Best Together</span>"
    ]
  },
  {
    "objectID": "Ch06.html#learning-from-failed-implementations",
    "href": "Ch06.html#learning-from-failed-implementations",
    "title": "6  Finding the Sweet Spot: Where AI and Humans Work Best Together",
    "section": "6.3 Learning from Failed Implementations",
    "text": "6.3 Learning from Failed Implementations\nNot all attempts at human-AI collaboration succeed. The autonomous vehicle industry offers telling examples. Companies that tried to eliminate human drivers entirely have struggled, while those that use AI to enhance human capabilities show more promise. Take Daimler Trucks’ approach: rather than pursuing full autonomy, they developed AI systems that help human drivers operate more safely and efficiently. The AI handles tasks like maintaining safe following distances and optimizing fuel consumption, while humans manage complex navigation and unexpected situations.\nThis pattern repeats across industries. Attempts to fully automate creative work often disappoint, while approaches that enhance human creativity succeed. Adobe’s AI features don’t replace designers but handle tedious tasks like image resizing and background removal, freeing humans to focus on creative direction and client needs.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finding the Sweet Spot: Where AI and Humans Work Best Together</span>"
    ]
  },
  {
    "objectID": "Ch06.html#the-role-of-management",
    "href": "Ch06.html#the-role-of-management",
    "title": "6  Finding the Sweet Spot: Where AI and Humans Work Best Together",
    "section": "6.4 The Role of Management",
    "text": "6.4 The Role of Management\nFinding the sweet spot requires rethinking traditional management approaches. Leaders must understand both AI’s capabilities and human psychology. When McKinsey implemented AI tools for its consultants, success came not from the technology itself but from careful attention to how consultants would interact with it. The firm recognized that consultants needed to maintain ownership of client relationships and strategic insights while leveraging AI for research and analysis.\nThis highlights a crucial point: the enhancement sweet spot isn’t static. As AI capabilities evolve, the boundary between human and machine tasks shifts. Organizations need adaptive frameworks that allow for continuous rebalancing of responsibilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finding the Sweet Spot: Where AI and Humans Work Best Together</span>"
    ]
  },
  {
    "objectID": "Ch06.html#implementation-guidelines",
    "href": "Ch06.html#implementation-guidelines",
    "title": "6  Finding the Sweet Spot: Where AI and Humans Work Best Together",
    "section": "6.5 Implementation Guidelines",
    "text": "6.5 Implementation Guidelines\nBased on our research, successful human-AI collaboration requires several key elements:\nClear role definition: Both humans and AI need well-defined responsibilities that play to their strengths. At Goldman Sachs, AI handles data analysis and pattern recognition in trading, while human traders focus on strategy and risk assessment.\nFeedback loops: Humans must be able to override and correct AI when necessary. This isn’t just about catching errors – it’s about maintaining human agency and improving the system over time.\nTraining and adaptation: Workers need support in developing new skills that complement AI capabilities. The goal isn’t to compete with AI but to leverage it effectively.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finding the Sweet Spot: Where AI and Humans Work Best Together</span>"
    ]
  },
  {
    "objectID": "Ch06.html#cultural-considerations",
    "href": "Ch06.html#cultural-considerations",
    "title": "6  Finding the Sweet Spot: Where AI and Humans Work Best Together",
    "section": "6.6 Cultural Considerations",
    "text": "6.6 Cultural Considerations\nPerhaps most importantly, organizations must maintain what we call “human centrality” – the principle that AI serves human objectives rather than the reverse. This requires careful attention to organizational culture. When Microsoft deployed AI tools across its engineering teams, success came from emphasizing how the technology would enhance rather than replace human capabilities.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finding the Sweet Spot: Where AI and Humans Work Best Together</span>"
    ]
  },
  {
    "objectID": "Ch06.html#looking-forward",
    "href": "Ch06.html#looking-forward",
    "title": "6  Finding the Sweet Spot: Where AI and Humans Work Best Together",
    "section": "6.7 Looking Forward",
    "text": "6.7 Looking Forward\nAs AI capabilities continue to advance, finding the enhancement sweet spot becomes increasingly crucial. Organizations that succeed will be those that maintain focus on human judgment while leveraging AI’s computational power. This isn’t just about efficiency – it’s about creating sustainable competitive advantage through superior decision-making.\nConsider the evolution of chess after Deep Blue defeated Garry Kasparov. Rather than eliminating human players, AI led to the emergence of centaur chess, where human-AI teams consistently outperform either humans or AI alone. This model points to the future of knowledge work: not a competition between human and artificial intelligence, but a synthesis that enhances human capabilities while preserving human agency.\nThe key lies in understanding that AI’s role is to handle the “how” while humans focus on the “what.” Organizations that grasp this principle and implement it effectively will find themselves operating in the sweet spot where human judgment and AI capabilities combine to create superior outcomes.\nLooking ahead, we expect to see continued evolution in how humans and AI interact. The enhancement sweet spot will shift as AI capabilities advance, but the fundamental principle remains: successful implementation requires keeping humans central to decision-making while leveraging AI’s unique capabilities. Organizations that master this balance will be best positioned to thrive in an AI-enhanced future.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Finding the Sweet Spot: Where AI and Humans Work Best Together</span>"
    ]
  },
  {
    "objectID": "Ch07.html",
    "href": "Ch07.html",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "",
    "text": "7.1 The Enhancement Framework\nThe gap between AI’s theoretical potential and its practical implementation remains stubbornly wide. Most organizations approach AI implementation backward, starting with the technology rather than the human element. They ask “What can AI do?” instead of “How can we enhance our people’s capabilities?” This fundamental mistake leads to costly failures and missed opportunities.\nConsider the case of a Fortune 500 consumer products company that recently evaluated Microsoft’s CoPilot suite. The project team, tasked with finding AI-driven productivity gains, discovered that while the technology could indeed compose email replies and summarize meetings, users spent as much time editing the AI’s output as they would have spent writing from scratch. The AI was attempting to replace rather than enhance human capabilities.\nThis pattern repeats across industries. Companies implement AI solutions looking for quick automation wins, only to discover that the technology works best when designed to augment human judgment rather than replace it. The key to successful implementation lies in understanding the distinct roles of human and artificial intelligence, then building systems that leverage the strengths of both.\nSuccessful AI implementation requires a clear framework for distinguishing between tasks that benefit from automation versus those that require human enhancement. This distinction often maps to what we call the “what versus how” paradigm.\nAI excels at executing the “how” - processing vast amounts of data, identifying patterns, and generating outputs based on learned patterns. Humans excel at determining “what” needs to be done, providing context, and exercising judgment about the appropriateness of AI-generated outputs. This framework helps organizations avoid the common pitfall of trying to automate judgment-heavy tasks that are better suited for enhancement.\nFor example, in financial services, AI can process market data and generate trading signals at superhuman speed (the “how”), but successful firms keep humans in charge of setting strategy and risk parameters (the “what”). JPMorgan’s implementation of AI in its trading operations demonstrates this principle. Rather than attempting to fully automate trading decisions, the bank uses AI to enhance traders’ capabilities by surfacing relevant patterns and anomalies while leaving final decisions to human judgment.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch07.html#building-trust-through-transparency",
    "href": "Ch07.html#building-trust-through-transparency",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "7.2 Building Trust Through Transparency",
    "text": "7.2 Building Trust Through Transparency\nOne of the biggest implementation challenges is building trust between human users and AI systems. This requires making the AI’s capabilities and limitations transparent to users while establishing clear boundaries for human oversight.\nThe healthcare sector offers instructive examples. Successful implementations of AI in medical diagnosis follow a clear pattern: the AI processes medical images or patient data to flag potential issues (the “how”), but doctors remain responsible for diagnosis and treatment decisions (the “what”). This approach maintains the critical element of human judgment while leveraging AI’s pattern-recognition capabilities.\nCrucially, these systems are designed to make their reasoning process visible to doctors. Rather than simply presenting conclusions, they highlight the specific patterns or anomalies that led to their recommendations. This transparency helps build trust and enables doctors to exercise informed judgment about the AI’s suggestions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch07.html#the-training-challenge",
    "href": "Ch07.html#the-training-challenge",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "7.3 The Training Challenge",
    "text": "7.3 The Training Challenge\nImplementing AI successfully requires significant investment in human training, but not in the way most organizations expect. Rather than focusing solely on technical training about how to use AI tools, successful implementations emphasize training in judgment - helping humans understand when and how to rely on AI assistance.\nConsider the example of AeroVironment’s implementation of AI in military applications. Operators receive extensive training not just in operating the AI systems, but in understanding their limitations and failure modes. This approach produces operators who can effectively collaborate with AI while maintaining the critical human judgment needed for military operations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch07.html#measuring-success",
    "href": "Ch07.html#measuring-success",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "7.4 Measuring Success",
    "text": "7.4 Measuring Success\nTraditional metrics often fail to capture the true value of AI enhancement implementations. Organizations frequently focus on easily measurable efficiency gains while missing the more substantial benefits of enhanced human judgment and decision-making.\nPalantir’s successful implementations offer a model for better measurement. Rather than focusing solely on automation metrics, they measure success through the quality of human-AI collaboration - tracking how effectively analysts use AI tools to reach better conclusions faster. This approach recognizes that the value of AI lies not in replacing human analysts but in enhancing their capabilities.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch07.html#common-implementation-pitfalls",
    "href": "Ch07.html#common-implementation-pitfalls",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "7.5 Common Implementation Pitfalls",
    "text": "7.5 Common Implementation Pitfalls\nSeveral common mistakes consistently undermine AI implementation efforts:\n\nOveremphasis on Automation: Organizations often focus on fully automating processes rather than enhancing human capabilities. This leads to resistance from users and missed opportunities for genuine enhancement.\nInsufficient Training in Judgment: Most training programs focus on technical operation rather than helping users understand when and how to rely on AI assistance.\nPoor Integration with Existing Workflows: AI tools are often implemented as standalone solutions rather than being integrated into existing work processes.\nLack of Clear Boundaries: Organizations frequently fail to establish clear guidelines about which decisions require human judgment and which can be delegated to AI.\nInadequate Feedback Loops: Many implementations lack effective mechanisms for humans to provide feedback on AI performance and for that feedback to improve the system.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch07.html#the-path-to-successful-implementation",
    "href": "Ch07.html#the-path-to-successful-implementation",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "7.6 The Path to Successful Implementation",
    "text": "7.6 The Path to Successful Implementation\nSuccessful AI implementation follows a clear pattern:\n\nStart with Human Judgment: Begin by identifying where human judgment adds the most value in your organization. These areas are typically candidates for enhancement rather than automation.\nDesign for Transparency: Ensure AI systems make their reasoning visible to users, enabling informed human oversight.\nIntegrate Gradually: Begin with small-scale implementations that allow users to build trust and understanding of the AI’s capabilities and limitations.\nEstablish Clear Boundaries: Define explicit guidelines for which decisions require human judgment and which can be delegated to AI.\nBuild Feedback Loops: Create mechanisms for continuous improvement based on human feedback about AI performance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch07.html#looking-ahead-the-future-of-implementation",
    "href": "Ch07.html#looking-ahead-the-future-of-implementation",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "7.7 Looking Ahead: The Future of Implementation",
    "text": "7.7 Looking Ahead: The Future of Implementation\nAs AI capabilities continue to advance, the implementation challenge will evolve. Vector databases, for example, are emerging as a crucial tool for enhancing human search and discovery capabilities. These systems don’t replace human judgment but rather augment it by making conceptual connections that might otherwise be missed.\nHowever, the fundamental principle remains: successful implementation requires keeping humans central to the process. As one senior technology executive noted, “The goal isn’t to make the AI smarter, but to make the human-AI collaboration more effective.”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch07.html#the-human-element-in-implementation",
    "href": "Ch07.html#the-human-element-in-implementation",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "7.8 The Human Element in Implementation",
    "text": "7.8 The Human Element in Implementation\nThe most successful AI implementations maintain what critics have called “seeing the human doing it” - the visible presence of human judgment and accountability in key decisions. This principle extends beyond mere oversight; it recognizes that human judgment, intuition, and accountability are essential elements of effective decision-making.\nConsider the creative industries, where AI tools are increasingly common but rarely trusted to work autonomously. The attempt to use AI to complete Beethoven’s unfinished tenth symphony demonstrates this principle. While the AI could generate music that superficially resembled Beethoven’s style, critics and audiences alike found it lacking the essential human element that makes great art compelling.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch07.html#investment-implications",
    "href": "Ch07.html#investment-implications",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "7.9 Investment Implications",
    "text": "7.9 Investment Implications\nFor investors and business leaders, understanding these implementation challenges is crucial. Success in AI implementation often correlates more strongly with an organization’s ability to enhance human capabilities than with the sophistication of its AI technology.\nCompanies that demonstrate a sophisticated understanding of human-AI collaboration, with clear frameworks for maintaining human judgment while leveraging AI capabilities, are more likely to succeed in the long term. This insight should guide both investment decisions and implementation strategies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch07.html#conclusion",
    "href": "Ch07.html#conclusion",
    "title": "7  The Implementation Challenge: Making Enhancement Work",
    "section": "7.10 Conclusion",
    "text": "7.10 Conclusion\nSuccessful AI implementation requires a fundamental shift in thinking - from automation to enhancement, from replacement to augmentation. Organizations that master this shift, keeping humans central while leveraging AI’s capabilities, will be best positioned to create sustainable value in the AI era.\nThe challenge isn’t technical - it’s organizational and human. Success requires careful attention to human factors, clear frameworks for collaboration, and a commitment to enhancing rather than replacing human capabilities. As AI continues to evolve, this human-centric approach to implementation will become increasingly crucial for organizational success.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Implementation Challenge: Making Enhancement Work</span>"
    ]
  },
  {
    "objectID": "Ch08.html",
    "href": "Ch08.html",
    "title": "8  The Human Element in Creative Work: Lessons from Beethoven’s Tenth",
    "section": "",
    "text": "8.1 The Beethoven Challenge\nIn 2021, a fascinating experiment took place at the intersection of artificial intelligence and classical music. An all-star team of musicologists, historians, and AI programmers attempted something unprecedented: completing Beethoven’s unfinished Tenth Symphony using artificial intelligence. The project offers profound insights into both the capabilities and limitations of AI in creative work, while illuminating why human authenticity remains irreplaceable even as AI capabilities advance.\nBeethoven left the world with nine completed symphonies and a handful of musical sketches for a tenth. For centuries, these fragments tantalized musicians and scholars, hinting at what might have been. The AI team at Playform AI saw an opportunity: they would train their models on Beethoven’s complete works, use the sketches as a foundation, and generate what they believed would be a plausible completion of the Tenth Symphony.\nOn paper, this appeared to be an ideal AI project. The team had: - A complete corpus of Beethoven’s work for training - Actual sketches from the composer for the specific piece - Access to leading experts in both music and AI - State-of-the-art machine learning capabilities\nIf AI could successfully complete this task, it would demonstrate remarkable creative capabilities. The result would be more than just a technical achievement – it would show that AI could authentically channel human genius.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Human Element in Creative Work: Lessons from Beethoven's Tenth</span>"
    ]
  },
  {
    "objectID": "Ch08.html#the-results-technical-success-artistic-failure",
    "href": "Ch08.html#the-results-technical-success-artistic-failure",
    "title": "8  The Human Element in Creative Work: Lessons from Beethoven’s Tenth",
    "section": "8.2 The Results: Technical Success, Artistic Failure",
    "text": "8.2 The Results: Technical Success, Artistic Failure\nThe resulting symphony is technically impressive. To an untrained ear, it sounds plausibly like classical music. The notes follow reasonable progressions, the orchestration is proper, and there are moments that sound distinctly Beethoven-esque. Yet something crucial is missing.\nAs Beethoven scholar Jan Swafford noted in his review, the work is “aimless and uninspired.” The missing element isn’t technical proficiency – it’s the human struggle for excellence, the creative tension that produces true artistic breakthrough. This reveals a fundamental truth about AI that extends far beyond music: technical competence is not the same as authentic creation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Human Element in Creative Work: Lessons from Beethoven's Tenth</span>"
    ]
  },
  {
    "objectID": "Ch08.html#the-role-of-human-struggle",
    "href": "Ch08.html#the-role-of-human-struggle",
    "title": "8  The Human Element in Creative Work: Lessons from Beethoven’s Tenth",
    "section": "8.3 The Role of Human Struggle",
    "text": "8.3 The Role of Human Struggle\nSwafford’s critique points to something deeper about human creativity: “We humans need to see the human doing it: Willie Mays making the catch that doesn’t look possible. When it comes to art, we need to see a woman or a man struggling with the universal mediocrity that is the natural lot of all of us and somehow out of some mélange of talent, skill, and luck doing the impossible.”\nThis insight helps explain why even technically perfect AI creations often feel hollow. Consider:\n\nThe Value of Imperfection: Beethoven’s own sketches were often mundane and uninspired. It was through sustained effort and refinement that he transformed ordinary musical ideas into extraordinary compositions. The process itself – the human struggle – is part of what we value.\nQuality Discrimination: Training AI on all of Beethoven’s works presents another challenge: Beethoven himself sometimes wrote mediocre pieces when working on commission. The AI cannot distinguish between his masterpieces and his mere commercial work. It lacks the human judgment to separate the transcendent from the ordinary.\nEmotional Connection: The audience’s knowledge that a human created the work is part of the work’s meaning. We connect with art partly because we know another human being struggled to create it.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Human Element in Creative Work: Lessons from Beethoven's Tenth</span>"
    ]
  },
  {
    "objectID": "Ch08.html#beyond-music-the-broader-implications",
    "href": "Ch08.html#beyond-music-the-broader-implications",
    "title": "8  The Human Element in Creative Work: Lessons from Beethoven’s Tenth",
    "section": "8.4 Beyond Music: The Broader Implications",
    "text": "8.4 Beyond Music: The Broader Implications\nThis principle – that we need to “see the human doing it” – extends far beyond classical music. Consider these parallels:\n\n8.4.1 Sports and Entertainment\nThe same dynamic explains why robotic sports would never generate the passion of human athletics. When Colombian and Argentine soccer fans stormed Miami’s Hard Rock Stadium to see Lionel Messi play, they weren’t just seeking to witness technical excellence – they wanted to see human brilliance in action. No matter how technically sophisticated, robots playing soccer would never generate such emotional investment.\n\n\n8.4.2 Business Leadership\nIn corporate settings, technically correct decisions aren’t always the best decisions. Leaders need to be seen making difficult choices, wrestling with uncertainty, and taking responsibility for outcomes. An AI might make statistically optimal decisions, but it cannot provide the human element that builds trust and inspires teams.\n\n\n8.4.3 Professional Services\nEven in fields where technical expertise is paramount – law, medicine, financial advice – clients need to see human judgment at work. They need to know that a human professional has wrestled with their unique situation and exercised judgment on their behalf.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Human Element in Creative Work: Lessons from Beethoven's Tenth</span>"
    ]
  },
  {
    "objectID": "Ch08.html#the-enhancement-opportunity",
    "href": "Ch08.html#the-enhancement-opportunity",
    "title": "8  The Human Element in Creative Work: Lessons from Beethoven’s Tenth",
    "section": "8.5 The Enhancement Opportunity",
    "text": "8.5 The Enhancement Opportunity\nThe Beethoven experiment reveals the true opportunity for AI in creative fields: enhancement rather than replacement. AI can be an invaluable tool for: - Generating initial ideas - Testing different approaches - Handling technical aspects of implementation - Providing feedback and suggestions\nBut the human element remains essential for: - Exercise of judgment - Quality discrimination - Emotional resonance - Authentic creation",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Human Element in Creative Work: Lessons from Beethoven's Tenth</span>"
    ]
  },
  {
    "objectID": "Ch08.html#looking-forward",
    "href": "Ch08.html#looking-forward",
    "title": "8  The Human Element in Creative Work: Lessons from Beethoven’s Tenth",
    "section": "8.6 Looking Forward",
    "text": "8.6 Looking Forward\nAs AI capabilities continue to advance, maintaining this balance between human authenticity and AI enhancement becomes crucial. Organizations that understand this will: - Keep humans visibly involved in key creative and decision-making processes - Use AI to augment rather than replace human judgment - Maintain transparency about the role of AI in their processes - Invest in developing human creativity and judgment alongside AI capabilities\nThe lesson from Beethoven’s Tenth is clear: technical proficiency, even at a very high level, is not enough. The human element – the visible struggle for excellence, the exercise of judgment, the emotional connection – remains irreplaceable. This insight should guide how we implement AI across industries and applications.\nFor business leaders, the implications are profound. Success in an AI-enhanced world doesn’t mean replacing human creativity and judgment with artificial intelligence. Instead, it means finding ways to use AI that preserve and amplify the human elements that create true value. The goal should be to let AI handle the technical “how” while humans focus on the essential “what” – the judgment, creativity, and authentic connection that only humans can provide.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Human Element in Creative Work: Lessons from Beethoven's Tenth</span>"
    ]
  },
  {
    "objectID": "Ch09.html",
    "href": "Ch09.html",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "",
    "text": "9.1 The Enhancement Premium\n[Chart 1: Total returns of SOXX vs S&P 500, 2020-2025, showing semiconductor outperformance]\nThe investment implications of artificial intelligence extend far beyond the obvious beneficiaries in Silicon Valley. While companies like Nvidia have captured headlines with astronomical returns, the real opportunity lies in identifying businesses that effectively leverage AI to enhance rather than replace human capabilities. This nuanced view requires looking past the hype to understand how AI actually creates sustainable competitive advantages.\nCompanies that successfully implement AI to augment human capabilities rather than pursue full automation tend to exhibit several characteristics that lead to superior returns:\n[Chart 2: Comparison of operating metrics (revenue per employee, ROIC) between companies pursuing enhancement vs replacement strategies]\nLet’s examine each of these characteristics in detail:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch09.html#the-enhancement-premium",
    "href": "Ch09.html#the-enhancement-premium",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "",
    "text": "Higher productivity per employee\nImproved capital efficiency\nGreater customer retention\nMore sustainable competitive advantages\nLower regulatory risk\n\n\n\n\n9.1.1 Higher Productivity Per Employee\nThe enhancement approach typically delivers superior productivity metrics compared to pure automation strategies. Rather than simply reducing headcount, enhancement allows companies to increase output per employee by:\n\nEliminating low-value repetitive tasks\nImproving decision quality through better analytics\nEnabling employees to handle more complex cases\nReducing error rates and rework\nAccelerating knowledge transfer between employees\n\n[Chart 2a: Productivity metrics across enhancement-focused companies]\nOur analysis of companies across multiple sectors shows that successful AI enhancement implementations typically deliver 30-45% improvements in revenue per employee over 3-5 years, compared to 15-20% for pure automation approaches. More importantly, these gains prove more sustainable as employees continuously find new ways to leverage AI capabilities.\n\n\n9.1.2 Improved Capital Efficiency\nEnhancement strategies typically require lower upfront capital investment than full automation while delivering superior returns on invested capital (ROIC). This occurs because:\n\nInfrastructure requirements are more modest\nImplementation can be iterative rather than “big bang”\nTraining costs are lower as existing skills remain valuable\nMaintenance costs are more predictable\nRisk of project failure is reduced\n\n[Chart 2b: ROIC comparison between enhancement and automation strategies]\nCompanies pursuing enhancement strategies typically maintain ROIC 800-1200 basis points above their cost of capital, compared to 400-600 basis points for automation-focused peers. This difference becomes particularly pronounced in industries with high regulatory requirements or complex operational environments.\n\n\n9.1.3 Greater Customer Retention\nEnhanced human capabilities consistently deliver superior customer satisfaction and retention compared to pure automation. This manifests in several ways:\n\nHigher Net Promoter Scores (NPS)\nLower customer churn rates\nIncreased share of wallet\nMore effective cross-selling\nStronger brand loyalty\n\n[Chart 2c: Customer retention metrics by AI strategy type]\nThe data is particularly striking in high-touch industries like wealth management and healthcare, where enhancement strategies show customer retention rates 15-20 percentage points higher than automation-focused competitors.\n\n\n9.1.4 More Sustainable Competitive Advantages\nEnhancement strategies create deeper moats than pure automation approaches because they:\n\nBuild on existing competitive advantages rather than trying to create new ones\nCombine proprietary data with human judgment in ways that are harder to replicate\nCreate positive feedback loops between AI systems and human expertise\nGenerate company-specific insights that go beyond generic AI capabilities\nDevelop organizational capabilities that are difficult to copy\n\n[Chart 2d: Comparison of competitive advantage durability metrics]\nThis sustainability shows up in financial metrics like gross margin stability and market share retention. Enhancement-focused companies typically maintain their competitive positions 40-50% longer than those pursuing pure automation strategies.\n\n\n9.1.5 Lower Regulatory Risk\nThe enhancement approach typically faces fewer regulatory hurdles and lower compliance costs because:\n\nIt maintains clear human accountability for decisions\nIt creates fewer labor relations issues\nIt raises fewer privacy and algorithmic bias concerns\nIt aligns better with existing regulatory frameworks\nIt provides clearer audit trails for decision-making\n\n[Chart 2e: Regulatory incident rates and compliance costs by strategy type]\nOur analysis shows that companies pursuing enhancement strategies typically spend 30-40% less on regulatory compliance and face 60-70% fewer regulatory incidents than those focused on full automation.\nConsider the contrast between two approaches in financial services. The first wave of robo-advisors attempted to completely automate investment management, promising lower fees through elimination of human advisors. While they achieved some success in basic portfolio allocation, they struggled to retain high-net-worth clients who value human judgment in complex financial planning. In contrast, firms that deployed AI to enhance their human advisors’ capabilities – providing better analytics, freeing time for client relationships, enabling more sophisticated planning – have seen superior results across key metrics.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch09.html#value-creation-vs-value-capture",
    "href": "Ch09.html#value-creation-vs-value-capture",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "9.2 Value Creation vs Value Capture",
    "text": "9.2 Value Creation vs Value Capture\nA critical distinction for investors is understanding where AI creates value versus where that value is captured. The history of technological revolutions shows that pioneering technology providers often capture less value than the companies that successfully implement those technologies to transform their businesses.\n[Chart 3: Historical comparison showing relative returns of technology providers vs successful implementers across multiple tech waves - PCs, Internet, Mobile]\nConsider the personal computer revolution: while Intel and Microsoft captured significant value through their effective duopoly on PC architecture, many of the largest fortunes were built by companies that used PCs to transform their industries – from Walmart’s supply chain optimization to Bloomberg’s financial terminals. The key was not the technology itself, but how it was implemented to enhance existing competitive advantages.\nThis pattern suggests three categories of potential AI winners:\n\n9.2.1 1. Infrastructure Providers\nCompanies providing the essential building blocks of AI implementation stand to benefit regardless of which applications ultimately succeed. This includes:\n\nSemiconductor manufacturers (especially those focused on AI-specific chips)\nCloud computing platforms\nSpecialized AI infrastructure (vector databases, AI development tools)\nData center operators\n\nThe key here is identifying companies with sustainable competitive advantages rather than simply riding the current wave of enthusiasm. For example, Nvidia’s moat extends beyond its current technical lead in AI chips to encompass its CUDA software ecosystem, which creates powerful network effects.\n\n\n9.2.2 2. Enhancement Enablers\nThese companies develop tools and platforms that help other businesses implement AI in ways that enhance human capabilities. Success in this category requires:\n\nDeep understanding of specific industry workflows\nAbility to integrate with existing systems\nStrong focus on user experience\nClear ROI proposition\n\n[Chart 4: Growth rates and gross margins of leading enhancement platform companies]\nThe most successful companies in this category solve specific, high-value problems rather than attempting to build general-purpose AI platforms. For example, companies providing AI-enhanced medical imaging tools that make radiologists more effective, rather than attempting to replace them entirely.\n\n\n9.2.3 3. Enhanced Incumbents\nPerhaps the largest opportunity lies with existing companies that successfully leverage AI to enhance their competitive advantages. The key characteristics to look for include:\n\nStrong existing market positions\nSignificant proprietary data assets\nCulture of technological innovation\nClear enhancement use cases\n\n[Chart 5: Performance comparison of incumbents with high vs low AI implementation effectiveness scores]\nManufacturing companies with decades of process data, insurers with rich claims histories, and healthcare providers with extensive patient records all have opportunities to create sustainable advantages through AI enhancement. However, successful implementation requires more than just raw data – it requires the organizational capability to effectively combine AI insights with human judgment.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch09.html#implementation-risk",
    "href": "Ch09.html#implementation-risk",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "9.3 Implementation Risk",
    "text": "9.3 Implementation Risk\nThe enhancement thesis suggests that many companies will destroy value through poor AI implementation strategies. Common failure modes include:\n\nOverestimating AI capabilities\nUnderinvesting in human capital\nPoor integration with existing workflows\nMisaligned incentives\nInadequate data infrastructure\n\n[Chart 6: Case studies of failed AI implementations and their impact on company performance]\nFor investors, this suggests the importance of understanding not just what AI initiatives a company is pursuing, but how they are implementing them. Key questions include:\n\nHow does AI fit into the company’s competitive strategy?\nWhat is the balance between automation and enhancement?\nHow are they measuring success?\nWhat is their approach to training and retaining key employees?\nHow are they managing data quality and governance?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch09.html#valuation-considerations",
    "href": "Ch09.html#valuation-considerations",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "9.4 Valuation Considerations",
    "text": "9.4 Valuation Considerations\nThe enhancement thesis has important implications for how we value AI-related investments. Traditional metrics like revenue growth and gross margins need to be supplemented with factors such as:\n\nQuality of data assets\nEffectiveness of human-AI integration\nSustainability of competitive advantages\nRegulatory risk exposure\n\n[Chart 7: Valuation metrics for different categories of AI-related companies]\nCompanies successfully pursuing enhancement strategies often exhibit:\n\nHigher revenue per employee\nBetter customer retention metrics\nMore sustainable margins\nLower regulatory risk\nHigher returns on invested capital",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch09.html#timing-considerations",
    "href": "Ch09.html#timing-considerations",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "9.5 Timing Considerations",
    "text": "9.5 Timing Considerations\nThe implementation of AI enhancement strategies follows a different timeline than pure automation efforts. While full automation projects often promise quick cost savings, enhancement strategies typically show results through:\n\nInitial productivity improvements\nGradual competitive advantages\nExpanding use cases\nNetwork effects\nSustained market share gains\n\n[Chart 8: Typical timeline of returns from enhancement vs automation strategies]\nThis suggests that investors need patience and a long-term perspective when evaluating enhancement plays. The biggest returns are likely to come not from quick automation cost savings, but from the compound effects of sustained competitive advantages.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch09.html#geographic-considerations",
    "href": "Ch09.html#geographic-considerations",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "9.6 Geographic Considerations",
    "text": "9.6 Geographic Considerations\nThe global nature of AI development creates important geographic diversification opportunities. While the United States leads in many areas, significant innovation is occurring in:\n\nEast Asia (particularly in hardware and manufacturing applications)\nEurope (especially in industrial and healthcare applications)\nIsrael (security and enterprise applications)\nIndia (service sector applications)\n\n[Chart 9: Global distribution of AI patents and investment by category]\nDifferent regions also show varying approaches to human-AI integration, influenced by local labor markets, regulations, and cultural factors. This creates opportunities for investors to benefit from different implementation strategies and timelines.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch09.html#regulatory-environment",
    "href": "Ch09.html#regulatory-environment",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "9.7 Regulatory Environment",
    "text": "9.7 Regulatory Environment\nThe enhancement thesis suggests lower regulatory risk than pure automation strategies. Companies focused on enhancing human capabilities rather than replacing workers are likely to face:\n\nLess political opposition\nFewer labor disputes\nMore manageable liability issues\nClearer regulatory frameworks\n\n[Chart 10: Comparison of regulatory incidents and costs between enhancement and automation-focused companies]\nHowever, investors must still monitor evolving regulations around:\n\nData privacy and security\nAlgorithm transparency\nWorker protection\nIndustry-specific requirements",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch09.html#investment-strategy-implications",
    "href": "Ch09.html#investment-strategy-implications",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "9.8 Investment Strategy Implications",
    "text": "9.8 Investment Strategy Implications\nThe enhancement thesis suggests several key principles for AI-related investment strategies:\n\nFocus on sustainable competitive advantages rather than technical leadership\nPrioritize companies with clear enhancement use cases\nLook for strong data assets and implementation capabilities\nConsider timing and geographic diversification\nMonitor regulatory developments\n\nSuccess requires moving beyond simple narratives about AI replacing humans to understand how technology can create lasting competitive advantages through enhancement of human capabilities.\nLet’s examine each principle in detail:\n\n9.8.1 Sustainable Competitive Advantages vs. Technical Leadership\nIn “Good to Great,” Jim Collins emphasized the importance of sustainable competitive advantages over flashy technology adoption. This principle is particularly relevant for AI investments. While technical leadership can provide temporary advantages, sustainable success requires what Michael Porter termed “strategic fit” – the alignment of multiple activities that competitors cannot easily replicate.\nKey indicators to evaluate include:\n\nNetwork effects from combined human-AI systems\nProprietary data moats\nOrganizational learning capabilities\nCultural adaptability to technological change\nLeadership understanding of AI’s strategic role\n\n[Chart 11: Comparison of long-term returns between technical leaders and strategic implementers]\nCompanies that build AI into their strategic architecture, rather than treating it as a standalone initiative, typically demonstrate superior long-term performance. This echoes W. Chan Kim and Renée Mauborgne’s Blue Ocean Strategy principle of making competition irrelevant through fundamental business model innovation.\n\n\n9.8.2 Clear Enhancement Use Cases\nFollowing Peter Drucker’s emphasis on effectiveness over efficiency, successful AI implementations should focus on enhancing core value-creating activities rather than merely reducing costs. Key evaluation criteria include:\n\nDirect impact on customer value proposition\nIntegration with existing workflows\nClear metrics for success\nScalability of enhancement effects\nEmployee adoption and satisfaction\n\n[Chart 12: Success rates of AI initiatives by clarity of use case]\nCompanies with well-defined enhancement use cases typically achieve 3-4x higher returns on AI investments compared to those pursuing general automation strategies. This aligns with Clayton Christensen’s jobs-to-be-done framework – successful AI enhancement addresses specific, valuable jobs that customers need done.\n\n\n9.8.3 Data Assets and Implementation Capabilities\nAs Thomas H. Davenport argued in “Competing on Analytics,” competitive advantage increasingly comes from how companies use data rather than merely possessing it. For AI enhancement strategies, key factors include:\n\nQuality and uniqueness of proprietary data\nData governance and management capabilities\nIntegration of structured and unstructured data\nAbility to combine human insight with machine learning\nTechnical debt management\n\n[Chart 13: Correlation between data capabilities and AI implementation success]\nThe most successful companies typically demonstrate what Gary Hamel calls “strategic architecture” – the ability to orchestrate multiple capabilities around a coherent vision for AI enhancement.\n\n\n9.8.4 Timing and Geographic Diversification\nDrawing on Geoffrey Moore’s “Crossing the Chasm” framework, different industries and regions are at different stages of AI adoption. Successful investment strategies should:\n\nMatch investment timing to adoption curves\nConsider regional variations in AI maturity\nAccount for industry-specific implementation challenges\nBalance early-mover advantages against execution risk\nMaintain flexibility in deployment strategies\n\n[Chart 14: AI adoption curves across industries and regions]\nThis approach helps avoid what Spencer Johnson described in “Who Moved My Cheese?” – becoming too attached to existing success patterns while missing emerging opportunities.\n\n\n9.8.5 Regulatory Development Monitoring\nFollowing the principle that John Hagel III and John Seely Brown described as “shaping strategies,” successful companies actively engage with evolving regulatory frameworks rather than merely reacting to them. Key areas to monitor include:\n\nData privacy and protection requirements\nAlgorithm transparency standards\nIndustry-specific regulations\nCross-border data flow restrictions\nLabor law implications\n\n[Chart 15: Impact of regulatory changes on AI implementation strategies]\nCompanies that proactively address regulatory concerns while pursuing enhancement strategies typically face lower compliance costs and fewer implementation delays.\n\n\n9.8.6 Implementation Framework\nSuccessful AI enhancement strategies typically follow what Tom Peters might call a “bias for action” while maintaining strategic discipline. Key elements include:\n\nClear strategic intent aligned with core competencies\nFocus on human-AI synergies rather than replacement\nIterative implementation with clear feedback loops\nStrong change management capabilities\nContinuous learning and adaptation\n\n[Chart 16: Success rates by implementation approach]\nThis framework combines elements of agile methodology with traditional change management principles, creating what Rita McGrath terms “transient advantage” through continuous innovation in how humans and AI work together.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch09.html#conclusion",
    "href": "Ch09.html#conclusion",
    "title": "9  Following the Money: Investment Implications of the Enhancement Thesis",
    "section": "9.9 Conclusion",
    "text": "9.9 Conclusion\nThe investment opportunities created by AI will be larger and more diverse than many currently recognize, but they will not necessarily accrue to the most obvious candidates. The biggest winners will be companies that effectively leverage AI to enhance human capabilities rather than those pursuing pure automation strategies. This requires investors to look beyond technical capabilities to understand how companies implement AI in ways that create sustainable competitive advantages.\nThe enhancement thesis suggests that the most attractive investments will be found not just among technology providers, but across industries where AI can significantly enhance existing competitive advantages. Success in identifying these opportunities requires combining traditional financial analysis with deep understanding of how AI actually creates value in specific business contexts.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Following the Money: Investment Implications of the Enhancement Thesis</span>"
    ]
  },
  {
    "objectID": "Ch10.html",
    "href": "Ch10.html",
    "title": "10  Building the Future: A Human-Centric Vision for AI",
    "section": "",
    "text": "10.1 The Enhancement Imperative\nThroughout this book, we’ve examined how artificial intelligence enhances rather than replaces human capabilities. As we look toward the future, the critical question is not whether AI will automate jobs away, but how we can build systems that amplify human judgment while preserving human agency. This final chapter outlines concrete steps for business leaders, policymakers, and society at large to ensure AI development remains human-centric.\nThe narrative around AI has focused excessively on automation and replacement, leading to misallocation of resources and flawed implementation strategies. Our research across industries reveals that successful AI deployments invariably preserve human judgment and agency. This isn’t just about maintaining employment – it’s about achieving superior outcomes.\n[Chart: Comparison of outcomes in fully automated vs. human-AI collaborative systems across key metrics: accuracy, adaptability to change, stakeholder trust, and long-term sustainability]\nConsider the evolution of automated trading systems in financial markets. Early attempts at fully autonomous trading frequently resulted in catastrophic failures when market conditions deviated from historical patterns. Today’s most successful trading operations combine AI’s pattern recognition capabilities with human traders’ contextual understanding and risk assessment. The machines excel at identifying opportunities, but humans remain essential for understanding how changing geopolitical dynamics or regulatory shifts might affect market behavior.\nThis pattern repeats across industries. In healthcare, AI excels at analyzing medical images and identifying potential anomalies, but doctors provide crucial judgment in interpreting these findings within the broader context of patient health. In creative fields, AI tools can generate endless variations of designs or content, but human creators remain essential for determining which outputs actually resonate with audiences.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building the Future: A Human-Centric Vision for AI</span>"
    ]
  },
  {
    "objectID": "Ch10.html#rethinking-ai-implementation",
    "href": "Ch10.html#rethinking-ai-implementation",
    "title": "10  Building the Future: A Human-Centric Vision for AI",
    "section": "10.2 Rethinking AI Implementation",
    "text": "10.2 Rethinking AI Implementation\nBusiness leaders must shift their AI implementation strategies away from automation-first approaches toward enhancement-focused frameworks. This requires:\n\nStarting with human workflows rather than technical capabilities\n\nMap existing decision processes\nIdentify areas where human judgment is crucial\nLook for opportunities to augment rather than replace human capabilities\n\nBuilding trust through transparency\n\nEnsure AI systems provide explanations for their recommendations\nMaintain clear accountability for decisions\nCreate feedback loops between human operators and AI systems\n\nInvesting in human capital alongside AI capabilities\n\nTrain workers to effectively collaborate with AI systems\nDevelop new roles that leverage uniquely human skills\nCreate career paths that evolve with technology\n\n\n[Chart: Framework for assessing AI implementation opportunities along two axes: potential for enhancement vs. automation, and importance of human judgment]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building the Future: A Human-Centric Vision for AI</span>"
    ]
  },
  {
    "objectID": "Ch10.html#policy-imperatives",
    "href": "Ch10.html#policy-imperatives",
    "title": "10  Building the Future: A Human-Centric Vision for AI",
    "section": "10.3 Policy Imperatives",
    "text": "10.3 Policy Imperatives\nPolicymakers face the challenge of fostering AI innovation while ensuring its development serves human interests. We propose several key principles:\n\n10.3.1 1. Preserving Human Agency\nRegulations should require meaningful human oversight in critical decisions. This doesn’t mean humans must review every AI output, but rather that systems should be designed to preserve human judgment where it matters most. For example:\n\nMandatory human review of AI-generated content in sensitive contexts\nRequirements for human oversight in high-stakes medical or financial decisions\nPreservation of human judgment in legal proceedings\n\n\n\n10.3.2 2. Promoting Transparency\nAI systems should be required to provide explanations for their recommendations in forms that humans can understand and evaluate. This is particularly crucial in:\n\nHealthcare decisions\nFinancial advice\nLegal proceedings\nEducational assessments\n\n\n\n10.3.3 3. Protecting Privacy and Data Rights\nAs AI systems become more powerful, protecting individual privacy and data rights becomes increasingly crucial. Policies should:\n\nGive individuals control over their personal data\nRequire explicit consent for AI training\nEnsure transparency in how personal data is used\nProtect against algorithmic discrimination\n\n[Chart: Matrix showing key policy areas and their relative importance across different sectors: healthcare, finance, education, etc.]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building the Future: A Human-Centric Vision for AI</span>"
    ]
  },
  {
    "objectID": "Ch10.html#investment-implications",
    "href": "Ch10.html#investment-implications",
    "title": "10  Building the Future: A Human-Centric Vision for AI",
    "section": "10.4 Investment Implications",
    "text": "10.4 Investment Implications\nThe shift toward human-centric AI has significant implications for investment strategy. Successful investors will need to:\n\nEvaluate companies based on their approach to human-AI collaboration\n\nLook for evidence of enhancement rather than pure automation strategies\nAssess investments in human capital alongside AI capabilities\nConsider the sustainability of human-AI collaborative models\n\nUnderstand the limitations of pure AI plays\n\nBe skeptical of companies promising full automation\nLook for business models that leverage uniquely human capabilities\nConsider the regulatory and social acceptance risks of automation-first approaches\n\nIdentify opportunities in human capital development\n\nTraining and education providers\nWorkflow tools that facilitate human-AI collaboration\nCompanies developing explainable AI systems\n\n\n[Chart: Performance comparison of companies with human-centric vs. automation-focused AI strategies]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building the Future: A Human-Centric Vision for AI</span>"
    ]
  },
  {
    "objectID": "Ch10.html#the-path-forward",
    "href": "Ch10.html#the-path-forward",
    "title": "10  Building the Future: A Human-Centric Vision for AI",
    "section": "10.5 The Path Forward",
    "text": "10.5 The Path Forward\nThe next decade will be crucial in determining whether AI development enhances or diminishes human capability and agency. Success requires:\n\n10.5.1 For Business Leaders:\n\nShift focus from automation to enhancement\nInvest in human capital alongside AI capabilities\nBuild trust through transparency and accountability\nDevelop clear frameworks for human-AI collaboration\n\n\n\n10.5.2 For Policymakers:\n\nCreate regulatory frameworks that preserve human agency\nPromote transparency and explainability\nProtect individual privacy and data rights\nFoster innovation while ensuring human-centric development\n\n\n\n10.5.3 For Society:\n\nEmphasize education that develops uniquely human capabilities\nBuild systems that amplify human judgment rather than replace it\nMaintain focus on human values and ethics in AI development\nPreserve space for human creativity and agency",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building the Future: A Human-Centric Vision for AI</span>"
    ]
  },
  {
    "objectID": "Ch10.html#conclusion",
    "href": "Ch10.html#conclusion",
    "title": "10  Building the Future: A Human-Centric Vision for AI",
    "section": "10.6 Conclusion",
    "text": "10.6 Conclusion\nThe AI revolution need not lead to widespread displacement or diminished human agency. By focusing on enhancement rather than replacement, we can build a future where artificial intelligence amplifies human capabilities while preserving human judgment and creativity. This requires conscious choices in how we develop and deploy AI systems, along with regulatory frameworks that protect human interests.\nThe companies that succeed in the AI era will be those that find ways to combine human judgment with artificial intelligence, creating systems that are more capable than either humans or machines alone. The societies that thrive will be those that preserve human agency while leveraging AI’s capabilities to solve pressing challenges.\nThe future of AI is not about machines replacing humans, but about humans and machines working together in ways that enhance rather than diminish human capability and agency. Building this future requires conscious choice and sustained effort from business leaders, policymakers, and society at large. The decisions we make in the coming years will determine whether AI fulfills its promise of enhancing human capability or instead diminishes human agency and judgment.\n[Final Chart: Vision for human-centric AI development showing the interconnection of business strategy, policy frameworks, and societal choices in creating a future that enhances rather than replaces human capabilities]\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Building the Future: A Human-Centric Vision for AI</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Throughout this book, we’ve explored why AI will enhance rather than replace human capabilities. As we conclude, let’s examine what this means for building a human-centric AI future.\n\nRICHARD &gt;\nAfter spending decades in technology implementation, I’ve observed a pattern: the most successful deployments of new technology are those that augment human capabilities rather than attempt to replicate them. This remains true with AI. Consider our earlier discussion of self-driving cars. The fundamental challenge isn’t processing power or sensor technology – it’s replicating the intuitive judgment that allows human drivers to anticipate potential dangers before they materialize.\nThe same principle applies across industries. AI can process vast amounts of medical images or financial data, but it cannot replace a doctor’s holistic understanding of patient health or an investor’s grasp of how geopolitical events might affect market psychology. The future lies not in pursuing full automation, but in finding the sweet spot where AI enhances human judgment.\n\n\nSAMI &gt;\nThis aligns with what I’ve observed in financial markets. The most successful AI implementations in finance aren’t the fully automated trading systems that attempt to replace human traders. Instead, they’re the tools that help analysts process more information more quickly, allowing them to focus their human judgment on higher-level strategy and risk assessment.\nConsider the case of JPMorgan’s ChatCFO. Rather than replacing financial analysts, it serves as a powerful tool that allows them to process vast amounts of financial data more efficiently. The human analysts remain essential for interpreting results and making strategic recommendations.\n\n\nRICHARD &gt;\nThis brings us to a crucial point about AI implementation. The key question isn’t “what tasks can AI perform?” but rather “how can AI enhance human capabilities?” This requires a fundamental shift in how we think about AI development and deployment.\nFirst, organizations need to move beyond the simple automation mindset. Instead of asking “can AI do this job?”, they should ask “how can AI help humans do this job better?” This might mean using AI to handle routine tasks while freeing humans to focus on judgment-intensive work, or using AI to process vast amounts of data while leaving the interpretation to human experts.\n\n\nSAMI &gt;\nThe investment implications here are significant. Companies that understand this enhancement paradigm will likely outperform those pursuing full automation. We’re already seeing this in healthcare, where companies developing AI tools to assist doctors are showing more promise than those attempting to replace medical judgment entirely.\n\n\nRICHARD &gt;\nLooking ahead, several principles should guide AI development:\n\nMaintain human agency and judgment at the center of decision-making\nDesign AI systems that complement rather than replace human capabilities\nFocus on transparency and explainability in AI systems\nPrioritize human-AI collaboration over full automation\nInvest in human skill development alongside AI capabilities\n\n\n\nSAMI &gt;\nFor policymakers, this means creating frameworks that encourage responsible AI development while preserving human agency. This might include:\n\nRegulations requiring human oversight of critical AI systems\nStandards for AI transparency and explainability\nInvestment in education and training programs that prepare workers for human-AI collaboration\nIncentives for companies developing enhancement-focused AI applications\n\n\n\nRICHARD &gt;\nRemember our discussion of Beethoven’s tenth symphony? The AI attempt to complete it demonstrated both the power and limitations of artificial intelligence. While the AI could generate music that superficially resembled Beethoven’s style, it couldn’t capture the spark of human creativity that made his work truly great.\nThis illustrates a broader truth about AI: it’s at its best when enhancing human capabilities rather than trying to replace them. The future of AI lies not in replicating human intelligence but in amplifying it.\n\n\nSAMI &gt;\nAs we look to the future, the winners in the AI revolution will be those who understand this fundamental truth. Whether in finance, healthcare, creative industries, or any other sector, success will come from finding ways to combine human judgment with AI capabilities.\nThe human element isn’t just a feel-good addition to AI systems – it’s essential to their effectiveness. As we’ve shown throughout this book, keeping humans “in the loop” leads to better outcomes than pursuing full automation.\nThe AI revolution is indeed transformative, but not in the way many predict. Instead of a future where AI replaces human workers, we’re entering an era of enhancement, where human capabilities are amplified by artificial intelligence. Understanding and embracing this reality is crucial for anyone looking to thrive in the AI-enhanced future.\n\n\n\n\nAbdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, et al. 2024. “Yi: Open Foundation Models by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein Generation with Evolutionary Diffusion: Sequence Is All You Need.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23. Virtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. “The Reversal Curse: LLMs Trained on \"A Is B\" Fail to Learn \"B Is A\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023. “Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The Consequences of Generative AI for Online Knowledge Communities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, et al. 2023. “Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024. “Stealing Part of a Production Language Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018. “Clinically Applicable Deep Learning for Diagnosis and Referral in Retinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023. “Evaluating ChatGPT as a Recommender System: A Rigorous Approach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. “Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.” Philosophical Psychology 20 (2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language AI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from AI Automation: A Review of the Arguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in Healthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, et al. 2024. “Large Language Model Influence on Diagnostic Reasoning: A Randomized Clinical Trial.” JAMA Network Open 7 (10): e2440969. https://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A. Christakis, Philip E. Tetlock, and William A. Cunningham. 2023. “AI and the Transformation of Social Science Research.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, and Rebecca J. Passonneau. 2023. “CALM : A Multi-Task Benchmark for Comprehensive Assessment of Language Model Bias.” arXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang, Jiguang Wang, and Hao Chen. 2024. “Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. “How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024. “ChatGPT Is Bullshit.” Ethics and Information Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” arXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J Sorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize How Cancer Patients Access Information: ChatGPT Represents a Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010. https://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy Ganta, and Selena Stewart. 2023. “ChatGPT Vs Google for Queries Related to Dementia and Other Cognitive Decline: Comparison of Results.” Journal of Medical Internet Research 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and Signaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024. “CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and Adam Waytz. 2023. “Exposure to Automation Explains Religious Declines.” Proceedings of the National Academy of Sciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and Seungwon Shin. 2023. “DarkBERT: A Language Model for the Dark Side of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024. “AlphaFold Meets Flow Matching for Generating Protein Ensembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. “Challenges and Applications of Large Language Models.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. “Mission: Impossible Language Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge.” JAMA 330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023. “Acoustic Analysis and Prediction of Type 2 Diabetes Mellitus Using Smartphone-Recorded Voice Segments.” Mayo Clinic Proceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in Mammographic Screening.” Nature Reviews Clinical Oncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. 2024. “Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language Models.” Preprint. Medical Education. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do. Cambridge, Massachusetts London, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep Learning Is Effective for Classifying Normal Versus Age-Related Macular Degeneration OCT Images.” Ophthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.” Edited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New England Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The AI Revolution in Medicine: GPT-4 and Beyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022. “DALL-E 2 Fails to Reliably Capture Common Syntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from Generative AI to Trustworthy AI: What LLMs Might Learn from Cyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. “Transformer-Lite: High-Efficiency Deployment of Large Language Models on Mobile Phone GPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating Verifiability in Generative Search Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, et al. 2023. “AgentBench: Evaluating LLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM: Optimizing Sub-Billion Parameter Language Models for On-Device Use Cases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. “BioGPT: Generative Pre-Trained Transformer for Biomedical Text Generation and Mining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R. Greenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From Glucose Patterns to Health Outcomes: A Generalizable Foundation Model for Continuous Glucose Monitor Data Analysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. “Let’s Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language and Thought in Large Language Models: A Cognitive Perspective.” arXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning Large Language Models for Clinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, et al. 2023. “Towards Accurate Differential Diagnosis with Large Language Models.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial.” Journal of Medical Internet Research 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for Regulatory Oversight of Large Language Models (or Generative AI) in Healthcare.” Npj Digital Medicine 6 (1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A Philosophical Introduction to Language Models – Part I: Continuity With Classic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. “Capabilities of GPT-4 on Medical Challenge Problems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics, Olamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023. “Organ Aging Signatures in the Plasma Proteome Track Health and Disease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. 2023. “Proving Test Set Contamination in Black Box Language Models.” arXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024. “Deepfake Generation and Detection: A Benchmark and Survey.” arXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023. “\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards Building Multilingual Language Model for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew D. Selbst. 2022. “The Fallacy of AI Functionality.” In 2022 ACM Conference on Fairness, Accountability, and Transparency, 959–72. https://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop K Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023. “Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study.” Journal of Medical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et al. 2024. “Mathematical Discoveries from Program Search with Large Language Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political Biases of ChatGPT.” Social Sciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of LLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, et al. 2024. “Capabilities of Gemini Models in Medicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles Brundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing Power and the Governance of Artificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. “The Curse of Recursion: Training on Generated Data Makes Models Forget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, et al. 2023. “Large Language Models Encode Clinical Knowledge.” Nature 620 (7972): 172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu, Tang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in Psychiatry Research, Diagnosis, and Therapy.” Asian Journal of Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu Zhou, Yun Lin, et al. 2024. “SpreadsheetLLM: Encoding Spreadsheets for Large Language Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, et al. 2024. “Towards Conversational Diagnostic AI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H. S. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022. “Will We Run Out of Data? An Analysis of the Limits of Scaling Datasets in Machine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023. “SAM-OCTA: A Fine-Tuning Strategy for Applying Foundation Model to OCTA Image Segmentation Tasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large Language Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024. “What Was Your Prompt? A Remote Keylogging Attack on AI Assistants.” arXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. “Do Llamas Work in English? On the Latent Language of Multilingual Transformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023. “The Shaky Foundations of Large Language Models and Foundation Models for Electronic Health Records.” Npj Digital Medicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet).” Perspectives on Psychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023. “Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda S Pescatello. 2024. “Comprehensiveness, Accuracy, and Readability of Exercise Recommendations Provided by an AI-Based Chatbot: Mixed Methods Study.” JMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for Joint Segmentation, Detection and Recognition of Biomedical Objects Across Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, et al. 2023. “CLIP in Medical Imaging: A Comprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, et al. 2024. “NATURAL PLAN: Benchmarking LLMs on Natural Language Planning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abdin, Marah, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed\nAwadallah, Hany Awadalla, Nguyen Bach, et al. 2024. “Phi-3\nTechnical Report: A\nHighly Capable Language\nModel Locally on Your\nPhone.” arXiv. http://arxiv.org/abs/2404.14219.\n\n\nAI, 01, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei\nZhang, et al. 2024. “Yi: Open Foundation\nModels by 01.AI.” arXiv. http://arxiv.org/abs/2403.04652.\n\n\nAlamdari, Sarah, Nitya Thakkar, Rianne Van Den Berg, Alex Xijie Lu,\nNicolo Fusi, Ava Pardis Amini, and Kevin K Yang. 2023. “Protein\nGeneration with Evolutionary Diffusion: Sequence Is All You\nNeed.” Preprint. Bioengineering. https://doi.org/10.1101/2023.09.11.556673.\n\n\nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. “On the Dangers of\nStochastic Parrots: Can\nLanguage Models Be\nToo Big? 🦜.” In Proceedings of the\n2021 ACM Conference on Fairness,\nAccountability, and Transparency, 610–23.\nVirtual Event Canada: ACM. https://doi.org/10.1145/3442188.3445922.\n\n\nBerglund, Lukas, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper\nStickland, Tomasz Korbak, and Owain Evans. 2023. “The\nReversal Curse: LLMs Trained on\n\"A Is B\" Fail to Learn \"B Is\nA\".” arXiv. http://arxiv.org/abs/2309.12288.\n\n\nBsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. 2023.\n“Principled Instructions Are\nAll You Need for\nQuestioning LLaMA-1/2,\nGPT-3.5/4.” arXiv. http://arxiv.org/abs/2312.16171.\n\n\nBurtch, Gordon, Dokyun Lee, and Zhichen Chen. 2024. “The\nConsequences of Generative AI for Online Knowledge\nCommunities.” Scientific Reports 14 (1): 10413. https://doi.org/10.1038/s41598-024-61221-0.\n\n\nButlin, Patrick, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan\nBirch, Axel Constant, George Deane, et al. 2023. “Consciousness in\nArtificial Intelligence: Insights\nfrom the Science of Consciousness.” https://doi.org/10.48550/ARXIV.2308.08708.\n\n\nCarlini, Nicholas, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas\nSteinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, et al. 2024.\n“Stealing Part of a Production\nLanguage Model.” arXiv. http://arxiv.org/abs/2403.06634.\n\n\nChang, Kent K., Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023a.\n“Speak, Memory: An\nArchaeology of Books Known to\nChatGPT/GPT-4.” https://doi.org/10.48550/ARXIV.2305.00118.\n\n\n———. 2023b. “Speak, Memory: An\nArchaeology of Books Known to\nChatGPT/GPT-4.” arXiv. http://arxiv.org/abs/2305.00118.\n\n\nDe Fauw, Jeffrey, Joseph R. Ledsam, Bernardino Romera-Paredes, Stanislav\nNikolov, Nenad Tomasev, Sam Blackwell, Harry Askham, et al. 2018.\n“Clinically Applicable Deep Learning for Diagnosis and Referral in\nRetinal Disease.” Nature Medicine 24 (9): 1342–50. https://doi.org/10.1038/s41591-018-0107-6.\n\n\nDi Palma, Dario, Giovanni Maria Biancofiore, Vito Walter Anelli,\nFedelucio Narducci, Tommaso Di Noia, and Eugenio Di Sciascio. 2023.\n“Evaluating ChatGPT as a Recommender\nSystem: A Rigorous\nApproach.” arXiv. http://arxiv.org/abs/2309.03613.\n\n\nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel\nIlharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021.\n“Documenting Large Webtext\nCorpora: A Case\nStudy on the Colossal Clean\nCrawled Corpus.” https://doi.org/10.48550/ARXIV.2104.08758.\n\n\nDreyfus, Hubert L. 2007. “Why Heideggerian\nAI Failed and How\nFixing It Would Require\nMaking It More\nHeideggerian.” Philosophical Psychology 20\n(2): 247–68. https://doi.org/10.1080/09515080701239510.\n\n\nEpoch AI. 2024. “Data on Large Language\nAI Models.” https://epochai.org/data/large-scale-ai-models.\n\n\nErdil, Ege, and Tamay Besiroglu. 2024. “Explosive Growth from\nAI Automation: A Review of the\nArguments.” arXiv. http://arxiv.org/abs/2309.11690.\n\n\nEsteva, Andre, Alexandre Robicquet, Bharath Ramsundar, Volodymyr\nKuleshov, Mark DePristo, Katherine Chou, Claire Cui, Greg Corrado,\nSebastian Thrun, and Jeff Dean. 2019. “A Guide to Deep Learning in\nHealthcare.” Nature Medicine 25 (1): 24–29. https://doi.org/10.1038/s41591-018-0316-z.\n\n\nFeng, Shangbin, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023.\n“From Pretraining Data to\nLanguage Models to Downstream\nTasks: Tracking the Trails of\nPolitical Biases Leading to\nUnfair NLP Models.” https://doi.org/10.48550/ARXIV.2305.08283.\n\n\nGoh, Ethan, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah\nKerman, Joséphine A. Cool, et al. 2024. “Large\nLanguage Model Influence on\nDiagnostic Reasoning: A\nRandomized Clinical\nTrial.” JAMA Network Open 7 (10): e2440969.\nhttps://doi.org/10.1001/jamanetworkopen.2024.40969.\n\n\nGrossmann, Igor, Matthew Feinberg, Dawn C. Parker, Nicholas A.\nChristakis, Philip E. Tetlock, and William A. Cunningham. 2023.\n“AI and the Transformation of Social Science\nResearch.” Science 380 (6650): 1108–9. https://doi.org/10.1126/science.adi1778.\n\n\nGupta, Vipul, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson,\nand Rebecca J. Passonneau. 2023. “CALM :\nA Multi-Task Benchmark for\nComprehensive Assessment of\nLanguage Model Bias.”\narXiv. http://arxiv.org/abs/2308.12539.\n\n\nHe, Yuting, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang,\nJiguang Wang, and Hao Chen. 2024. “Foundation Model\nfor Advancing Healthcare:\nChallenges, Opportunities, and\nFuture Directions.” arXiv. http://arxiv.org/abs/2404.03264.\n\n\nHendy, Amr, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr,\nHitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. “How Good Are\nGPT Models at Machine\nTranslation? A Comprehensive\nEvaluation.” arXiv. http://arxiv.org/abs/2302.09210.\n\n\nHicks, Michael Townsen, James Humphries, and Joe Slater. 2024.\n“ChatGPT Is Bullshit.” Ethics and\nInformation Technology 26 (2): 38. https://doi.org/10.1007/s10676-024-09775-5.\n\n\nHoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,\nTrevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022.\n“Training Compute-Optimal\nLarge Language Models.”\narXiv. http://arxiv.org/abs/2203.15556.\n\n\nHopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J\nSorich. 2023. “Artificial Intelligence Chatbots Will Revolutionize\nHow Cancer Patients Access Information: ChatGPT Represents\na Paradigm-Shift.” JNCI Cancer Spectrum 7 (2): pkad010.\nhttps://doi.org/10.1093/jncics/pkad010.\n\n\nHristidis, Vagelis, Nicole Ruggiano, Ellen L Brown, Sai Rithesh Reddy\nGanta, and Selena Stewart. 2023. “ChatGPT Vs\nGoogle for Queries Related to\nDementia and Other Cognitive\nDecline: Comparison of\nResults.” Journal of Medical Internet\nResearch 25 (July): e48966. https://doi.org/10.2196/48966.\n\n\nHuang, Haifeng, and Zhi Li. 2013. “Propaganda and\nSignaling.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2325101.\n\n\nHuang, Kaixuan, Yuanhao Qu, Henry Cousins, William A. Johnson, Di Yin,\nMihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, and Le Cong. 2024.\n“CRISPR-GPT: An\nLLM Agent for Automated\nDesign of Gene-Editing\nExperiments.” arXiv. http://arxiv.org/abs/2404.18021.\n\n\nJackson, Joshua Conrad, Kai Chi Yam, Pok Man Tang, Chris G. Sibley, and\nAdam Waytz. 2023. “Exposure to Automation Explains Religious\nDeclines.” Proceedings of the National Academy of\nSciences 120 (34): e2304748120. https://doi.org/10.1073/pnas.2304748120.\n\n\nJin, Youngjin, Eugene Jang, Jian Cui, Jin-Woo Chung, Yongjae Lee, and\nSeungwon Shin. 2023. “DarkBERT: A\nLanguage Model for the Dark\nSide of the Internet.” https://doi.org/10.48550/ARXIV.2305.08596.\n\n\nJing, Bowen, Bonnie Berger, and Tommi Jaakkola. 2024.\n“AlphaFold Meets Flow\nMatching for Generating Protein\nEnsembles.” arXiv. http://arxiv.org/abs/2402.04845.\n\n\nKaddour, Jean, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta\nRaileanu, and Robert McHardy. 2023. “Challenges and\nApplications of Large Language\nModels.” https://doi.org/10.48550/ARXIV.2307.10169.\n\n\nKallini, Julie, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald,\nand Christopher Potts. 2024. “Mission: Impossible\nLanguage Models.” arXiv. http://arxiv.org/abs/2401.06416.\n\n\nKanjee, Zahir, Byron Crowe, and Adam Rodman. 2023. “Accuracy of a\nGenerative Artificial\nIntelligence Model in a Complex\nDiagnostic Challenge.” JAMA\n330 (1): 78. https://doi.org/10.1001/jama.2023.8288.\n\n\nKaufman, Jaycee M., Anirudh Thommandram, and Yan Fossat. 2023.\n“Acoustic Analysis and Prediction of\nType 2 Diabetes Mellitus\nUsing Smartphone-Recorded\nVoice Segments.” Mayo Clinic\nProceedings: Digital Health 1 (4): 534–44. https://doi.org/10.1016/j.mcpdig.2023.08.005.\n\n\nKillock, David. 2020. “AI Outperforms Radiologists in\nMammographic Screening.” Nature Reviews Clinical\nOncology 17 (3): 134–34. https://doi.org/10.1038/s41571-020-0329-7.\n\n\nKim, Yubin, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park.\n2024. “Health-LLM: Large\nLanguage Models for Health\nPrediction via Wearable Sensor\nData.” arXiv. http://arxiv.org/abs/2401.06866.\n\n\nKung, Tiffany H., Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina\nSillos, Lorie De Leon, Camille Elepaño, et al. 2022. “Performance\nof ChatGPT on USMLE: Potential\nfor AI-Assisted Medical\nEducation Using Large\nLanguage Models.” Preprint. Medical\nEducation. https://doi.org/10.1101/2022.12.19.22283643.\n\n\nLarson, Erik J. 2021. The Myth of Artificial Intelligence: Why\nComputers Can’t Think the Way We Do. Cambridge, Massachusetts\nLondon, England: The Belknap Press of Harvard University Press.\n\n\nLee, Cecilia S., Doug M. Baughman, and Aaron Y. Lee. 2017. “Deep\nLearning Is Effective for\nClassifying Normal Versus\nAge-Related Macular\nDegeneration OCT Images.”\nOphthalmology Retina 1 (4): 322–27. https://doi.org/10.1016/j.oret.2016.12.009.\n\n\nLee, Peter, Sebastien Bubeck, and Joseph Petro. 2023. “Benefits,\nLimits, and Risks of GPT-4 as an\nAI Chatbot for Medicine.”\nEdited by Jeffrey M. Drazen, Isaac S. Kohane, and Tze-Yun Leong. New\nEngland Journal of Medicine 388 (13): 1233–39. https://doi.org/10.1056/NEJMsr2214184.\n\n\nLee, Peter, Carey Goldberg, and Isaac Kohane. 2023. The\nAI Revolution in Medicine: GPT-4 and\nBeyond. 1st ed. Hoboken: Pearson.\n\n\nLeivada, Evelina, Elliot Murphy, and Gary Marcus. 2022.\n“DALL-E 2 Fails to\nReliably Capture Common\nSyntactic Processes.” arXiv. http://arxiv.org/abs/2210.12889.\n\n\nLenat, Doug, and Gary Marcus. 2023. “Getting from\nGenerative AI to Trustworthy\nAI: What LLMs Might Learn from\nCyc.” arXiv. http://arxiv.org/abs/2308.04445.\n\n\nLi, Luchang, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie.\n2024. “Transformer-Lite: High-Efficiency\nDeployment of Large Language\nModels on Mobile Phone\nGPUs.” arXiv. http://arxiv.org/abs/2403.20041.\n\n\nLiu, Nelson F., Tianyi Zhang, and Percy Liang. 2023. “Evaluating\nVerifiability in Generative\nSearch Engines.” arXiv. http://arxiv.org/abs/2304.09848.\n\n\nLiu, Xiao, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu\nGu, et al. 2023. “AgentBench: Evaluating\nLLMs as Agents.” https://doi.org/10.48550/ARXIV.2308.03688.\n\n\nLiu, Zechun, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian,\nIgor Fedorov, Yunyang Xiong, et al. 2024. “MobileLLM:\nOptimizing Sub-Billion Parameter\nLanguage Models for\nOn-Device Use\nCases.” arXiv. http://arxiv.org/abs/2402.14905.\n\n\nLu, Chris, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and\nDavid Ha. 2024. “The AI Scientist:\nTowards Fully Automated\nOpen-Ended Scientific\nDiscovery.” arXiv. http://arxiv.org/abs/2408.06292.\n\n\nLuo, Renqian, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon,\nand Tie-Yan Liu. 2022. “BioGPT: Generative\nPre-Trained Transformer for Biomedical Text Generation and\nMining.” Briefings in Bioinformatics 23 (6): bbac409. https://doi.org/10.1093/bib/bbac409.\n\n\nLutsker, Guy, Gal Sapir, Anastasia Godneva, Smadar Shilo, Jerry R.\nGreenfield, Dorit Samocha-Bonet, Shie Mannor, et al. 2024. “From\nGlucose Patterns to Health\nOutcomes: A Generalizable\nFoundation Model for Continuous\nGlucose Monitor Data\nAnalysis.” arXiv. http://arxiv.org/abs/2408.11876.\n\n\nMa, Xiao, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen.\n2023. “Let’s Do a Thought\nExperiment: Using Counterfactuals\nto Improve Moral\nReasoning.” https://doi.org/10.48550/ARXIV.2306.14308.\n\n\nMahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua\nB. Tenenbaum, and Evelina Fedorenko. 2023. “Dissociating Language\nand Thought in Large Language Models: A Cognitive Perspective.”\narXiv. http://arxiv.org/abs/2301.06627.\n\n\nManathunga, Supun, and Isuru Hettigoda. 2023. “Aligning\nLarge Language Models for\nClinical Tasks.” arXiv. http://arxiv.org/abs/2309.02884.\n\n\nMcDuff, Daniel, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake\nGarrison, Karan Singhal, et al. 2023. “Towards\nAccurate Differential Diagnosis\nwith Large Language\nModels.” arXiv. http://arxiv.org/abs/2312.00164.\n\n\nMeskó, Bertalan. 2023. “Prompt Engineering as an\nImportant Emerging Skill for\nMedical Professionals:\nTutorial.” Journal of Medical Internet\nResearch 25 (October): e50638. https://doi.org/10.2196/50638.\n\n\nMeskó, Bertalan, and Eric J. Topol. 2023. “The Imperative for\nRegulatory Oversight of Large Language Models (or Generative\nAI) in Healthcare.” Npj Digital Medicine 6\n(1): 120. https://doi.org/10.1038/s41746-023-00873-0.\n\n\nMillière, Raphaël, and Cameron Buckner. 2024. “A\nPhilosophical Introduction to\nLanguage Models – Part\nI: Continuity With\nClassic Debates.” arXiv. http://arxiv.org/abs/2401.03910.\n\n\nNarayanan, Arvind, and Sayash Kapoor. 2024. AI Snake\nOil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell\nthe Difference. Princeton Oxford: Princeton University Press.\n\n\nNori, Harsha, Nicholas King, Scott Mayer McKinney, Dean Carignan, and\nEric Horvitz. 2023. “Capabilities of GPT-4 on\nMedical Challenge\nProblems.” https://doi.org/10.48550/ARXIV.2303.13375.\n\n\nOh, Hamilton Se-Hwee, Jarod Rutledge, Daniel Nachun, Róbert Pálovics,\nOlamide Abiose, Patricia Moran-Losada, Divya Channappa, et al. 2023.\n“Organ Aging Signatures in the Plasma Proteome Track Health and\nDisease.” Nature 624 (7990): 164–72. https://doi.org/10.1038/s41586-023-06802-1.\n\n\nOren, Yonatan, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and\nTatsunori B. Hashimoto. 2023. “Proving Test\nSet Contamination in Black\nBox Language Models.”\narXiv. http://arxiv.org/abs/2310.17623.\n\n\nPei, Gan, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang,\nZhenyu Zhang, Jian Yang, Chunhua Shen, and Dacheng Tao. 2024.\n“Deepfake Generation and Detection:\nA Benchmark and Survey.”\narXiv. http://arxiv.org/abs/2403.17881.\n\n\nQian, Cheng, Xinran Zhao, and Sherry Tongshuang Wu. 2023.\n“\"Merge Conflicts!\"\nExploring the Impacts of External\nDistractors to Parametric\nKnowledge Graphs.” arXiv. http://arxiv.org/abs/2309.08594.\n\n\nQiu, Pengcheng, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang,\nYa Zhang, Yanfeng Wang, and Weidi Xie. 2024. “Towards\nBuilding Multilingual Language\nModel for Medicine.” arXiv. http://arxiv.org/abs/2402.13963.\n\n\nRaji, Inioluwa Deborah, I. Elizabeth Kumar, Aaron Horowitz, and Andrew\nD. Selbst. 2022. “The Fallacy of AI\nFunctionality.” In 2022 ACM\nConference on Fairness,\nAccountability, and Transparency, 959–72.\nhttps://doi.org/10.1145/3531146.3533158.\n\n\nRao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop\nK Prasad, Adam Landman, Keith Dreyer, and Marc D Succi. 2023.\n“Assessing the Utility of ChatGPT\nThroughout the Entire Clinical\nWorkflow: Development and\nUsability Study.” Journal of\nMedical Internet Research 25 (August): e48659. https://doi.org/10.2196/48659.\n\n\nRomera-Paredes, Bernardino, Mohammadamin Barekatain, Alexander Novikov,\nMatej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, et\nal. 2024. “Mathematical Discoveries from Program Search with Large\nLanguage Models.” Nature 625 (7995): 468–75. https://doi.org/10.1038/s41586-023-06924-6.\n\n\nRöttger, Paul, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck,\nHannah Rose Kirk, Hinrich Schütze, and Dirk Hovy. 2024. “Political\nCompass or Spinning Arrow?\nTowards More Meaningful\nEvaluations for Values and\nOpinions in Large Language\nModels.” arXiv. http://arxiv.org/abs/2402.16786.\n\n\nRozado, David. 2023. “The Political\nBiases of ChatGPT.” Social\nSciences 12 (3): 148. https://doi.org/10.3390/socsci12030148.\n\n\n———. 2024. “The Political Preferences of\nLLMs.” arXiv. http://arxiv.org/abs/2402.01789.\n\n\nSaab, Khaled, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery\nWulczyn, Fan Zhang, et al. 2024. “Capabilities of\nGemini Models in\nMedicine.” arXiv. http://arxiv.org/abs/2404.18416.\n\n\nSastry, Girish, Lennart Heim, Haydn Belfield, Markus Anderljung, Miles\nBrundage, Julian Hazell, Cullen O’Keefe, et al. 2024. “Computing\nPower and the Governance of\nArtificial Intelligence.” arXiv. http://arxiv.org/abs/2402.08797.\n\n\nShumailov, Ilia, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas\nPapernot, and Ross Anderson. 2023. “The Curse of\nRecursion: Training on Generated\nData Makes Models\nForget.” arXiv. http://arxiv.org/abs/2305.17493.\n\n\nSinghal, Karan, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei,\nHyung Won Chung, Nathan Scales, et al. 2023. “Large Language\nModels Encode Clinical Knowledge.” Nature 620 (7972):\n172–80. https://doi.org/10.1038/s41586-023-06291-2.\n\n\nSun, Jie, Qun-Xi Dong, San-Wang Wang, Yong-Bo Zheng, Xiao-Xing Liu,\nTang-Sheng Lu, Kai Yuan, et al. 2023. “Artificial Intelligence in\nPsychiatry Research, Diagnosis, and Therapy.” Asian Journal\nof Psychiatry 87 (September): 103705. https://doi.org/10.1016/j.ajp.2023.103705.\n\n\nTian, Yuzhang, Jianbo Zhao, Haoyu Dong, Junyu Xiong, Shiyu Xia, Mengyu\nZhou, Yun Lin, et al. 2024. “SpreadsheetLLM:\nEncoding Spreadsheets for Large\nLanguage Models.” arXiv. http://arxiv.org/abs/2407.09025.\n\n\nTu, Tao, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg,\nRyutaro Tanno, Amy Wang, et al. 2024. “Towards\nConversational Diagnostic\nAI.” https://doi.org/10.48550/ARXIV.2401.05654.\n\n\nUdandarao, Vishaal, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip H.\nS. Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. “No\n\"Zero-Shot\" Without\nExponential Data: Pretraining\nConcept Frequency Determines\nMultimodal Model\nPerformance.” arXiv. http://arxiv.org/abs/2404.04125.\n\n\nVillalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius\nHobbhahn, and Anson Ho. 2022. “Will We Run Out of Data?\nAn Analysis of the Limits of Scaling Datasets in\nMachine Learning.” arXiv. http://arxiv.org/abs/2211.04325.\n\n\nWang, Chengliang, Xinrun Chen, Haojian Ning, and Shiying Li. 2023.\n“SAM-OCTA: A\nFine-Tuning Strategy for\nApplying Foundation Model to\nOCTA Image Segmentation\nTasks.” arXiv. http://arxiv.org/abs/2309.11758.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,\nFei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.\n“Chain-of-Thought Prompting\nElicits Reasoning in Large\nLanguage Models.” arXiv. http://arxiv.org/abs/2201.11903.\n\n\nWei, Jerry, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin\nTran, Daiyi Peng, et al. 2024. “Long-Form Factuality in Large\nLanguage Models.” arXiv. http://arxiv.org/abs/2403.18802.\n\n\nWeiss, Roy, Daniel Ayzenshteyn, Guy Amit, and Yisroel Mirsky. 2024.\n“What Was Your Prompt?\nA Remote Keylogging\nAttack on AI Assistants.”\narXiv. http://arxiv.org/abs/2403.09751.\n\n\nWendler, Chris, Veniamin Veselovsky, Giovanni Monea, and Robert West.\n2024. “Do Llamas Work in\nEnglish? On the Latent\nLanguage of Multilingual\nTransformers.” arXiv. http://arxiv.org/abs/2402.10588.\n\n\nWornow, Michael, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg,\nScott Fleming, Michael A. Pfeffer, Jason Fries, and Nigam H. Shah. 2023.\n“The Shaky Foundations of Large Language Models and Foundation\nModels for Electronic Health Records.” Npj Digital\nMedicine 6 (1): 135. https://doi.org/10.1038/s41746-023-00879-8.\n\n\nYiu, Eunice, Eliza Kosoy, and Alison Gopnik. 2023. “Transmission\nVersus Truth, Imitation\nVersus Innovation: What\nChildren Can Do That\nLarge Language and\nLanguage-and-Vision Models\nCannot (Yet).” Perspectives on\nPsychological Science, October, 17456916231201401. https://doi.org/10.1177/17456916231201401.\n\n\nYu, Feiyang, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo\nPontes Reis, Eduardo Kaiser Ururahy Nunes Fonseca, et al. 2023.\n“Evaluating Progress in Automatic Chest X-Ray\nRadiology Report Generation.” Patterns 4 (9): 100802. https://doi.org/10.1016/j.patter.2023.100802.\n\n\nZaleski, Amanda L, Rachel Berkowsky, Kelly Jean Thomas Craig, and Linda\nS Pescatello. 2024. “Comprehensiveness, Accuracy, and\nReadability of Exercise\nRecommendations Provided by an\nAI-Based Chatbot:\nMixed Methods Study.”\nJMIR Medical Education 10 (January): e51308. https://doi.org/10.2196/51308.\n\n\nZhao, Theodore, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid\nKiblawi, Tristan Naumann, et al. 2024. “A Foundation Model for\nJoint Segmentation, Detection and Recognition of Biomedical Objects\nAcross Nine Modalities.” Nature Methods, November. https://doi.org/10.1038/s41592-024-02499-w.\n\n\nZhao, Zihao, Yuxiao Liu, Han Wu, Yonghao Li, Sheng Wang, Lin Teng,\nDisheng Liu, et al. 2023. “CLIP in\nMedical Imaging: A\nComprehensive Survey.” arXiv. http://arxiv.org/abs/2312.07353.\n\n\nZheng, Huaixiu Steven, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin\nChen, Azade Nova, Le Hou, et al. 2024. “NATURAL\nPLAN: Benchmarking LLMs on\nNatural Language\nPlanning.” arXiv. http://arxiv.org/abs/2406.04520.\n\n\nZhou, Shuyan, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek\nSridhar, Xianyi Cheng, et al. 2023. “WebArena:\nA Realistic Web\nEnvironment for Building\nAutonomous Agents.” arXiv. http://arxiv.org/abs/2307.13854.",
    "crumbs": [
      "References"
    ]
  }
]