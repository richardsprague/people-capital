
@article{de__fauw_clinically_2018,
	title = {Clinically applicable deep learning for diagnosis and referral in retinal disease},
	volume = {24},
	issn = {1546-170X},
	url = {https://doi.org/10.1038/s41591-018-0107-6},
	doi = {10.1038/s41591-018-0107-6},
	abstract = {The volume and complexity of diagnostic imaging is increasing at a pace faster than the availability of human expertise to interpret it. Artificial intelligence has shown great promise in classifying two-dimensional photographs of some common diseases and typically relies on databases of millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here, we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional optical coherence tomography scans from patients referred to a major eye hospital. We demonstrate performance in making a referral recommendation that reaches or exceeds that of experts on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we demonstrate that the tissue segmentations produced by our architecture act as a device-independent representation; referral accuracy is maintained when using tissue segmentations from a different type of device. Our work removes previous barriers to wider clinical use without prohibitive training data requirements across multiple pathologies in a real-world setting.},
	number = {9},
	journal = {Nature Medicine},
	author = {De  Fauw, Jeffrey and Ledsam, Joseph R. and Romera-Paredes, Bernardino and Nikolov, Stanislav and Tomasev, Nenad and Blackwell, Sam and Askham, Harry and Glorot, Xavier and O’Donoghue, Brendan and Visentin, Daniel and van den  Driessche, George and Lakshminarayanan, Balaji and Meyer, Clemens and Mackinder, Faith and Bouton, Simon and Ayoub, Kareem and Chopra, Reena and King, Dominic and Karthikesalingam, Alan and Hughes, Cían O. and Raine, Rosalind and Hughes, Julian and Sim, Dawn A. and Egan, Catherine and Tufail, Adnan and Montgomery, Hugh and Hassabis, Demis and Rees, Geraint and Back, Trevor and Khaw, Peng T. and Suleyman, Mustafa and Cornebise, Julien and Keane, Pearse A. and Ronneberger, Olaf},
	month = sep,
	year = {2018},
	keywords = {eye},
	pages = {1342--1350},
}

@article{esteva_guide_2019,
	title = {A guide to deep learning in healthcare},
	volume = {25},
	issn = {1546-170X},
	url = {https://doi.org/10.1038/s41591-018-0316-z},
	doi = {10.1038/s41591-018-0316-z},
	abstract = {Here we present deep-learning techniques for healthcare, centering our discussion on deep learning in computer vision, natural language processing, reinforcement learning, and generalized methods. We describe how these computational techniques can impact a few key areas of medicine and explore how to build end-to-end systems. Our discussion of computer vision focuses largely on medical imaging, and we describe the application of natural language processing to domains such as electronic health record data. Similarly, reinforcement learning is discussed in the context of robotic-assisted surgery, and generalized deep-learning methods for genomics are reviewed.},
	number = {1},
	journal = {Nature Medicine},
	author = {Esteva, Andre and Robicquet, Alexandre and Ramsundar, Bharath and Kuleshov, Volodymyr and DePristo, Mark and Chou, Katherine and Cui, Claire and Corrado, Greg and Thrun, Sebastian and Dean, Jeff},
	month = jan,
	year = {2019},
	pages = {24--29},
	file = {Esteva et al. - 2019 - A guide to deep learning in healthcare.pdf:/Users/sprague/Zotero/storage/F35ZKUTC/Esteva et al. - 2019 - A guide to deep learning in healthcare.pdf:application/pdf},
}

@article{lee_deep_2017,
	title = {Deep {Learning} {Is} {Effective} for {Classifying} {Normal} versus {Age}-{Related} {Macular} {Degeneration} {OCT} {Images}},
	volume = {1},
	issn = {24686530},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2468653016301749},
	doi = {10.1016/j.oret.2016.12.009},
	language = {en},
	number = {4},
	urldate = {2019-02-25},
	journal = {Ophthalmology Retina},
	author = {Lee, Cecilia S. and Baughman, Doug M. and Lee, Aaron Y.},
	month = jul,
	year = {2017},
	pages = {322--327},
}

@misc{leivada_dall-e_2022,
	title = {{DALL}-{E} 2 {Fails} to {Reliably} {Capture} {Common} {Syntactic} {Processes}},
	url = {http://arxiv.org/abs/2210.12889},
	abstract = {Machine intelligence is increasingly being linked to claims about sentience, language processing, and an ability to comprehend and transform natural language into a range of stimuli. We systematically analyze the ability of DALL-E 2 to capture 8 grammatical phenomena pertaining to compositionality that are widely discussed in linguistics and pervasive in human language: binding principles and coreference, passives, word order, coordination, comparatives, negation, ellipsis, and structural ambiguity. Whereas young children routinely master these phenomena, learning systematic mappings between syntax and semantics, DALL-E 2 is unable to reliably infer meanings that are consistent with the syntax. These results challenge recent claims concerning the capacity of such systems to understand of human language. We make available the full set of test materials as a benchmark for future testing.},
	urldate = {2022-10-29},
	publisher = {arXiv},
	author = {Leivada, Evelina and Murphy, Elliot and Marcus, Gary},
	month = oct,
	year = {2022},
	note = {arXiv:2210.12889 [cs]},
	keywords = {linguistics, ai},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/6ZUBP7LC/Leivada et al. - 2022 - DALL-E 2 Fails to Reliably Capture Common Syntacti.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/WBFDFM2Z/2210.html:text/html},
}

@techreport{kung_performance_2022,
	type = {preprint},
	title = {Performance of {ChatGPT} on {USMLE}: {Potential} for {AI}-{Assisted} {Medical} {Education} {Using} {Large} {Language} {Models}},
	shorttitle = {Performance of {ChatGPT} on {USMLE}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2022.12.19.22283643},
	abstract = {ABSTRACT
          We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.},
	language = {en},
	urldate = {2023-01-23},
	institution = {Medical Education},
	author = {Kung, Tiffany H. and Cheatham, Morgan and {ChatGPT} and Medenilla, Arielle and Sillos, Czarina and De Leon, Lorie and Elepaño, Camille and Madriaga, Maria and Aggabao, Rimel and Diaz-Candido, Giezel and Maningo, James and Tseng, Victor},
	month = dec,
	year = {2022},
	doi = {10.1101/2022.12.19.22283643},
	keywords = {ai},
	file = {Submitted Version:/Users/sprague/Zotero/storage/GVGULS6S/Kung et al. - 2022 - Performance of ChatGPT on USMLE Potential for AI-.pdf:application/pdf},
}

@article{luo_biogpt_2022,
	title = {{BioGPT}: generative pre-trained transformer for biomedical text generation and mining},
	volume = {23},
	issn = {1467-5463, 1477-4054},
	shorttitle = {{BioGPT}},
	url = {https://academic.oup.com/bib/article/doi/10.1093/bib/bbac409/6713511},
	doi = {10.1093/bib/bbac409},
	abstract = {Abstract
            Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\%, 38.42\% and 40.76\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.},
	language = {en},
	number = {6},
	urldate = {2023-02-01},
	journal = {Briefings in Bioinformatics},
	author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
	month = nov,
	year = {2022},
	pages = {bbac409},
	file = {Submitted Version:/Users/sprague/Zotero/storage/275PFR46/Luo et al. - 2022 - BioGPT generative pre-trained transformer for bio.pdf:application/pdf},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
	keywords = {linguistics, ai, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/SVK7XMLP/Mahowald et al. - 2023 - Dissociating language and thought in large languag.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/NSTBJWPX/2301.html:text/html},
}

@book{lee_ai_2023,
	address = {Hoboken},
	edition = {1},
	title = {The {AI} revolution in medicine: {GPT}-4 and beyond},
	isbn = {978-0-13-820013-8},
	shorttitle = {The {AI} revolution in medicine},
	publisher = {Pearson},
	author = {Lee, Peter and Goldberg, Carey and Kohane, Isaac},
	year = {2023},
	keywords = {healthcare, ai},
}

@article{drazen_benefits_2023,
	title = {Benefits, {Limits}, and {Risks} of {GPT}-4 as an {AI} {Chatbot} for {Medicine}},
	volume = {388},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMsr2214184},
	doi = {10.1056/NEJMsr2214184},
	language = {en},
	number = {13},
	urldate = {2023-05-09},
	journal = {New England Journal of Medicine},
	author = {Lee, Peter and Bubeck, Sebastien and Petro, Joseph},
	editor = {Drazen, Jeffrey M. and Kohane, Isaac S. and Leong, Tze-Yun},
	month = mar,
	year = {2023},
	keywords = {ai},
	pages = {1233--1239},
	file = {Lee et al. - 2023 - Benefits, Limits, and Risks of GPT-4 as an AI Chat.pdf:/Users/sprague/Zotero/storage/IY8NLVWW/Lee et al. - 2023 - Benefits, Limits, and Risks of GPT-4 as an AI Chat.pdf:application/pdf},
}

@article{chang_speak_2023,
	title = {Speak, {Memory}: {An} {Archaeology} of {Books} {Known} to {ChatGPT}/{GPT}-4},
	copyright = {Creative Commons Attribution Share Alike 4.0 International},
	shorttitle = {Speak, {Memory}},
	url = {https://arxiv.org/abs/2305.00118},
	doi = {10.48550/ARXIV.2305.00118},
	abstract = {In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks. We argue that this supports a case for open models whose training data is known.},
	urldate = {2023-07-16},
	author = {Chang, Kent K. and Cramer, Mackenzie and Soni, Sandeep and Bamman, David},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {ai, Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{dodge_documenting_2021,
	title = {Documenting {Large} {Webtext} {Corpora}: {A} {Case} {Study} on the {Colossal} {Clean} {Crawled} {Corpus}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Documenting {Large} {Webtext} {Corpora}},
	url = {https://arxiv.org/abs/2104.08758},
	doi = {10.48550/ARXIV.2104.08758},
	abstract = {Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.},
	urldate = {2023-07-16},
	author = {Dodge, Jesse and Sap, Maarten and Marasović, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Artificial Intelligence (cs.AI)},
}

@article{rozado_political_2023,
	title = {The {Political} {Biases} of {ChatGPT}},
	volume = {12},
	issn = {2076-0760},
	url = {https://www.mdpi.com/2076-0760/12/3/148},
	doi = {10.3390/socsci12030148},
	abstract = {Recent advancements in Large Language Models (LLMs) suggest imminent commercial applications of such AI systems where they will serve as gateways to interact with technology and the accumulated body of human knowledge. The possibility of political biases embedded in these models raises concerns about their potential misusage. In this work, we report the results of administering 15 different political orientation tests (14 in English, 1 in Spanish) to a state-of-the-art Large Language Model, the popular ChatGPT from OpenAI. The results are consistent across tests; 14 of the 15 instruments diagnose ChatGPT answers to their questions as manifesting a preference for left-leaning viewpoints. When asked explicitly about its political preferences, ChatGPT often claims to hold no political opinions and to just strive to provide factual and neutral information. It is desirable that public facing artificial intelligence systems provide accurate and factual information about empirically verifiable issues, but such systems should strive for political neutrality on largely normative questions for which there is no straightforward way to empirically validate a viewpoint. Thus, ethical AI systems should present users with balanced arguments on the issue at hand and avoid claiming neutrality while displaying clear signs of political bias in their content.},
	language = {en},
	number = {3},
	urldate = {2023-07-16},
	journal = {Social Sciences},
	author = {Rozado, David},
	month = mar,
	year = {2023},
	pages = {148},
	file = {Full Text:/Users/sprague/Zotero/storage/Z2Q22RR4/Rozado - 2023 - The Political Biases of ChatGPT.pdf:application/pdf},
}

@misc{liu_evaluating_2023,
	title = {Evaluating {Verifiability} in {Generative} {Search} {Engines}},
	url = {http://arxiv.org/abs/2304.09848},
	abstract = {Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5\% of generated sentences are fully supported by citations and only 74.5\% of citations support their associated sentence. We believe that these results are concerningly low for systems that may serve as a primary tool for information-seeking users, especially given their facade of trustworthiness. We hope that our results further motivate the development of trustworthy generative search engines and help researchers and users better understand the shortcomings of existing commercial systems.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Zhang, Tianyi and Liang, Percy},
	month = apr,
	year = {2023},
	note = {arXiv:2304.09848 [cs]},
	keywords = {ai, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/HKUWGGQR/Liu et al. - 2023 - Evaluating Verifiability in Generative Search Engi.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/ISBE4SU8/2304.html:text/html},
}

@article{mesko_imperative_2023,
	title = {The imperative for regulatory oversight of large language models (or generative {AI}) in healthcare},
	volume = {6},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-023-00873-0},
	doi = {10.1038/s41746-023-00873-0},
	abstract = {Abstract
            The rapid advancements in artificial intelligence (AI) have led to the development of sophisticated large language models (LLMs) such as GPT-4 and Bard. The potential implementation of LLMs in healthcare settings has already garnered considerable attention because of their diverse applications that include facilitating clinical documentation, obtaining insurance pre-authorization, summarizing research papers, or working as a chatbot to answer questions for patients about their specific data and concerns. While offering transformative potential, LLMs warrant a very cautious approach since these models are trained differently from AI-based medical technologies that are regulated already, especially within the critical context of caring for patients. The newest version, GPT-4, that was released in March, 2023, brings the potentials of this technology to support multiple medical tasks; and risks from mishandling results it provides to varying reliability to a new level. Besides being an advanced LLM, it will be able to read texts on images and analyze the context of those images. The regulation of GPT-4 and generative AI in medicine and healthcare without damaging their exciting and transformative potential is a timely and critical challenge to ensure safety, maintain ethical standards, and protect patient privacy. We argue that regulatory oversight should assure medical professionals and patients can use LLMs without causing harm or compromising their data or privacy. This paper summarizes our practical recommendations for what we can expect from regulators to bring this vision to reality.},
	language = {en},
	number = {1},
	urldate = {2023-07-29},
	journal = {npj Digital Medicine},
	author = {Meskó, Bertalan and Topol, Eric J.},
	month = jul,
	year = {2023},
	pages = {120},
	file = {Full Text:/Users/sprague/Zotero/storage/FPBDUR3P/Meskó and Topol - 2023 - The imperative for regulatory oversight of large l.pdf:application/pdf},
}

@article{nori_capabilities_2023,
	title = {Capabilities of {GPT}-4 on {Medical} {Challenge} {Problems}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.13375},
	doi = {10.48550/ARXIV.2303.13375},
	abstract = {Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications like medicine. Our results show that GPT-4, without any specialized prompt crafting, exceeds the passing score on USMLE by over 20 points and outperforms earlier general-purpose models (GPT-3.5) as well as models specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B). In addition, GPT-4 is significantly better calibrated than GPT-3.5, demonstrating a much-improved ability to predict the likelihood that its answers are correct. We also explore the behavior of the model qualitatively through a case study that shows the ability of GPT-4 to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. Implications of the findings are discussed for potential uses of GPT-4 in medical education, assessment, and clinical practice, with appropriate attention to challenges of accuracy and safety.},
	urldate = {2023-08-06},
	author = {Nori, Harsha and King, Nicholas and McKinney, Scott Mayer and Carignan, Dean and Horvitz, Eric},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Artificial Intelligence (cs.AI)},
}

@article{wornow_shaky_2023,
	title = {The shaky foundations of large language models and foundation models for electronic health records},
	volume = {6},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-023-00879-8},
	doi = {10.1038/s41746-023-00879-8},
	abstract = {Abstract
            The success of foundation models such as ChatGPT and AlphaFold has spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models’ capabilities. In this narrative review, we examine 84 foundation models trained on non-imaging EMR data (i.e., clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g., MIMIC-III) or broad, public biomedical corpora (e.g., PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. Considering these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.},
	language = {en},
	number = {1},
	urldate = {2023-08-06},
	journal = {npj Digital Medicine},
	author = {Wornow, Michael and Xu, Yizhe and Thapa, Rahul and Patel, Birju and Steinberg, Ethan and Fleming, Scott and Pfeffer, Michael A. and Fries, Jason and Shah, Nigam H.},
	month = jul,
	year = {2023},
	pages = {135},
	file = {Full Text:/Users/sprague/Zotero/storage/YSVWGMTB/Wornow et al. - 2023 - The shaky foundations of large language models and.pdf:application/pdf},
}

@article{hristidis_chatgpt_2023,
	title = {{ChatGPT} vs {Google} for {Queries} {Related} to {Dementia} and {Other} {Cognitive} {Decline}: {Comparison} of {Results}},
	volume = {25},
	issn = {1438-8871},
	shorttitle = {{ChatGPT} vs {Google} for {Queries} {Related} to {Dementia} and {Other} {Cognitive} {Decline}},
	url = {https://www.jmir.org/2023/1/e48966},
	doi = {10.2196/48966},
	abstract = {Background
              People living with dementia or other cognitive decline and their caregivers (PLWD) increasingly rely on the web to find information about their condition and available resources and services. The recent advancements in large language models (LLMs), such as ChatGPT, provide a new alternative to the more traditional web search engines, such as Google.
            
            
              Objective
              This study compared the quality of the results of ChatGPT and Google for a collection of PLWD-related queries.
            
            
              Methods
              A set of 30 informational and 30 service delivery (transactional) PLWD-related queries were selected and submitted to both Google and ChatGPT. Three domain experts assessed the results for their currency of information, reliability of the source, objectivity, relevance to the query, and similarity of their response. The readability of the results was also analyzed. Interrater reliability coefficients were calculated for all outcomes.
            
            
              Results
              Google had superior currency and higher reliability. ChatGPT results were evaluated as more objective. ChatGPT had a significantly higher response relevance, while Google often drew upon sources that were referral services for dementia care or service providers themselves. The readability was low for both platforms, especially for ChatGPT (mean grade level 12.17, SD 1.94) compared to Google (mean grade level 9.86, SD 3.47). The similarity between the content of ChatGPT and Google responses was rated as high for 13 (21.7\%) responses, medium for 16 (26.7\%) responses, and low for 31 (51.6\%) responses.
            
            
              Conclusions
              Both Google and ChatGPT have strengths and weaknesses. ChatGPT rarely includes the source of a result. Google more often provides a date for and a known reliable source of the response compared to ChatGPT, whereas ChatGPT supplies more relevant responses to queries. The results of ChatGPT may be out of date and often do not specify a validity time stamp. Google sometimes returns results based on commercial entities. The readability scores for both indicate that responses are often not appropriate for persons with low health literacy skills. In the future, the addition of both the source and the date of health-related information and availability in other languages may increase the value of these platforms for both nonmedical and medical professionals.},
	language = {en},
	urldate = {2023-08-07},
	journal = {Journal of Medical Internet Research},
	author = {Hristidis, Vagelis and Ruggiano, Nicole and Brown, Ellen L and Ganta, Sai Rithesh Reddy and Stewart, Selena},
	month = jul,
	year = {2023},
	pages = {e48966},
}

@article{kaddour_challenges_2023,
	title = {Challenges and {Applications} of {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2307.10169},
	doi = {10.48550/ARXIV.2307.10169},
	abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
	urldate = {2023-08-07},
	author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Machine Learning (cs.LG)},
}

@article{singhal_large_2023,
	title = {Large language models encode clinical knowledge},
	volume = {620},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06291-2},
	doi = {10.1038/s41586-023-06291-2},
	abstract = {Abstract
            
              Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model
              1
              (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM
              2
              on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA
              3
              , MedMCQA
              4
              , PubMedQA
              5
              and Measuring Massive Multitask Language Understanding (MMLU) clinical topics
              6
              ), including 67.6\% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17\%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
	language = {en},
	number = {7972},
	urldate = {2023-08-08},
	journal = {Nature},
	author = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Babiker, Abubakr and Schärli, Nathanael and Chowdhery, Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and Agüera Y Arcas, Blaise and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
	month = aug,
	year = {2023},
	pages = {172--180},
	file = {Full Text:/Users/sprague/Zotero/storage/84WFJWZ8/Singhal et al. - 2023 - Large language models encode clinical knowledge.pdf:application/pdf},
}

@article{kanjee_accuracy_2023,
	title = {Accuracy of a {Generative} {Artificial} {Intelligence} {Model} in a {Complex} {Diagnostic} {Challenge}},
	volume = {330},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2806457},
	doi = {10.1001/jama.2023.8288},
	abstract = {This study assesses the diagnostic accuracy of the Generative Pre-trained Transformer 4 (GPT-4) artificial intelligence (AI) model in a series of challenging cases.},
	language = {en},
	number = {1},
	urldate = {2023-08-08},
	journal = {JAMA},
	author = {Kanjee, Zahir and Crowe, Byron and Rodman, Adam},
	month = jul,
	year = {2023},
	pages = {78},
}

@article{feng_pretraining_2023,
	title = {From {Pretraining} {Data} to {Language} {Models} to {Downstream} {Tasks}: {Tracking} the {Trails} of {Political} {Biases} {Leading} to {Unfair} {NLP} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {From {Pretraining} {Data} to {Language} {Models} to {Downstream} {Tasks}},
	url = {https://arxiv.org/abs/2305.08283},
	doi = {10.48550/ARXIV.2305.08283},
	abstract = {Language models (LMs) are pretrained on diverse data sources, including news, discussion forums, books, and online encyclopedias. A significant portion of this data includes opinions and perspectives which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure political biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings that reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.},
	urldate = {2023-08-08},
	author = {Feng, Shangbin and Park, Chan Young and Liu, Yuhan and Tsvetkov, Yulia},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{ma_lets_2023,
	title = {Let's {Do} a {Thought} {Experiment}: {Using} {Counterfactuals} to {Improve} {Moral} {Reasoning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Let's {Do} a {Thought} {Experiment}},
	url = {https://arxiv.org/abs/2306.14308},
	doi = {10.48550/ARXIV.2306.14308},
	abstract = {Language models still struggle on moral reasoning, despite their impressive performance in many other tasks. In particular, the Moral Scenarios task in MMLU (Multi-task Language Understanding) is among the worst performing tasks for many language models, including GPT-3. In this work, we propose a new prompting framework, Thought Experiments, to teach language models to do better moral reasoning using counterfactuals. Experiment results show that our framework elicits counterfactual questions and answers from the model, which in turn helps improve the accuracy on Moral Scenarios task by 9-16\% compared to other zero-shot baselines. Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4\% compared to direct zero-shot. We further observed that with minimal human supervision in the form of 5 few-shot examples, the accuracy of the task can be improved to as much as 80\%.},
	urldate = {2023-08-08},
	author = {Ma, Xiao and Mishra, Swaroop and Beirami, Ahmad and Beutel, Alex and Chen, Jilin},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Artificial Intelligence (cs.AI)},
}

@article{jin_darkbert_2023,
	title = {{DarkBERT}: {A} {Language} {Model} for the {Dark} {Side} of the {Internet}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{DarkBERT}},
	url = {https://arxiv.org/abs/2305.08596},
	doi = {10.48550/ARXIV.2305.08596},
	abstract = {Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web.},
	urldate = {2023-08-08},
	author = {Jin, Youngjin and Jang, Eugene and Cui, Jian and Chung, Jin-Woo and Lee, Yongjae and Shin, Seungwon},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, I.2.7},
}

@article{liu_agentbench_2023,
	title = {{AgentBench}: {Evaluating} {LLMs} as {Agents}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {{AgentBench}},
	url = {https://arxiv.org/abs/2308.03688},
	doi = {10.48550/ARXIV.2308.03688},
	abstract = {Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench},
	urldate = {2023-08-14},
	author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Machine Learning (cs.LG), AI},
}

@article{butlin_consciousness_2023,
	title = {Consciousness in {Artificial} {Intelligence}: {Insights} from the {Science} of {Consciousness}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {Consciousness in {Artificial} {Intelligence}},
	url = {https://arxiv.org/abs/2308.08708},
	doi = {10.48550/ARXIV.2308.08708},
	abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.},
	urldate = {2023-08-21},
	author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {ai, FOS: Biological sciences, FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Computers and Society (cs.CY), Neurons and Cognition (q-bio.NC), consciousness, neuroscience},
}

@misc{lenat_getting_2023,
	title = {Getting from {Generative} {AI} to {Trustworthy} {AI}: {What} {LLMs} might learn from {Cyc}},
	shorttitle = {Getting from {Generative} {AI} to {Trustworthy} {AI}},
	url = {http://arxiv.org/abs/2308.04445},
	abstract = {Generative AI, the most popular current approach to AI, consists of large language models (LLMs) that are trained to produce outputs that are plausible, but not necessarily correct. Although their abilities are often uncanny, they are lacking in aspects of reasoning, leading LLMs to be less than completely trustworthy. Furthermore, their results tend to be both unpredictable and uninterpretable. We lay out 16 desiderata for future AI, and discuss an alternative approach to AI which could theoretically address many of the limitations associated with current approaches: AI educated with curated pieces of explicit knowledge and rules of thumb, enabling an inference engine to automatically deduce the logical entailments of all that knowledge. Even long arguments produced this way can be both trustworthy and interpretable, since the full step-by-step line of reasoning is always available, and for each step the provenance of the knowledge used can be documented and audited. There is however a catch: if the logical language is expressive enough to fully represent the meaning of anything we can say in English, then the inference engine runs much too slowly. That's why symbolic AI systems typically settle for some fast but much less expressive logic, such as knowledge graphs. We describe how one AI system, Cyc, has developed ways to overcome that tradeoff and is able to reason in higher order logic in real time. We suggest that any trustworthy general AI will need to hybridize the approaches, the LLM approach and more formal approach, and lay out a path to realizing that dream.},
	urldate = {2023-09-02},
	publisher = {arXiv},
	author = {Lenat, Doug and Marcus, Gary},
	month = jul,
	year = {2023},
	note = {arXiv:2308.04445 [cs]},
	keywords = {Computer Science - Artificial Intelligence, AI, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/YHQW33CP/Lenat and Marcus - 2023 - Getting from Generative AI to Trustworthy AI What.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/J84HYHYX/2308.html:text/html},
}

@article{grossmann_ai_2023,
	title = {{AI} and the transformation of social science research},
	volume = {380},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.adi1778},
	doi = {10.1126/science.adi1778},
	abstract = {Careful bias management and data fidelity are key
          , 
            
              Advances in artificial intelligence (AI), particularly large language models (LLMs), are substantially affecting social science research. These transformer-based machine-learning models pretrained on vast amounts of text data are increasingly capable of simulating human-like responses and behaviors (
              
                1
              
              ,
              
                2
              
              ), offering opportunities to test theories and hypotheses about human behavior at great scale and speed. This presents urgent challenges: How can social science research practices be adapted, even reinvented, to harness the power of foundational AI? And how can this be done while ensuring transparent and replicable research?},
	language = {en},
	number = {6650},
	urldate = {2023-09-05},
	journal = {Science},
	author = {Grossmann, Igor and Feinberg, Matthew and Parker, Dawn C. and Christakis, Nicholas A. and Tetlock, Philip E. and Cunningham, William A.},
	month = jun,
	year = {2023},
	pages = {1108--1109},
}

@article{rao_assessing_2023,
	title = {Assessing the {Utility} of {ChatGPT} {Throughout} the {Entire} {Clinical} {Workflow}: {Development} and {Usability} {Study}},
	volume = {25},
	issn = {1438-8871},
	shorttitle = {Assessing the {Utility} of {ChatGPT} {Throughout} the {Entire} {Clinical} {Workflow}},
	url = {https://www.jmir.org/2023/1/e48659},
	doi = {10.2196/48659},
	abstract = {Background
              Large language model (LLM)–based artificial intelligence chatbots direct the power of large training data sets toward successive, related tasks as opposed to single-ask tasks, for which artificial intelligence already achieves impressive performance. The capacity of LLMs to assist in the full scope of iterative clinical reasoning via successive prompting, in effect acting as artificial physicians, has not yet been evaluated.
            
            
              Objective
              This study aimed to evaluate ChatGPT’s capacity for ongoing clinical decision support via its performance on standardized clinical vignettes.
            
            
              Methods
              We inputted all 36 published clinical vignettes from the Merck Sharpe \& Dohme (MSD) Clinical Manual into ChatGPT and compared its accuracy on differential diagnoses, diagnostic testing, final diagnosis, and management based on patient age, gender, and case acuity. Accuracy was measured by the proportion of correct responses to the questions posed within the clinical vignettes tested, as calculated by human scorers. We further conducted linear regression to assess the contributing factors toward ChatGPT’s performance on clinical tasks.
            
            
              Results
              ChatGPT achieved an overall accuracy of 71.7\% (95\% CI 69.3\%-74.1\%) across all 36 clinical vignettes. The LLM demonstrated the highest performance in making a final diagnosis with an accuracy of 76.9\% (95\% CI 67.8\%-86.1\%) and the lowest performance in generating an initial differential diagnosis with an accuracy of 60.3\% (95\% CI 54.2\%-66.6\%). Compared to answering questions about general medical knowledge, ChatGPT demonstrated inferior performance on differential diagnosis (β=–15.8\%; P{\textless}.001) and clinical management (β=–7.4\%; P=.02) question types.
            
            
              Conclusions
              ChatGPT achieves impressive accuracy in clinical decision-making, with increasing strength as it gains more clinical information at its disposal. In particular, ChatGPT demonstrates the greatest accuracy in tasks of final diagnosis as compared to initial diagnosis. Limitations include possible model hallucinations and the unclear composition of ChatGPT’s training data set.},
	language = {en},
	urldate = {2023-09-07},
	journal = {Journal of Medical Internet Research},
	author = {Rao, Arya and Pang, Michael and Kim, John and Kamineni, Meghana and Lie, Winston and Prasad, Anoop K and Landman, Adam and Dreyer, Keith and Succi, Marc D},
	month = aug,
	year = {2023},
	pages = {e48659},
	file = {Full Text:/Users/sprague/Zotero/storage/J4RSWP7P/Rao et al. - 2023 - Assessing the Utility of ChatGPT Throughout the En.pdf:application/pdf},
}

@misc{di_palma_evaluating_2023,
	title = {Evaluating {ChatGPT} as a {Recommender} {System}: {A} {Rigorous} {Approach}},
	shorttitle = {Evaluating {ChatGPT} as a {Recommender} {System}},
	url = {http://arxiv.org/abs/2309.03613},
	abstract = {Recent popularity surrounds large AI language models due to their impressive natural language capabilities. They contribute significantly to language-related tasks, including prompt-based learning, making them valuable for various specific tasks. This approach unlocks their full potential, enhancing precision and generalization. Research communities are actively exploring their applications, with ChatGPT receiving recognition. Despite extensive research on large language models, their potential in recommendation scenarios still needs to be explored. This study aims to fill this gap by investigating ChatGPT's capabilities as a zero-shot recommender system. Our goals include evaluating its ability to use user preferences for recommendations, reordering existing recommendation lists, leveraging information from similar users, and handling cold-start situations. We assess ChatGPT's performance through comprehensive experiments using three datasets (MovieLens Small, Last.FM, and Facebook Book). We compare ChatGPT's performance against standard recommendation algorithms and other large language models, such as GPT-3.5 and PaLM-2. To measure recommendation effectiveness, we employ widely-used evaluation metrics like Mean Average Precision (MAP), Recall, Precision, F1, normalized Discounted Cumulative Gain (nDCG), Item Coverage, Expected Popularity Complement (EPC), Average Coverage of Long Tail (ACLT), Average Recommendation Popularity (ARP), and Popularity-based Ranking-based Equal Opportunity (PopREO). Through thoroughly exploring ChatGPT's abilities in recommender systems, our study aims to contribute to the growing body of research on the versatility and potential applications of large language models. Our experiment code is available on the GitHub repository: https://github.com/sisinflab/Recommender-ChatGPT},
	urldate = {2023-09-11},
	publisher = {arXiv},
	author = {Di Palma, Dario and Biancofiore, Giovanni Maria and Anelli, Vito Walter and Narducci, Fedelucio and Di Noia, Tommaso and Di Sciascio, Eugenio},
	month = sep,
	year = {2023},
	note = {arXiv:2309.03613 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/YN5NEMUW/Di Palma et al. - 2023 - Evaluating ChatGPT as a Recommender System A Rigo.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/5BP8QLRN/2309.html:text/html},
}

@misc{gupta_calm_2023,
	title = {{CALM} : {A} {Multi}-task {Benchmark} for {Comprehensive} {Assessment} of {Language} {Model} {Bias}},
	shorttitle = {{CALM}},
	url = {http://arxiv.org/abs/2308.12539},
	abstract = {As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language models including six prominent families of LMs such as Llama-2. In two LM series, OPT and Bloom, we found that larger parameter models are more biased than lower parameter models. We found the T0 series of models to be the least biased. Furthermore, we noticed a tradeoff between gender and racial bias with increasing model size in some model series. The code is available at https://github.com/vipulgupta1011/CALM.},
	urldate = {2023-09-15},
	publisher = {arXiv},
	author = {Gupta, Vipul and Venkit, Pranav Narayanan and Laurençon, Hugo and Wilson, Shomir and Passonneau, Rebecca J.},
	month = aug,
	year = {2023},
	note = {arXiv:2308.12539 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/XISICJG8/Gupta et al. - 2023 - CALM  A Multi-task Benchmark for Comprehensive As.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/SSTZUY7D/2308.html:text/html},
}

@techreport{alamdari_protein_2023,
	type = {preprint},
	title = {Protein generation with evolutionary diffusion: sequence is all you need},
	shorttitle = {Protein generation with evolutionary diffusion},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.09.11.556673},
	abstract = {Deep generative models are increasingly powerful tools for the in silico design of novel proteins. Recently, a family of generative models called diffusion models has demonstrated the ability to generate biologically plausible proteins that are dissimilar to any actual proteins seen in nature, enabling unprecedented capability and control in de novo protein design. However, current state-of-the-art models generate protein structures, which severely limits the scope of their training data and restricts generations to a small and biased subset of protein design space. Here, we introduce a general-purpose diffusion framework, EvoDiff, that combines evolutionary-scale data with the distinct conditioning capabilities of diffusion models for controllable protein generation in sequence space. EvoDiff generates high-fidelity, diverse, and structurally-plausible proteins that cover natural sequence and functional space. Critically, EvoDiff can generate proteins inaccessible to structure-based models, such as those with disordered regions, while maintaining the ability to design scaffolds for functional structural motifs, demonstrating the universality of our sequence-based formulation. We envision that EvoDiff will expand capabilities in protein engineering beyond the structure-function paradigm toward programmable, sequence-first design.},
	language = {en},
	urldate = {2023-09-15},
	institution = {Bioengineering},
	author = {Alamdari, Sarah and Thakkar, Nitya and Van Den Berg, Rianne and Lu, Alex Xijie and Fusi, Nicolo and Amini, Ava Pardis and Yang, Kevin K},
	month = sep,
	year = {2023},
	doi = {10.1101/2023.09.11.556673},
	file = {Submitted Version:/Users/sprague/Zotero/storage/YRMYWI38/Alamdari et al. - 2023 - Protein generation with evolutionary diffusion se.pdf:application/pdf},
}

@inproceedings{raji_fallacy_2022,
	title = {The {Fallacy} of {AI} {Functionality}},
	url = {http://arxiv.org/abs/2206.09511},
	doi = {10.1145/3531146.3533158},
	abstract = {Deployed AI systems often do not work. They can be constructed haphazardly, deployed indiscriminately, and promoted deceptively. However, despite this reality, scholars, the press, and policymakers pay too little attention to functionality. This leads to technical and policy solutions focused on "ethical" or value-aligned deployments, often skipping over the prior question of whether a given system functions, or provides any benefits at all. To describe the harms of various types of functionality failures, we analyze a set of case studies to create a taxonomy of known AI functionality issues. We then point to policy and organizational responses that are often overlooked and become more readily available once functionality is drawn into focus. We argue that functionality is a meaningful AI policy challenge, operating as a necessary first step towards protecting affected communities from algorithmic harm.},
	urldate = {2023-09-19},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Raji, Inioluwa Deborah and Kumar, I. Elizabeth and Horowitz, Aaron and Selbst, Andrew D.},
	month = jun,
	year = {2022},
	note = {arXiv:2206.09511 [cs]},
	keywords = {AI, Computer Science - Machine Learning},
	pages = {959--972},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/RPJUZHA6/Raji et al. - 2022 - The Fallacy of AI Functionality.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/AS6C6JZH/2206.html:text/html},
}

@misc{qian_merge_2023,
	title = {"{Merge} {Conflicts}!" {Exploring} the {Impacts} of {External} {Distractors} to {Parametric} {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2309.08594},
	abstract = {Large language models (LLMs) acquire extensive knowledge during pre-training, known as their parametric knowledge. However, in order to remain up-to-date and align with human instructions, LLMs inevitably require external knowledge during their interactions with users. This raises a crucial question: How will LLMs respond when external knowledge interferes with their parametric knowledge? To investigate this question, we propose a framework that systematically elicits LLM parametric knowledge and introduces external knowledge. Specifically, we uncover the impacts by constructing a parametric knowledge graph to reveal the different knowledge structures of LLMs, and introduce external knowledge through distractors of varying degrees, methods, positions, and formats. Our experiments on both black-box and open-source models demonstrate that LLMs tend to produce responses that deviate from their parametric knowledge, particularly when they encounter direct conflicts or confounding changes of information within detailed contexts. We also find that while LLMs are sensitive to the veracity of external knowledge, they can still be distracted by unrelated information. These findings highlight the risk of hallucination when integrating external knowledge, even indirectly, during interactions with current LLMs. All the data and results are publicly available.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Qian, Cheng and Zhao, Xinran and Wu, Sherry Tongshuang},
	month = sep,
	year = {2023},
	note = {arXiv:2309.08594 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/UTMWYCQ4/Qian et al. - 2023 - Merge Conflicts! Exploring the Impacts of Extern.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/CMDGTD4T/2309.html:text/html},
}

@misc{manathunga_aligning_2023,
	title = {Aligning {Large} {Language} {Models} for {Clinical} {Tasks}},
	url = {http://arxiv.org/abs/2309.02884},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable adaptability, showcasing their capacity to excel in tasks for which they were not explicitly trained. However, despite their impressive natural language processing (NLP) capabilities, effective alignment of LLMs remains a crucial challenge when deploying them for specific clinical applications. The ability to generate responses with factually accurate content and to engage in non-trivial reasoning steps are crucial for the LLMs to be eligible for applications in clinical medicine. Employing a combination of techniques including instruction-tuning and in-prompt strategies like few-shot and chain-of-thought prompting has significantly enhanced the performance of LLMs. Our proposed alignment strategy for medical question-answering, known as 'expand-guess-refine', offers a parameter and data-efficient solution. A preliminary analysis of this method demonstrated outstanding performance, achieving a score of 70.63\% on a subset of questions sourced from the USMLE dataset.},
	urldate = {2023-09-21},
	publisher = {arXiv},
	author = {Manathunga, Supun and Hettigoda, Isuru},
	month = sep,
	year = {2023},
	note = {arXiv:2309.02884 [cs]},
	keywords = {Computer Science - Computation and Language, I.2, I.7, J.3},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/DQ2NJISI/Manathunga and Hettigoda - 2023 - Aligning Large Language Models for Clinical Tasks.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/3LRRPEAZ/2309.html:text/html},
}

@misc{berglund_reversal_2023,
	title = {The {Reversal} {Curse}: {LLMs} trained on "{A} is {B}" fail to learn "{B} is {A}"},
	shorttitle = {The {Reversal} {Curse}},
	url = {http://arxiv.org/abs/2309.12288},
	abstract = {We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79\% of the time, compared to 33\% for the latter. This shows a failure of logical deduction that we hypothesize is caused by the Reversal Curse. Code is available at https://github.com/lukasberglund/reversal\_curse.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
	month = sep,
	year = {2023},
	note = {arXiv:2309.12288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/NGP6GDTZ/Berglund et al. - 2023 - The Reversal Curse LLMs trained on A is B fail .pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/RZ5SZWXA/2309.html:text/html},
}

@article{mesko_prompt_2023,
	title = {Prompt {Engineering} as an {Important} {Emerging} {Skill} for {Medical} {Professionals}: {Tutorial}},
	volume = {25},
	issn = {1438-8871},
	shorttitle = {Prompt {Engineering} as an {Important} {Emerging} {Skill} for {Medical} {Professionals}},
	url = {https://www.jmir.org/2023/1/e50638},
	doi = {10.2196/50638},
	abstract = {Prompt engineering is a relatively new field of research that refers to the practice of designing, refining, and implementing prompts or instructions that guide the output of large language models (LLMs) to help in various tasks. With the emergence of LLMs, the most popular one being ChatGPT that has attracted the attention of over a 100 million users in only 2 months, artificial intelligence (AI), especially generative AI, has become accessible for the masses. This is an unprecedented paradigm shift not only because of the use of AI becoming more widespread but also due to the possible implications of LLMs in health care. As more patients and medical professionals use AI-based tools, LLMs being the most popular representatives of that group, it seems inevitable to address the challenge to improve this skill. This paper summarizes the current state of research about prompt engineering and, at the same time, aims at providing practical recommendations for the wide range of health care professionals to improve their interactions with LLMs.},
	language = {en},
	urldate = {2023-10-10},
	journal = {Journal of Medical Internet Research},
	author = {Meskó, Bertalan},
	month = oct,
	year = {2023},
	pages = {e50638},
}

@misc{villalobos_will_2022,
	title = {Will we run out of data? {An} analysis of the limits of scaling datasets in {Machine} {Learning}},
	shorttitle = {Will we run out of data?},
	url = {http://arxiv.org/abs/2211.04325},
	abstract = {We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
	month = oct,
	year = {2022},
	note = {arXiv:2211.04325 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/GP8SXMSW/Villalobos et al. - 2022 - Will we run out of data An analysis of the limits.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/ZECFM94Q/2211.html:text/html},
}

@misc{shumailov_curse_2023,
	title = {The {Curse} of {Recursion}: {Training} on {Generated} {Data} {Makes} {Models} {Forget}},
	shorttitle = {The {Curse} of {Recursion}},
	url = {http://arxiv.org/abs/2305.17493},
	abstract = {Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-\{n\} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.},
	urldate = {2023-10-12},
	publisher = {arXiv},
	author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
	month = may,
	year = {2023},
	note = {arXiv:2305.17493 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/ESWXBMYE/Shumailov et al. - 2023 - The Curse of Recursion Training on Generated Data.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/FJ94FYVK/2305.html:text/html},
}

@misc{wang_sam-octa_2023,
	title = {{SAM}-{OCTA}: {A} {Fine}-{Tuning} {Strategy} for {Applying} {Foundation} {Model} to {OCTA} {Image} {Segmentation} {Tasks}},
	shorttitle = {{SAM}-{OCTA}},
	url = {http://arxiv.org/abs/2309.11758},
	abstract = {In the analysis of optical coherence tomography angiography (OCTA) images, the operation of segmenting specific targets is necessary. Existing methods typically train on supervised datasets with limited samples (approximately a few hundred), which can lead to overfitting. To address this, the low-rank adaptation technique is adopted for foundation model fine-tuning and proposed corresponding prompt point generation strategies to process various segmentation tasks on OCTA datasets. This method is named SAM-OCTA and has been experimented on the publicly available OCTA-500 dataset. While achieving state-of-the-art performance metrics, this method accomplishes local vessel segmentation as well as effective artery-vein segmentation, which was not well-solved in previous works. The code is available at: https://github.com/ShellRedia/SAM-OCTA.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Wang, Chengliang and Chen, Xinrun and Ning, Haojian and Li, Shiying},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11758 [cs]},
	keywords = {eye, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/9FHV5ZWS/Wang et al. - 2023 - SAM-OCTA A Fine-Tuning Strategy for Applying Foun.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/YRRZE3FI/2309.html:text/html},
}

@article{kaufman_acoustic_2023,
	title = {Acoustic {Analysis} and {Prediction} of {Type} 2 {Diabetes} {Mellitus} {Using} {Smartphone}-{Recorded} {Voice} {Segments}},
	volume = {1},
	issn = {29497612},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2949761223000731},
	doi = {10.1016/j.mcpdig.2023.08.005},
	language = {en},
	number = {4},
	urldate = {2023-11-14},
	journal = {Mayo Clinic Proceedings: Digital Health},
	author = {Kaufman, Jaycee M. and Thommandram, Anirudh and Fossat, Yan},
	month = dec,
	year = {2023},
	keywords = {glucose, diabetes, smartphone, voice},
	pages = {534--544},
}

@misc{oren_proving_2023,
	title = {Proving {Test} {Set} {Contamination} in {Black} {Box} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.17623},
	abstract = {Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks. Going from speculation to proof of contamination is challenging, as the pretraining data used by proprietary models are often not publicly accessible. We show that it is possible to provide provable guarantees of test set contamination in language models without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples. We demonstrate that our procedure is sensitive enough to reliably prove test set contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets of only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Using our test, we audit five popular publicly accessible language models for test set contamination and find little evidence for pervasive contamination.},
	urldate = {2023-11-21},
	publisher = {arXiv},
	author = {Oren, Yonatan and Meister, Nicole and Chatterji, Niladri and Ladhak, Faisal and Hashimoto, Tatsunori B.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.17623 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/RJW44G4E/Oren et al. - 2023 - Proving Test Set Contamination in Black Box Langua.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/G9GFRYSG/2310.html:text/html},
}

@article{oh_organ_2023,
	title = {Organ aging signatures in the plasma proteome track health and disease},
	volume = {624},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06802-1},
	doi = {10.1038/s41586-023-06802-1},
	abstract = {Abstract
            
              Animal studies show aging varies between individuals as well as between organs within an individual
              1–4
              , but whether this is true in humans and its effect on age-related diseases is unknown. We utilized levels of human blood plasma proteins originating from specific organs to measure organ-specific aging differences in living individuals. Using machine learning models, we analysed aging in 11 major organs and estimated organ age reproducibly in five independent cohorts encompassing 5,676 adults across the human lifespan. We discovered nearly 20\% of the population show strongly accelerated age in one organ and 1.7\% are multi-organ agers. Accelerated organ aging confers 20–50\% higher mortality risk, and organ-specific diseases relate to faster aging of those organs. We find individuals with accelerated heart aging have a 250\% increased heart failure risk and accelerated brain and vascular aging predict Alzheimer’s disease (AD) progression independently from and as strongly as plasma pTau-181 (ref.
              5
              ), the current best blood-based biomarker for AD. Our models link vascular calcification, extracellular matrix alterations and synaptic protein shedding to early cognitive decline. We introduce a simple and interpretable method to study organ aging using plasma proteomics data, predicting diseases and aging effects.},
	language = {en},
	number = {7990},
	urldate = {2023-12-07},
	journal = {Nature},
	author = {Oh, Hamilton Se-Hwee and Rutledge, Jarod and Nachun, Daniel and Pálovics, Róbert and Abiose, Olamide and Moran-Losada, Patricia and Channappa, Divya and Urey, Deniz Yagmur and Kim, Kate and Sung, Yun Ju and Wang, Lihua and Timsina, Jigyasha and Western, Dan and Liu, Menghan and Kohlfeld, Pat and Budde, John and Wilson, Edward N. and Guen, Yann and Maurer, Taylor M. and Haney, Michael and Yang, Andrew C. and He, Zihuai and Greicius, Michael D. and Andreasson, Katrin I. and Sathyan, Sanish and Weiss, Erica F. and Milman, Sofiya and Barzilai, Nir and Cruchaga, Carlos and Wagner, Anthony D. and Mormino, Elizabeth and Lehallier, Benoit and Henderson, Victor W. and Longo, Frank M. and Montgomery, Stephen B. and Wyss-Coray, Tony},
	month = dec,
	year = {2023},
	pages = {164--172},
	file = {Full Text:/Users/sprague/Zotero/storage/2HFW3MPL/Oh et al. - 2023 - Organ aging signatures in the plasma proteome trac.pdf:application/pdf},
}

@misc{zhao_clip_2023,
	title = {{CLIP} in {Medical} {Imaging}: {A} {Comprehensive} {Survey}},
	shorttitle = {{CLIP} in {Medical} {Imaging}},
	url = {http://arxiv.org/abs/2312.07353},
	abstract = {Contrastive Language-Image Pre-training (CLIP), a straightforward yet effective pre-training paradigm, successfully introduces semantic-rich text supervision to vision models and has demonstrated promising results in various tasks due to its generalizability and interpretability. It has recently gained increasing interest in the medical imaging domain, either as a powerful pre-training paradigm for medical vision language alignment or a pre-trained key component for various clinical tasks. With the aim of facilitating a deeper understanding of this promising direction, this survey offers an in-depth exploration of the CLIP paradigm within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications. Our survey (1) starts with a brief introduction to the fundamentals of CLIP methodology. (2) Then, we investigate the adaptation of CLIP pre-training in the medical domain, focusing on how to optimize CLIP given characteristics of medical images and reports. (3) Furthermore, we explore the practical utilization of CLIP pre-trained models in various tasks, including classification, dense prediction, and cross-modal tasks. (4) Finally, we discuss existing limitations of CLIP in the context of medical imaging and propose forward-looking directions to address the demands of medical imaging domain. We expect that this comprehensive survey will provide researchers in the field of medical image analysis with a holistic understanding of the CLIP paradigm and its potential implications. The project page is available at https://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging, which will be regularly updated.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Zhao, Zihao and Liu, Yuxiao and Wu, Han and Li, Yonghao and Wang, Sheng and Teng, Lin and Liu, Disheng and Li, Xiang and Cui, Zhiming and Wang, Qian and Shen, Dinggang},
	month = dec,
	year = {2023},
	note = {arXiv:2312.07353 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/3RGDBF7T/Zhao et al. - 2023 - CLIP in Medical Imaging A Comprehensive Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/L86CITQI/2312.html:text/html},
}

@misc{milliere_philosophical_2024,
	title = {A {Philosophical} {Introduction} to {Language} {Models} -- {Part} {I}: {Continuity} {With} {Classic} {Debates}},
	shorttitle = {A {Philosophical} {Introduction} to {Language} {Models} -- {Part} {I}},
	url = {http://arxiv.org/abs/2401.03910},
	abstract = {Large language models like GPT-4 have achieved remarkable proficiency in a broad spectrum of language-based tasks, some of which are traditionally associated with hallmarks of human intelligence. This has prompted ongoing disagreements about the extent to which we can meaningfully ascribe any kind of linguistic or cognitive competence to language models. Such questions have deep philosophical roots, echoing longstanding debates about the status of artificial neural networks as cognitive models. This article -- the first part of two companion papers -- serves both as a primer on language models for philosophers, and as an opinionated survey of their significance in relation to classic debates in the philosophy cognitive science, artificial intelligence, and linguistics. We cover topics such as compositionality, language acquisition, semantic competence, grounding, world models, and the transmission of cultural knowledge. We argue that the success of language models challenges several long-held assumptions about artificial neural networks. However, we also highlight the need for further empirical investigation to better understand their internal mechanisms. This sets the stage for the companion paper (Part II), which turns to novel empirical methods for probing the inner workings of language models, and new philosophical questions prompted by their latest developments.},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Millière, Raphaël and Buckner, Cameron},
	month = jan,
	year = {2024},
	note = {arXiv:2401.03910 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, AI, Computer Science - Machine Learning, LLM, philosophy},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/BE4VNV75/Millière and Buckner - 2024 - A Philosophical Introduction to Language Models --.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/3FRQLYIU/2401.html:text/html},
}

@misc{bsharat_principled_2023,
	title = {Principled {Instructions} {Are} {All} {You} {Need} for {Questioning} {LLaMA}-1/2, {GPT}-3.5/4},
	url = {http://arxiv.org/abs/2312.16171},
	abstract = {This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work provides a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS.},
	urldate = {2024-01-12},
	publisher = {arXiv},
	author = {Bsharat, Sondos Mahmoud and Myrzakhan, Aidar and Shen, Zhiqiang},
	month = dec,
	year = {2023},
	note = {arXiv:2312.16171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/6MZKSZHM/Bsharat et al. - 2023 - Principled Instructions Are All You Need for Quest.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/IQG2A2FC/2312.html:text/html},
}

@article{tu_towards_2024,
	title = {Towards {Conversational} {Diagnostic} {AI}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2401.05654},
	doi = {10.48550/ARXIV.2401.05654},
	abstract = {At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.
 AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.},
	urldate = {2024-01-20},
	author = {Tu, Tao and Palepu, Anil and Schaekermann, Mike and Saab, Khaled and Freyberg, Jan and Tanno, Ryutaro and Wang, Amy and Li, Brenna and Amin, Mohamed and Tomasev, Nenad and Azizi, Shekoofeh and Singhal, Karan and Cheng, Yong and Hou, Le and Webson, Albert and Kulkarni, Kavita and Mahdavi, S Sara and Semturs, Christopher and Gottweis, Juraj and Barral, Joelle and Chou, Katherine and Corrado, Greg S and Matias, Yossi and Karthikesalingam, Alan and Natarajan, Vivek},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Artificial Intelligence (cs.AI), Machine Learning (cs.LG)},
}

@misc{kim_health-llm_2024,
	title = {Health-{LLM}: {Large} {Language} {Models} for {Health} {Prediction} via {Wearable} {Sensor} {Data}},
	shorttitle = {Health-{LLM}},
	url = {http://arxiv.org/abs/2401.06866},
	abstract = {Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and non-linguistic data is important. This paper investigates the capacity of LLMs to deliver multi-modal health predictions based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of eight state-of-the-art LLMs with diverse prompting and fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps, GLOBEM, AW\_FB, MIT-BIH \& MIMIC-III). Our experiments cover thirteen consumer health prediction tasks in mental health, activity, metabolic, sleep, and cardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable performance to larger models (GPT-3.5 and GPT-4), achieving the best performance in 5 out of 13 tasks. Ablation studies highlight the effectiveness of context enhancement strategies, and generalization capability of the fine-tuned models across training datasets and the size of training samples. Notably, we observe that our context enhancement can yield up to 23.8\% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance.},
	urldate = {2024-01-28},
	publisher = {arXiv},
	author = {Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won},
	month = jan,
	year = {2024},
	note = {arXiv:2401.06866 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/Q6MN249A/Kim et al. - 2024 - Health-LLM Large Language Models for Health Predi.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/2WBBYA6K/2401.html:text/html},
}

@article{hopkins_artificial_2023,
	title = {Artificial intelligence chatbots will revolutionize how cancer patients access information: {ChatGPT} represents a paradigm-shift},
	volume = {7},
	issn = {2515-5091},
	shorttitle = {Artificial intelligence chatbots will revolutionize how cancer patients access information},
	url = {https://academic.oup.com/jncics/article/doi/10.1093/jncics/pkad010/7049531},
	doi = {10.1093/jncics/pkad010},
	abstract = {Abstract
            On November 30, 2022, OpenAI enabled public access to ChatGPT, a next-generation artificial intelligence with a highly sophisticated ability to write, solve coding issues, and answer questions. This communication draws attention to the prospect that ChatGPT and its successors will become important virtual assistants to patients and health-care providers. In our assessments, ranging from answering basic fact-based questions to responding to complex clinical questions, ChatGPT demonstrated a remarkable ability to formulate interpretable responses, which appeared to minimize the likelihood of alarm compared with Google’s feature snippet. Arguably, the ChatGPT use case presents an urgent need for regulators and health-care professionals to be involved in developing standards for minimum quality and to raise patient awareness of current limitations of emerging artificial intelligence assistants. This commentary aims to raise awareness at the tipping point of a paradigm shift.},
	language = {en},
	number = {2},
	urldate = {2024-02-19},
	journal = {JNCI Cancer Spectrum},
	author = {Hopkins, Ashley M and Logan, Jessica M and Kichenadasse, Ganessan and Sorich, Michael J},
	month = mar,
	year = {2023},
	pages = {pkad010},
	file = {Full Text:/Users/sprague/Zotero/storage/T39R27NK/Hopkins et al. - 2023 - Artificial intelligence chatbots will revolutioniz.pdf:application/pdf},
}

@inproceedings{bender_dangers_2021,
	address = {Virtual Event Canada},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}? 🦜},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	language = {en},
	urldate = {2024-02-21},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
	file = {Full Text:/Users/sprague/Zotero/storage/2RT8XUQ3/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf:application/pdf},
}

@misc{qiu_towards_2024,
	title = {Towards {Building} {Multilingual} {Language} {Model} for {Medicine}},
	url = {http://arxiv.org/abs/2402.13963},
	abstract = {In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench. We will make the resources publicly available, including code, model weights, and datasets.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Qiu, Pengcheng and Wu, Chaoyi and Zhang, Xiaoman and Lin, Weixiong and Wang, Haicheng and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13963 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/6YRFMTQ7/Qiu et al. - 2024 - Towards Building Multilingual Language Model for M.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/REQ8C5QQ/2402.html:text/html},
}

@article{romera-paredes_mathematical_2024,
	title = {Mathematical discoveries from program search with large language models},
	volume = {625},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06924-6},
	doi = {10.1038/s41586-023-06924-6},
	abstract = {Abstract
            
              Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements
              1,2
              . This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches
              3
              . Applying FunSearch to a central problem in extremal combinatorics—the cap set problem—we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.},
	language = {en},
	number = {7995},
	urldate = {2024-02-23},
	journal = {Nature},
	author = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. Pawan and Dupont, Emilien and Ruiz, Francisco J. R. and Ellenberg, Jordan S. and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
	month = jan,
	year = {2024},
	pages = {468--475},
	file = {Full Text:/Users/sprague/Zotero/storage/NH9V7LV6/Romera-Paredes et al. - 2024 - Mathematical discoveries from program search with .pdf:application/pdf},
}

@misc{mcduff_towards_2023,
	title = {Towards {Accurate} {Differential} {Diagnosis} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2312.00164},
	abstract = {An accurate differential diagnosis (DDx) is a cornerstone of medical care, often reached through an iterative process of interpretation that combines clinical history, physical examination, investigations and procedures. Interactive interfaces powered by Large Language Models (LLMs) present new opportunities to both assist and automate aspects of this process. In this study, we introduce an LLM optimized for diagnostic reasoning, and evaluate its ability to generate a DDx alone or as an aid to clinicians. 20 clinicians evaluated 302 challenging, real-world medical cases sourced from the New England Journal of Medicine (NEJM) case reports. Each case report was read by two clinicians, who were randomized to one of two assistive conditions: either assistance from search engines and standard medical resources, or LLM assistance in addition to these tools. All clinicians provided a baseline, unassisted DDx prior to using the respective assistive tools. Our LLM for DDx exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1\% vs 33.6\%, [p = 0.04]). Comparing the two assisted study arms, the DDx quality score was higher for clinicians assisted by our LLM (top-10 accuracy 51.7\%) compared to clinicians without its assistance (36.1\%) (McNemar's Test: 45.7, p {\textless} 0.01) and clinicians with search (44.4\%) (4.75, p = 0.03). Further, clinicians assisted by our LLM arrived at more comprehensive differential lists than those without its assistance. Our study suggests that our LLM for DDx has potential to improve clinicians' diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients' access to specialist-level expertise.},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {McDuff, Daniel and Schaekermann, Mike and Tu, Tao and Palepu, Anil and Wang, Amy and Garrison, Jake and Singhal, Karan and Sharma, Yash and Azizi, Shekoofeh and Kulkarni, Kavita and Hou, Le and Cheng, Yong and Liu, Yun and Mahdavi, S. Sara and Prakash, Sushant and Pathak, Anupam and Semturs, Christopher and Patel, Shwetak and Webster, Dale R. and Dominowska, Ewa and Gottweis, Juraj and Barral, Joelle and Chou, Katherine and Corrado, Greg S. and Matias, Yossi and Sunshine, Jake and Karthikesalingam, Alan and Natarajan, Vivek},
	month = nov,
	year = {2023},
	note = {arXiv:2312.00164 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/7469BDJ8/McDuff et al. - 2023 - Towards Accurate Differential Diagnosis with Large.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/NCNZIW93/2312.html:text/html},
}

@misc{rozado_political_2024,
	title = {The {Political} {Preferences} of {LLMs}},
	url = {http://arxiv.org/abs/2402.01789},
	abstract = {We report here a comprehensive analysis about the political preferences embedded in Large Language Models (LLMs). Namely, we administer 11 political orientation tests, designed to identify the political preferences of the test taker, to 24 state-of-the-art conversational LLMs, both close and open source. The results indicate that when probed with questions/statements with political connotations most conversational LLMs tend to generate responses that are diagnosed by most political test instruments as manifesting preferences for left-of-center viewpoints. We note that this is not the case for base (i.e. foundation) models upon which LLMs optimized for conversation with humans are built. However, base models' suboptimal performance at coherently answering questions suggests caution when interpreting their classification by political orientation tests. Though not conclusive, our results provide preliminary evidence for the intriguing hypothesis that the embedding of political preferences into LLMs might be happening mostly post-pretraining. Namely, during the supervised fine-tuning (SFT) and/or Reinforcement Learning (RL) stages of the conversational LLMs training pipeline. We provide further support for this hypothesis by showing that LLMs are easily steerable into target locations of the political spectrum via SFT requiring only modest compute and custom data, illustrating the ability of SFT to imprint political preferences onto LLMs. As LLMs have started to displace more traditional information sources such as search engines or Wikipedia, the implications of political biases embedded in LLMs has important societal ramifications.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Rozado, David},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01789 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/2N9P9EFP/Rozado - 2024 - The Political Preferences of LLMs.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/FLPVYEBI/2402.html:text/html},
}

@article{huang_propaganda_2013,
	title = {Propaganda and {Signaling}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2325101},
	doi = {10.2139/ssrn.2325101},
	language = {en},
	urldate = {2024-02-26},
	journal = {SSRN Electronic Journal},
	author = {Huang, Haifeng and Li, Zhi},
	year = {2013},
}

@misc{jing_alphafold_2024,
	title = {{AlphaFold} {Meets} {Flow} {Matching} for {Generating} {Protein} {Ensembles}},
	url = {http://arxiv.org/abs/2402.04845},
	abstract = {The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at https://github.com/bjing2016/alphaflow.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Jing, Bowen and Berger, Bonnie and Jaakkola, Tommi},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04845 [cs, q-bio]},
	keywords = {Biotechnology, AI, Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/H5MPGTE6/Jing et al. - 2024 - AlphaFold Meets Flow Matching for Generating Prote.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/K45C85R5/2402.html:text/html},
}

@article{yu_evaluating_2023,
	title = {Evaluating progress in automatic chest {X}-ray radiology report generation},
	volume = {4},
	issn = {26663899},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666389923001575},
	doi = {10.1016/j.patter.2023.100802},
	language = {en},
	number = {9},
	urldate = {2024-02-29},
	journal = {Patterns},
	author = {Yu, Feiyang and Endo, Mark and Krishnan, Rayan and Pan, Ian and Tsai, Andy and Reis, Eduardo Pontes and Fonseca, Eduardo Kaiser Ururahy Nunes and Lee, Henrique Min Ho and Abad, Zahra Shakeri Hossein and Ng, Andrew Y. and Langlotz, Curtis P. and Venugopal, Vasantha Kumar and Rajpurkar, Pranav},
	month = sep,
	year = {2023},
	pages = {100802},
	file = {Full Text:/Users/sprague/Zotero/storage/EXCKKRTI/Yu et al. - 2023 - Evaluating progress in automatic chest X-ray radio.pdf:application/pdf},
}

@misc{sastry_computing_2024,
	title = {Computing {Power} and the {Governance} of {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2402.08797},
	abstract = {Computing power, or "compute," is crucial for the development and deployment of artificial intelligence (AI) capabilities. As a result, governments and companies have started to leverage compute as a means to govern AI. For example, governments are investing in domestic compute capacity, controlling the flow of compute to competing countries, and subsidizing compute access to certain sectors. However, these efforts only scratch the surface of how compute can be used to govern AI development and deployment. Relative to other key inputs to AI (data and algorithms), AI-relevant compute is a particularly effective point of intervention: it is detectable, excludable, and quantifiable, and is produced via an extremely concentrated supply chain. These characteristics, alongside the singular importance of compute for cutting-edge AI models, suggest that governing compute can contribute to achieving common policy objectives, such as ensuring the safety and beneficial use of AI. More precisely, policymakers could use compute to facilitate regulatory visibility of AI, allocate resources to promote beneficial outcomes, and enforce restrictions against irresponsible or malicious AI development and usage. However, while compute-based policies and technologies have the potential to assist in these areas, there is significant variation in their readiness for implementation. Some ideas are currently being piloted, while others are hindered by the need for fundamental research. Furthermore, naive or poorly scoped approaches to compute governance carry significant risks in areas like privacy, economic impacts, and centralization of power. We end by suggesting guardrails to minimize these risks from compute governance.},
	urldate = {2024-03-03},
	publisher = {arXiv},
	author = {Sastry, Girish and Heim, Lennart and Belfield, Haydn and Anderljung, Markus and Brundage, Miles and Hazell, Julian and O'Keefe, Cullen and Hadfield, Gillian K. and Ngo, Richard and Pilz, Konstantin and Gor, George and Bluemke, Emma and Shoker, Sarah and Egan, Janet and Trager, Robert F. and Avin, Shahar and Weller, Adrian and Bengio, Yoshua and Coyle, Diane},
	month = feb,
	year = {2024},
	note = {arXiv:2402.08797 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/WZJZZCZA/Sastry et al. - 2024 - Computing Power and the Governance of Artificial I.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/DJACPTPA/2402.html:text/html},
}

@article{zaleski_comprehensiveness_2024,
	title = {Comprehensiveness, {Accuracy}, and {Readability} of {Exercise} {Recommendations} {Provided} by an {AI}-{Based} {Chatbot}: {Mixed} {Methods} {Study}},
	volume = {10},
	issn = {2369-3762},
	shorttitle = {Comprehensiveness, {Accuracy}, and {Readability} of {Exercise} {Recommendations} {Provided} by an {AI}-{Based} {Chatbot}},
	url = {https://mededu.jmir.org/2024/1/e51308},
	doi = {10.2196/51308},
	abstract = {Background
              Regular physical activity is critical for health and disease prevention. Yet, health care providers and patients face barriers to implement evidence-based lifestyle recommendations. The potential to augment care with the increased availability of artificial intelligence (AI) technologies is limitless; however, the suitability of AI-generated exercise recommendations has yet to be explored.
            
            
              Objective
              The purpose of this study was to assess the comprehensiveness, accuracy, and readability of individualized exercise recommendations generated by a novel AI chatbot.
            
            
              Methods
              A coding scheme was developed to score AI-generated exercise recommendations across ten categories informed by gold-standard exercise recommendations, including (1) health condition–specific benefits of exercise, (2) exercise preparticipation health screening, (3) frequency, (4) intensity, (5) time, (6) type, (7) volume, (8) progression, (9) special considerations, and (10) references to the primary literature. The AI chatbot was prompted to provide individualized exercise recommendations for 26 clinical populations using an open-source application programming interface. Two independent reviewers coded AI-generated content for each category and calculated comprehensiveness (\%) and factual accuracy (\%) on a scale of 0\%-100\%. Readability was assessed using the Flesch-Kincaid formula. Qualitative analysis identified and categorized themes from AI-generated output.
            
            
              Results
              AI-generated exercise recommendations were 41.2\% (107/260) comprehensive and 90.7\% (146/161) accurate, with the majority (8/15, 53\%) of inaccuracy related to the need for exercise preparticipation medical clearance. Average readability level of AI-generated exercise recommendations was at the college level (mean 13.7, SD 1.7), with an average Flesch reading ease score of 31.1 (SD 7.7). Several recurring themes and observations of AI-generated output included concern for liability and safety, preference for aerobic exercise, and potential bias and direct discrimination against certain age-based populations and individuals with disabilities.
            
            
              Conclusions
              There were notable gaps in the comprehensiveness, accuracy, and readability of AI-generated exercise recommendations. Exercise and health care professionals should be aware of these limitations when using and endorsing AI-based technologies as a tool to support lifestyle change involving exercise.},
	language = {en},
	urldate = {2024-03-03},
	journal = {JMIR Medical Education},
	author = {Zaleski, Amanda L and Berkowsky, Rachel and Craig, Kelly Jean Thomas and Pescatello, Linda S},
	month = jan,
	year = {2024},
	keywords = {fitness, exercise},
	pages = {e51308},
	file = {Full Text:/Users/sprague/Zotero/storage/D9PSAIBH/Zaleski et al. - 2024 - Comprehensiveness, Accuracy, and Readability of Ex.pdf:application/pdf},
}

@misc{zhou_webarena_2023,
	title = {{WebArena}: {A} {Realistic} {Web} {Environment} for {Building} {Autonomous} {Agents}},
	shorttitle = {{WebArena}},
	url = {http://arxiv.org/abs/2307.13854},
	abstract = {With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41\%, significantly lower than the human performance of 78.24\%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.},
	urldate = {2024-03-04},
	publisher = {arXiv},
	author = {Zhou, Shuyan and Xu, Frank F. and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou, Tianyue and Bisk, Yonatan and Fried, Daniel and Alon, Uri and Neubig, Graham},
	month = oct,
	year = {2023},
	note = {arXiv:2307.13854 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/ND48T8XA/Zhou et al. - 2023 - WebArena A Realistic Web Environment for Building.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/L4PN962W/2307.html:text/html},
}

@article{dreyfus_why_2007,
	title = {Why {Heideggerian} {AI} {Failed} and {How} {Fixing} it {Would} {Require} {Making} it {More} {Heideggerian}},
	volume = {20},
	issn = {0951-5089, 1465-394X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09515080701239510},
	doi = {10.1080/09515080701239510},
	language = {en},
	number = {2},
	urldate = {2024-03-04},
	journal = {Philosophical Psychology},
	author = {Dreyfus, Hubert L.},
	month = apr,
	year = {2007},
	keywords = {philosophy, heidegger},
	pages = {247--268},
	file = {Dreyfus - 2007 - Why Heideggerian AI Failed and How Fixing it Would.pdf:/Users/sprague/Zotero/storage/WLYP74SR/Dreyfus - 2007 - Why Heideggerian AI Failed and How Fixing it Would.pdf:application/pdf},
}

@misc{rottger_political_2024,
	title = {Political {Compass} or {Spinning} {Arrow}? {Towards} {More} {Meaningful} {Evaluations} for {Values} and {Opinions} in {Large} {Language} {Models}},
	shorttitle = {Political {Compass} or {Spinning} {Arrow}?},
	url = {http://arxiv.org/abs/2402.16786},
	abstract = {Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Röttger, Paul and Hofmann, Valentin and Pyatkin, Valentina and Hinck, Musashi and Kirk, Hannah Rose and Schütze, Hinrich and Hovy, Dirk},
	month = feb,
	year = {2024},
	note = {arXiv:2402.16786 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/VBXTPH9B/Röttger et al. - 2024 - Political Compass or Spinning Arrow Towards More .pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/HQP9M8B5/2402.html:text/html},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/GEEYT9VR/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/ERGPEXTC/2201.html:text/html},
}

@misc{ai_yi_2024,
	title = {Yi: {Open} {Foundation} {Models} by 01.{AI}},
	shorttitle = {Yi},
	url = {http://arxiv.org/abs/2403.04652},
	abstract = {We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {AI, 01 and Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and Yu, Kaidong and Liu, Peng and Liu, Qiang and Yue, Shawn and Yang, Senbin and Yang, Shiming and Yu, Tao and Xie, Wen and Huang, Wenhao and Hu, Xiaohui and Ren, Xiaoyi and Niu, Xinyao and Nie, Pengcheng and Xu, Yuchi and Liu, Yudong and Wang, Yue and Cai, Yuxuan and Gu, Zhenyu and Liu, Zhiyuan and Dai, Zonghong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04652 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/LBSGHYGC/AI et al. - 2024 - Yi Open Foundation Models by 01.AI.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/K6RHN7UR/2403.html:text/html},
}

@misc{carlini_stealing_2024,
	title = {Stealing {Part} of a {Production} {Language} {Model}},
	url = {http://arxiv.org/abs/2403.06634},
	abstract = {We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under {\textbackslash}\$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under {\textbackslash}\$2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.},
	urldate = {2024-03-13},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Paleka, Daniel and Dvijotham, Krishnamurthy Dj and Steinke, Thomas and Hayase, Jonathan and Cooper, A. Feder and Lee, Katherine and Jagielski, Matthew and Nasr, Milad and Conmy, Arthur and Wallace, Eric and Rolnick, David and Tramèr, Florian},
	month = mar,
	year = {2024},
	note = {arXiv:2403.06634 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/7SJZV5DE/Carlini et al. - 2024 - Stealing Part of a Production Language Model.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/6UAXG46V/2403.html:text/html},
}

@misc{pei_deepfake_2024,
	title = {Deepfake {Generation} and {Detection}: {A} {Benchmark} and {Survey}},
	shorttitle = {Deepfake {Generation} and {Detection}},
	url = {http://arxiv.org/abs/2403.17881},
	abstract = {In addition to the advancements in deepfake generation, corresponding detection technologies need to continuously evolve to regulate the potential misuse of deepfakes, such as for privacy invasion and phishing attacks. This survey comprehensively reviews the latest developments in deepfake generation and detection, summarizing and analyzing the current state of the art in this rapidly evolving field. We first unify task definitions, comprehensively introduce datasets and metrics, and discuss the development of generation and detection technology frameworks. Then, we discuss the development of several related sub-fields and focus on researching four mainstream deepfake fields: popular face swap, face reenactment, talking face generation, and facial attribute editing, as well as foreign detection. Subsequently, we comprehensively benchmark representative methods on popular datasets for each field, fully evaluating the latest and influential works published in top conferences/journals. Finally, we analyze the challenges and future research directions of the discussed fields. We closely follow the latest developments in https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Pei, Gan and Zhang, Jiangning and Hu, Menghan and Zhai, Guangtao and Wang, Chengjie and Zhang, Zhenyu and Yang, Jian and Shen, Chunhua and Tao, Dacheng},
	month = mar,
	year = {2024},
	note = {arXiv:2403.17881 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/MU7MBREK/Pei et al. - 2024 - Deepfake Generation and Detection A Benchmark and.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/YTC6KW55/2403.html:text/html},
}

@misc{weiss_what_2024,
	title = {What {Was} {Your} {Prompt}? {A} {Remote} {Keylogging} {Attack} on {AI} {Assistants}},
	shorttitle = {What {Was} {Your} {Prompt}?},
	url = {http://arxiv.org/abs/2403.09751},
	abstract = {AI assistants are becoming an integral part of society, used for asking advice or help in personal and confidential issues. In this paper, we unveil a novel side-channel that can be used to read encrypted responses from AI Assistants over the web: the token-length side-channel. We found that many vendors, including OpenAI and Microsoft, have this side-channel. However, inferring the content of a response from a token-length sequence alone proves challenging. This is because tokens are akin to words, and responses can be several sentences long leading to millions of grammatically correct sentences. In this paper, we show how this can be overcome by (1) utilizing the power of a large language model (LLM) to translate these sequences, (2) providing the LLM with inter-sentence context to narrow the search space and (3) performing a known-plaintext attack by fine-tuning the model on the target model's writing style. Using these methods, we were able to accurately reconstruct 29{\textbackslash}\% of an AI assistant's responses and successfully infer the topic from 55{\textbackslash}\% of them. To demonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and Microsoft's Copilot on both browser and API traffic.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Weiss, Roy and Ayzenshteyn, Daniel and Amit, Guy and Mirsky, Yisroel},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09751 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/IFU7SAXC/Weiss et al. - 2024 - What Was Your Prompt A Remote Keylogging Attack o.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/XSQR7YZJ/2403.html:text/html},
}

@misc{wei_long-form_2024,
	title = {Long-form factuality in large language models},
	url = {http://arxiv.org/abs/2403.18802},
	abstract = {Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user's preferred response length (recall). Empirically, we demonstrate that LLM agents can achieve superhuman rating performance - on a set of {\textasciitilde}16k individual facts, SAFE agrees with crowdsourced human annotators 72\% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76\% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators. We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da and Du, Cosmo and Le, Quoc V.},
	month = mar,
	year = {2024},
	note = {arXiv:2403.18802 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/RXCCPZ4D/Wei et al. - 2024 - Long-form factuality in large language models.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/CP7JUD35/2403.html:text/html},
}

@misc{li_transformer-lite_2024,
	title = {Transformer-{Lite}: {High}-efficiency {Deployment} of {Large} {Language} {Models} on {Mobile} {Phone} {GPUs}},
	shorttitle = {Transformer-{Lite}},
	url = {http://arxiv.org/abs/2403.20041},
	abstract = {The Large Language Model (LLM) is widely employed for tasks such as intelligent assistants, text summarization, translation, and multi-modality on mobile phones. However, the current methods for on-device LLM deployment maintain slow inference speed, which causes poor user experience. To facilitate high-efficiency LLM deployment on device GPUs, we propose four optimization techniques: (a) a symbolic expression-based approach to support dynamic shape model inference; (b) operator optimizations and execution priority setting to enhance inference speed and reduce phone lagging; (c) an FP4 quantization method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based technique to eliminate the need for copying KV cache after LLM inference. Furthermore, we implement these methods in our mobile inference engine, Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We evaluated Transformer-Lite's performance using LLMs with varied architectures and parameters ranging from 2B to 14B. Specifically, we achieved prefill and decoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s and 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based FastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the prefill speed and 2{\textasciitilde}3x speedup for the decoding speed.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Li, Luchang and Qian, Sheng and Lu, Jie and Yuan, Lunxi and Wang, Rui and Xie, Qin},
	month = mar,
	year = {2024},
	note = {arXiv:2403.20041 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/VIVRVCEI/Li et al. - 2024 - Transformer-Lite High-efficiency Deployment of La.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/PMDZ2IHV/2403.html:text/html},
}

@article{jackson_exposure_2023,
	title = {Exposure to automation explains religious declines},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2304748120},
	doi = {10.1073/pnas.2304748120},
	abstract = {The global decline of religiosity represents one of the most significant societal shifts in recent history. After millennia of near-universal religious identification, the world is experiencing a regionally uneven trend toward secularization. We propose an explanation of this decline, which claims that automation—the development of robots and artificial intelligence (AI)—can partly explain modern religious declines. We build four unique datasets composed of more than 3 million individuals which show that robotics and AI exposure is linked to 21st-century religious declines across nations, metropolitan regions, and individual people. Key results hold controlling for other technological developments (e.g., electricity grid access and telecommunications development), socioeconomic indicators (e.g., wealth, residential mobility, and demographics), and factors implicated in previous theories of religious decline (e.g., individual choice norms). An experiment also supports our hypotheses. Our findings partly explain contemporary trends in religious decline and foreshadow where religiosity may wane in the future.},
	language = {en},
	number = {34},
	urldate = {2024-04-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Jackson, Joshua Conrad and Yam, Kai Chi and Tang, Pok Man and Sibley, Chris G. and Waytz, Adam},
	month = aug,
	year = {2023},
	pages = {e2304748120},
	file = {Full Text:/Users/sprague/Zotero/storage/DL84W8YH/Jackson et al. - 2023 - Exposure to automation explains religious declines.pdf:application/pdf},
}

@misc{udandarao_no_2024,
	title = {No "{Zero}-{Shot}" {Without} {Exponential} {Data}: {Pretraining} {Concept} {Frequency} {Determines} {Multimodal} {Model} {Performance}},
	shorttitle = {No "{Zero}-{Shot}" {Without} {Exponential} {Data}},
	url = {http://arxiv.org/abs/2404.04125},
	abstract = {Web-crawled pretraining datasets underlie the impressive "zero-shot" evaluation performance of multimodal models, such as CLIP for classification/retrieval and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of "zero-shot" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during "zero-shot" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets? We comprehensively investigate this question across 34 models and five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting "zero-shot" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream "zero-shot" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the "Let it Wag!" benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to "zero-shot" generalization capabilities under large-scale training paradigms remains to be found.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Udandarao, Vishaal and Prabhu, Ameya and Ghosh, Adhiraj and Sharma, Yash and Torr, Philip H. S. and Bibi, Adel and Albanie, Samuel and Bethge, Matthias},
	month = apr,
	year = {2024},
	note = {arXiv:2404.04125 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/7DG7XKNV/Udandarao et al. - 2024 - No Zero-Shot Without Exponential Data Pretraini.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/J4QX5FRS/2404.html:text/html},
}

@misc{he_foundation_2024,
	title = {Foundation {Model} for {Advancing} {Healthcare}: {Challenges}, {Opportunities}, and {Future} {Directions}},
	shorttitle = {Foundation {Model} for {Advancing} {Healthcare}},
	url = {http://arxiv.org/abs/2404.03264},
	abstract = {Foundation model, which is pre-trained on broad data and is able to adapt to a wide range of tasks, is advancing healthcare. It promotes the development of healthcare artificial intelligence (AI) models, breaking the contradiction between limited AI models and diverse healthcare practices. Much more widespread healthcare scenarios will benefit from the development of a healthcare foundation model (HFM), improving their advanced intelligent healthcare services. Despite the impending widespread deployment of HFMs, there is currently a lack of clear understanding about how they work in the healthcare field, their current challenges, and where they are headed in the future. To answer these questions, a comprehensive and deep survey of the challenges, opportunities, and future directions of HFMs is presented in this survey. It first conducted a comprehensive overview of the HFM including the methods, data, and applications for a quick grasp of the current progress. Then, it made an in-depth exploration of the challenges present in data, algorithms, and computing infrastructures for constructing and widespread application of foundation models in healthcare. This survey also identifies emerging and promising directions in this field for future development. We believe that this survey will enhance the community's comprehension of the current progress of HFM and serve as a valuable source of guidance for future development in this field. The latest HFM papers and related resources are maintained on our website: https://github.com/YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {He, Yuting and Huang, Fuxiang and Jiang, Xinrui and Nie, Yuxiang and Wang, Minghao and Wang, Jiguang and Chen, Hao},
	month = apr,
	year = {2024},
	note = {arXiv:2404.03264 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/U5F7Y77R/He et al. - 2024 - Foundation Model for Advancing Healthcare Challen.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/NAGPBTT7/2404.html:text/html},
}

@misc{abdin_phi-3_2024,
	title = {Phi-3 {Technical} {Report}: {A} {Highly} {Capable} {Language} {Model} {Locally} on {Your} {Phone}},
	shorttitle = {Phi-3 {Technical} {Report}},
	url = {http://arxiv.org/abs/2404.14219},
	abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69\% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75\% and 78\% on MMLU, and 8.7 and 8.9 on MT-bench).},
	urldate = {2024-04-24},
	publisher = {arXiv},
	author = {Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, Sébastien and Cai, Martin and Mendes, Caio César Teodoro and Chen, Weizhu and Chaudhary, Vishrav and Chopra, Parul and Del Giorno, Allie and de Rosa, Gustavo and Dixon, Matthew and Eldan, Ronen and Iter, Dan and Garg, Amit and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and Hewett, Russell J. and Huynh, Jamie and Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahoud and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Liang, Chen and Liu, Weishung and Lin, Eric and Lin, Zeqi and Madan, Piyush and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Rosset, Corby and Roy, Sambudha and Ruwase, Olatunji and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Song, Xia and Tanaka, Masahiro and Wang, Xin and Ward, Rachel and Wang, Guanhua and Witte, Philipp and Wyatt, Michael and Xu, Can and Xu, Jiahang and Yadav, Sonali and Yang, Fan and Yang, Ziyi and Yu, Donghan and Zhang, Chengruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yue and Zhang, Yunan and Zhou, Xiren},
	month = apr,
	year = {2024},
	note = {arXiv:2404.14219 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/ILWT5RUE/Abdin et al. - 2024 - Phi-3 Technical Report A Highly Capable Language .pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/ZZ5RY8C5/2404.html:text/html},
}

@misc{wendler_llamas_2024,
	title = {Do {Llamas} {Work} in {English}? {On} the {Latent} {Language} of {Multilingual} {Transformers}},
	shorttitle = {Do {Llamas} {Work} in {English}?},
	url = {http://arxiv.org/abs/2402.10588},
	abstract = {We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in "input space", "concept space", and "output space", respectively. Crucially, our evidence suggests that the abstract "concept space" lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Wendler, Chris and Veselovsky, Veniamin and Monea, Giovanni and West, Robert},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10588 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/FW8567BY/Wendler et al. - 2024 - Do Llamas Work in English On the Latent Language .pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/VM5FCR8C/2402.html:text/html},
}

@misc{huang_crispr-gpt_2024,
	title = {{CRISPR}-{GPT}: {An} {LLM} {Agent} for {Automated} {Design} of {Gene}-{Editing} {Experiments}},
	shorttitle = {{CRISPR}-{GPT}},
	url = {http://arxiv.org/abs/2404.18021},
	abstract = {The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agent's effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Huang, Kaixuan and Qu, Yuanhao and Cousins, Henry and Johnson, William A. and Yin, Di and Shah, Mihir and Zhou, Denny and Altman, Russ and Wang, Mengdi and Cong, Le},
	month = apr,
	year = {2024},
	note = {arXiv:2404.18021 [cs, q-bio]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/BIV9F295/Huang et al. - 2024 - CRISPR-GPT An LLM Agent for Automated Design of G.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/KNYVGR28/2404.html:text/html},
}

@misc{saab_capabilities_2024,
	title = {Capabilities of {Gemini} {Models} in {Medicine}},
	url = {http://arxiv.org/abs/2404.18416},
	abstract = {Excellence in a wide variety of medical applications poses considerable challenges for AI, requiring advanced reasoning, access to up-to-date medical knowledge and understanding of complex multimodal data. Gemini models, with strong general capabilities in multimodal and long-context reasoning, offer exciting possibilities in medicine. Building on these core strengths of Gemini, we introduce Med-Gemini, a family of highly capable multimodal models that are specialized in medicine with the ability to seamlessly use web search, and that can be efficiently tailored to novel modalities using custom encoders. We evaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art (SoTA) performance on 10 of them, and surpass the GPT-4 model family on every benchmark where a direct comparison is viable, often by a wide margin. On the popular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves SoTA performance of 91.1\% accuracy, using a novel uncertainty-guided search strategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU (health \& medicine), Med-Gemini improves over GPT-4V by an average relative margin of 44.5\%. We demonstrate the effectiveness of Med-Gemini's long-context capabilities through SoTA performance on a needle-in-a-haystack retrieval task from long de-identified health records and medical video question answering, surpassing prior bespoke methods using only in-context learning. Finally, Med-Gemini's performance suggests real-world utility by surpassing human experts on tasks such as medical text summarization, alongside demonstrations of promising potential for multimodal medical dialogue, medical research and education. Taken together, our results offer compelling evidence for Med-Gemini's potential, although further rigorous evaluation will be crucial before real-world deployment in this safety-critical domain.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Saab, Khaled and Tu, Tao and Weng, Wei-Hung and Tanno, Ryutaro and Stutz, David and Wulczyn, Ellery and Zhang, Fan and Strother, Tim and Park, Chunjong and Vedadi, Elahe and Chaves, Juanma Zambrano and Hu, Szu-Yeu and Schaekermann, Mike and Kamath, Aishwarya and Cheng, Yong and Barrett, David G. T. and Cheung, Cathy and Mustafa, Basil and Palepu, Anil and McDuff, Daniel and Hou, Le and Golany, Tomer and Liu, Luyang and Alayrac, Jean-baptiste and Houlsby, Neil and Tomasev, Nenad and Freyberg, Jan and Lau, Charles and Kemp, Jonas and Lai, Jeremy and Azizi, Shekoofeh and Kanada, Kimberly and Man, SiWai and Kulkarni, Kavita and Sun, Ruoxi and Shakeri, Siamak and He, Luheng and Caine, Ben and Webson, Albert and Latysheva, Natasha and Johnson, Melvin and Mansfield, Philip and Lu, Jian and Rivlin, Ehud and Anderson, Jesper and Green, Bradley and Wong, Renee and Krause, Jonathan and Shlens, Jonathon and Dominowska, Ewa and Eslami, S. M. Ali and Chou, Katherine and Cui, Claire and Vinyals, Oriol and Kavukcuoglu, Koray and Manyika, James and Dean, Jeff and Hassabis, Demis and Matias, Yossi and Webster, Dale and Barral, Joelle and Corrado, Greg and Semturs, Christopher and Mahdavi, S. Sara and Gottweis, Juraj and Karthikesalingam, Alan and Natarajan, Vivek},
	month = may,
	year = {2024},
	note = {arXiv:2404.18416 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/7WB8R7FW/Saab et al. - 2024 - Capabilities of Gemini Models in Medicine.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/Q8SNJKD4/2404.html:text/html},
}

@article{killock_ai_2020,
	title = {{AI} outperforms radiologists in mammographic screening},
	volume = {17},
	issn = {1759-4774, 1759-4782},
	url = {https://www.nature.com/articles/s41571-020-0329-7},
	doi = {10.1038/s41571-020-0329-7},
	language = {en},
	number = {3},
	urldate = {2024-05-08},
	journal = {Nature Reviews Clinical Oncology},
	author = {Killock, David},
	month = mar,
	year = {2020},
	pages = {134--134},
}

@article{hicks_chatgpt_2024,
	title = {{ChatGPT} is bullshit},
	volume = {26},
	issn = {1388-1957, 1572-8439},
	url = {https://link.springer.com/10.1007/s10676-024-09775-5},
	doi = {10.1007/s10676-024-09775-5},
	abstract = {Abstract
            
              Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as
              bullshit
              in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.},
	language = {en},
	number = {2},
	urldate = {2024-06-16},
	journal = {Ethics and Information Technology},
	author = {Hicks, Michael Townsen and Humphries, James and Slater, Joe},
	month = jun,
	year = {2024},
	pages = {38},
}

@misc{zheng_natural_2024,
	title = {{NATURAL} {PLAN}: {Benchmarking} {LLMs} on {Natural} {Language} {Planning}},
	shorttitle = {{NATURAL} {PLAN}},
	url = {http://arxiv.org/abs/2406.04520},
	abstract = {We introduce NATURAL PLAN, a realistic planning benchmark in natural language containing 3 key tasks: Trip Planning, Meeting Planning, and Calendar Scheduling. We focus our evaluation on the planning capabilities of LLMs with full information on the task, by providing outputs from tools such as Google Flights, Google Maps, and Google Calendar as contexts to the models. This eliminates the need for a tool-use environment for evaluating LLMs on Planning. We observe that NATURAL PLAN is a challenging benchmark for state of the art models. For example, in Trip Planning, GPT-4 and Gemini 1.5 Pro could only achieve 31.1\% and 34.8\% solve rate respectively. We find that model performance drops drastically as the complexity of the problem increases: all models perform below 5\% when there are 10 cities, highlighting a significant gap in planning in natural language for SoTA LLMs. We also conduct extensive ablation studies on NATURAL PLAN to further shed light on the (in)effectiveness of approaches such as self-correction, few-shot generalization, and in-context planning with long-contexts on improving LLM planning.},
	urldate = {2024-06-25},
	publisher = {arXiv},
	author = {Zheng, Huaixiu Steven and Mishra, Swaroop and Zhang, Hugh and Chen, Xinyun and Chen, Minmin and Nova, Azade and Hou, Le and Cheng, Heng-Tze and Le, Quoc V. and Chi, Ed H. and Zhou, Denny},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04520 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/MBFBQ4KQ/Zheng et al. - 2024 - NATURAL PLAN Benchmarking LLMs on Natural Languag.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/3RAZT5PQ/2406.html:text/html},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	urldate = {2024-07-02},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/7JQEMHDV/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/D4UL7UGC/2203.html:text/html},
}

@misc{liu_mobilellm_2024,
	title = {{MobileLLM}: {Optimizing} {Sub}-billion {Parameter} {Language} {Models} for {On}-{Device} {Use} {Cases}},
	shorttitle = {{MobileLLM}},
	url = {http://arxiv.org/abs/2402.14905},
	abstract = {This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7\%/4.3\% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight-sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7\%/0.8\% than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and Lai, Liangzhen and Chandra, Vikas},
	month = jun,
	year = {2024},
	note = {arXiv:2402.14905 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/GX52FSAM/Liu et al. - 2024 - MobileLLM Optimizing Sub-billion Parameter Langua.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/FU76U4SL/2402.html:text/html},
}

@article{sun_artificial_2023,
	title = {Artificial intelligence in psychiatry research, diagnosis, and therapy},
	volume = {87},
	issn = {18762018},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1876201823002617},
	doi = {10.1016/j.ajp.2023.103705},
	language = {en},
	urldate = {2024-07-09},
	journal = {Asian Journal of Psychiatry},
	author = {Sun, Jie and Dong, Qun-Xi and Wang, San-Wang and Zheng, Yong-Bo and Liu, Xiao-Xing and Lu, Tang-Sheng and Yuan, Kai and Shi, Jie and Hu, Bin and Lu, Lin and Han, Ying},
	month = sep,
	year = {2023},
	keywords = {psychology, AI},
	pages = {103705},
}

@misc{tian_spreadsheetllm_2024,
	title = {{SpreadsheetLLM}: {Encoding} {Spreadsheets} for {Large} {Language} {Models}},
	shorttitle = {{SpreadsheetLLM}},
	url = {http://arxiv.org/abs/2407.09025},
	abstract = {Spreadsheets, with their extensive two-dimensional grids, various layouts, and diverse formatting options, present notable challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6\% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, but achieves a state-of-the-art 78.9\% F1 score, surpassing the best existing models by 12.3\%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Tian, Yuzhang and Zhao, Jianbo and Dong, Haoyu and Xiong, Junyu and Xia, Shiyu and Zhou, Mengyu and Lin, Yun and Cambronero, José and He, Yeye and Han, Shi and Zhang, Dongmei},
	month = jul,
	year = {2024},
	note = {arXiv:2407.09025 [cs]},
	keywords = {Computer Science - Artificial Intelligence, LLM, spreadsheet},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/32H2EDV4/Tian et al. - 2024 - SpreadsheetLLM Encoding Spreadsheets for Large La.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/J9GUP8AS/2407.html:text/html},
}

@article{burtch_consequences_2024,
	title = {The consequences of generative {AI} for online knowledge communities},
	volume = {14},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-61221-0},
	doi = {10.1038/s41598-024-61221-0},
	abstract = {Abstract
            Generative artificial intelligence technologies, especially large language models (LLMs) like ChatGPT, are revolutionizing information acquisition and content production across a variety of domains. These technologies have a significant potential to impact participation and content production in online knowledge communities. We provide initial evidence of this, analyzing data from Stack Overflow and Reddit developer communities between October 2021 and March 2023, documenting ChatGPT’s influence on user activity in the former. We observe significant declines in both website visits and question volumes at Stack Overflow, particularly around topics where ChatGPT excels. By contrast, activity in Reddit communities shows no evidence of decline, suggesting the importance of social fabric as a buffer against the community-degrading effects of LLMs. Finally, the decline in participation on Stack Overflow is found to be concentrated among newer users, indicating that more junior, less socially embedded users are particularly likely to exit.},
	language = {en},
	number = {1},
	urldate = {2024-08-01},
	journal = {Scientific Reports},
	author = {Burtch, Gordon and Lee, Dokyun and Chen, Zhichen},
	month = may,
	year = {2024},
	keywords = {behavior, ai, llm, socialmedia},
	pages = {10413},
	file = {Full Text:/Users/sprague/Zotero/storage/L6PA7PV5/Burtch et al. - 2024 - The consequences of generative AI for online knowl.pdf:application/pdf},
}

@misc{erdil_explosive_2024,
	title = {Explosive growth from {AI} automation: {A} review of the arguments},
	shorttitle = {Explosive growth from {AI} automation},
	url = {http://arxiv.org/abs/2309.11690},
	abstract = {We examine whether substantial AI automation could accelerate global economic growth by about an order of magnitude, akin to the economic growth effects of the Industrial Revolution. We identify three primary drivers for such growth: 1) the scalability of an AI "labor force" restoring a regime of increasing returns to scale, 2) the rapid expansion of an AI labor force, and 3) a massive increase in output from rapid automation occurring over a brief period of time. Against this backdrop, we evaluate nine counterarguments, including regulatory hurdles, production bottlenecks, alignment issues, and the pace of automation. We tentatively assess these arguments, finding most are unlikely deciders. We conclude that explosive growth seems plausible with AI capable of broadly substituting for human labor, but high confidence in this claim seems currently unwarranted. Key questions remain about the intensity of regulatory responses to AI, physical bottlenecks in production, the economic value of superhuman abilities, and the rate at which AI automation could occur.},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Erdil, Ege and Besiroglu, Tamay},
	month = jul,
	year = {2024},
	note = {arXiv:2309.11690 [econ, q-fin]},
	keywords = {Economics - General Economics},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/RAUK5JSC/Erdil and Besiroglu - 2024 - Explosive growth from AI automation A review of t.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/PBBIS98I/2309.html:text/html},
}

@misc{lu_ai_2024,
	title = {The {AI} {Scientist}: {Towards} {Fully} {Automated} {Open}-{Ended} {Scientific} {Discovery}},
	shorttitle = {The {AI} {Scientist}},
	url = {http://arxiv.org/abs/2408.06292},
	abstract = {One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than \$15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06292 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/SYSH7DN8/Lu et al. - 2024 - The AI Scientist Towards Fully Automated Open-End.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/653W56QM/2408.html:text/html},
}

@misc{lutsker_glucose_2024,
	title = {From {Glucose} {Patterns} to {Health} {Outcomes}: {A} {Generalizable} {Foundation} {Model} for {Continuous} {Glucose} {Monitor} {Data} {Analysis}},
	shorttitle = {From {Glucose} {Patterns} to {Health} {Outcomes}},
	url = {http://arxiv.org/abs/2408.11876},
	abstract = {Recent advances in self-supervised learning enabled novel medical AI models, known as foundation models (FMs) that offer great potential for characterizing health from diverse biomedical data. Continuous glucose monitoring (CGM) provides rich, temporal data on glycemic patterns, but its full potential for predicting broader health outcomes remains underutilized. Here, we present GluFormer, a generative foundation model on biomedical temporal data based on a transformer architecture, and trained on over 10 million CGM measurements from 10,812 non-diabetic individuals. We tokenized the CGM training data and trained GluFormer using next token prediction in a generative, autoregressive manner. We demonstrate that GluFormer generalizes effectively to 15 different external datasets, including 4936 individuals across 5 different geographical regions, 6 different CGM devices, and several metabolic disorders, including normoglycemic, prediabetic, and diabetic populations, as well as those with gestational diabetes and obesity. GluFormer produces embeddings which outperform traditional CGM analysis tools, and achieves high Pearson correlations in predicting clinical parameters such as HbA1c, liver-related parameters, blood lipids, and sleep-related indices. Notably, GluFormer can also predict onset of future health outcomes even 4 years in advance. We also show that CGM embeddings from pre-intervention periods in Randomized Clinical Trials (RCTs) outperform other methods in predicting primary and secondary outcomes. When integrating dietary data into GluFormer, we show that the enhanced model can accurately generate CGM data based only on dietary intake data, simulate outcomes of dietary interventions, and predict individual responses to specific foods. Overall, we show that GluFormer accurately predicts health outcomes which generalize across different populations metabolic conditions.},
	urldate = {2024-08-23},
	publisher = {arXiv},
	author = {Lutsker, Guy and Sapir, Gal and Godneva, Anastasia and Shilo, Smadar and Greenfield, Jerry R. and Samocha-Bonet, Dorit and Mannor, Shie and Meirom, Eli and Chechik, Gal and Rossman, Hagai and Segal, Eran},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11876 [cs, q-bio]},
	keywords = {cgm, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods, llm},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/HKIKPLN7/Lutsker et al. - 2024 - From Glucose Patterns to Health Outcomes A Genera.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/QRSGVL44/2408.html:text/html},
}

@misc{kallini_mission_2024,
	title = {Mission: {Impossible} {Language} {Models}},
	shorttitle = {Mission},
	url = {http://arxiv.org/abs/2401.06416},
	abstract = {Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.},
	urldate = {2024-08-27},
	publisher = {arXiv},
	author = {Kallini, Julie and Papadimitriou, Isabel and Futrell, Richard and Mahowald, Kyle and Potts, Christopher},
	month = aug,
	year = {2024},
	note = {arXiv:2401.06416 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, llm, chomsky},
	file = {arXiv Fulltext PDF:/Users/sprague/Zotero/storage/VYFZWZLV/Kallini et al. - 2024 - Mission Impossible Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/sprague/Zotero/storage/KLHQT9XB/2401.html:text/html},
}

@article{yiu_transmission_2023,
	title = {Transmission {Versus} {Truth}, {Imitation} {Versus} {Innovation}: {What} {Children} {Can} {Do} {That} {Large} {Language} and {Language}-and-{Vision} {Models} {Cannot} ({Yet})},
	issn = {1745-6916, 1745-6924},
	shorttitle = {Transmission {Versus} {Truth}, {Imitation} {Versus} {Innovation}},
	url = {http://journals.sagepub.com/doi/10.1177/17456916231201401},
	doi = {10.1177/17456916231201401},
	abstract = {Much discussion about large language models and language-and-vision models has focused on whether these models are intelligent agents. We present an alternative perspective. First, we argue that these artificial intelligence (AI) models are cultural technologies that enhance cultural transmission and are efficient and powerful imitation engines. Second, we explore what AI models can tell us about imitation and innovation by testing whether they can be used to discover new tools and novel causal structures and contrasting their responses with those of human children. Our work serves as a first step in determining which particular representations and competences, as well as which kinds of knowledge or skill, can be derived from particular learning techniques and data. In particular, we explore which kinds of cognitive capacities can be enabled by statistical analysis of large-scale linguistic data. Critically, our findings suggest that machines may need more than large-scale language and image data to allow the kinds of innovation that a small child can produce.},
	language = {en},
	urldate = {2024-08-27},
	journal = {Perspectives on Psychological Science},
	author = {Yiu, Eunice and Kosoy, Eliza and Gopnik, Alison},
	month = oct,
	year = {2023},
	keywords = {psychology, language, llm},
	pages = {17456916231201401},
	file = {Full Text:/Users/sprague/Zotero/storage/RMGU5HND/Yiu et al. - 2023 - Transmission Versus Truth, Imitation Versus Innova.pdf:application/pdf},
}

@misc{epoch_ai_data_2024,
	title = {Data on {Large} {Language} {AI} {Models}},
	url = {https://epochai.org/data/large-scale-ai-models},
	urldate = {2024-08-29},
	author = {{Epoch AI}},
	year = {2024},
	keywords = {llm},
}

@misc{chang_speak_2023-1,
	title = {Speak, {Memory}: {An} {Archaeology} of {Books} {Known} to {ChatGPT}/{GPT}-4},
	shorttitle = {Speak, {Memory}},
	url = {http://arxiv.org/abs/2305.00118},
	abstract = {In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks. We argue that this supports a case for open models whose training data is known.},
	urldate = {2024-10-20},
	publisher = {arXiv},
	author = {Chang, Kent K. and Cramer, Mackenzie and Soni, Sandeep and Bamman, David},
	month = oct,
	year = {2023},
	note = {arXiv:2305.00118 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/sprague/Zotero/storage/G4UQUP9Z/Chang et al. - 2023 - Speak, Memory An Archaeology of Books Known to Ch.pdf:application/pdf;Snapshot:/Users/sprague/Zotero/storage/PTC3PBXW/2305.html:text/html},
}

@misc{hendy_how_2023,
	title = {How {Good} {Are} {GPT} {Models} at {Machine} {Translation}? {A} {Comprehensive} {Evaluation}},
	shorttitle = {How {Good} {Are} {GPT} {Models} at {Machine} {Translation}?},
	url = {http://arxiv.org/abs/2302.09210},
	abstract = {Generative Pre-trained Transformer (GPT) models have shown remarkable capabilities for natural language generation, but their performance for machine translation has not been thoroughly investigated. In this paper, we present a comprehensive evaluation of GPT models for machine translation, covering various aspects such as quality of different GPT models in comparison with state-of-the-art research and commercial systems, effect of prompting strategies, robustness towards domain shifts and document-level translation. We experiment with eighteen different translation directions involving high and low resource languages, as well as non English-centric translations, and evaluate the performance of three GPT models: ChatGPT, GPT3.5 (text-davinci-003), and text-davinci-002. Our results show that GPT models achieve very competitive translation quality for high resource languages, while having limited capabilities for low resource languages. We also show that hybrid approaches, which combine GPT models with other translation systems, can further enhance the translation quality. We perform comprehensive analysis and human evaluation to further understand the characteristics of GPT translations. We hope that our paper provides valuable insights for researchers and practitioners in the field and helps to better understand the potential and limitations of GPT models for translation.},
	urldate = {2024-11-05},
	publisher = {arXiv},
	author = {Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09210 [cs]},
	keywords = {linguistics, ai, Computer Science - Computation and Language, translation},
	file = {Preprint PDF:/Users/sprague/Zotero/storage/TYA8ELAN/Hendy et al. - 2023 - How Good Are GPT Models at Machine Translation A .pdf:application/pdf;Snapshot:/Users/sprague/Zotero/storage/4D73RLY5/2302.html:text/html},
}

@article{goh_large_2024,
	title = {Large {Language} {Model} {Influence} on {Diagnostic} {Reasoning}: {A} {Randomized} {Clinical} {Trial}},
	volume = {7},
	issn = {2574-3805},
	shorttitle = {Large {Language} {Model} {Influence} on {Diagnostic} {Reasoning}},
	url = {https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2825395},
	doi = {10.1001/jamanetworkopen.2024.40969},
	abstract = {Importance
              Large language models (LLMs) have shown promise in their performance on both multiple-choice and open-ended medical reasoning examinations, but it remains unknown whether the use of such tools improves physician diagnostic reasoning.
            
            
              Objective
              To assess the effect of an LLM on physicians’ diagnostic reasoning compared with conventional resources.
            
            
              Design, Setting, and Participants
              A single-blind randomized clinical trial was conducted from November 29 to December 29, 2023. Using remote video conferencing and in-person participation across multiple academic medical institutions, physicians with training in family medicine, internal medicine, or emergency medicine were recruited.
            
            
              Intervention
              Participants were randomized to either access the LLM in addition to conventional diagnostic resources or conventional resources only, stratified by career stage. Participants were allocated 60 minutes to review up to 6 clinical vignettes.
            
            
              Main Outcomes and Measures
              The primary outcome was performance on a standardized rubric of diagnostic performance based on differential diagnosis accuracy, appropriateness of supporting and opposing factors, and next diagnostic evaluation steps, validated and graded via blinded expert consensus. Secondary outcomes included time spent per case (in seconds) and final diagnosis accuracy. All analyses followed the intention-to-treat principle. A secondary exploratory analysis evaluated the standalone performance of the LLM by comparing the primary outcomes between the LLM alone group and the conventional resource group.
            
            
              Results
              
                Fifty physicians (26 attendings, 24 residents; median years in practice, 3 [IQR, 2-8]) participated virtually as well as at 1 in-person site. The median diagnostic reasoning score per case was 76\% (IQR, 66\%-87\%) for the LLM group and 74\% (IQR, 63\%-84\%) for the conventional resources-only group, with an adjusted difference of 2 percentage points (95\% CI, −4 to 8 percentage points;
                P
                 = .60). The median time spent per case for the LLM group was 519 (IQR, 371-668) seconds, compared with 565 (IQR, 456-788) seconds for the conventional resources group, with a time difference of −82 (95\% CI, −195 to 31;
                P
                 = .20) seconds.  The LLM alone scored 16 percentage points (95\% CI, 2-30 percentage points;
                P
                 = .03) higher than the conventional resources group.
              
            
            
              Conclusions and Relevance
              In this trial, the availability of an LLM to physicians as a diagnostic aid did not significantly improve clinical reasoning compared with conventional resources. The LLM alone demonstrated higher performance than both physician groups, indicating the need for technology and workforce development to realize the potential of physician-artificial intelligence collaboration in clinical practice.
            
            
              Trial Registration
              
                ClinicalTrials.gov Identifier:
                NCT06157944},
	language = {en},
	number = {10},
	urldate = {2024-11-17},
	journal = {JAMA Network Open},
	author = {Goh, Ethan and Gallo, Robert and Hom, Jason and Strong, Eric and Weng, Yingjie and Kerman, Hannah and Cool, Joséphine A. and Kanjee, Zahir and Parsons, Andrew S. and Ahuja, Neera and Horvitz, Eric and Yang, Daniel and Milstein, Arnold and Olson, Andrew P. J. and Rodman, Adam and Chen, Jonathan H.},
	month = oct,
	year = {2024},
	pages = {e2440969},
	file = {Full Text:/Users/sprague/Zotero/storage/QFGHQHZM/Goh et al. - 2024 - Large Language Model Influence on Diagnostic Reaso.pdf:application/pdf},
}

@article{zhao_foundation_2024,
	title = {A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-024-02499-w},
	doi = {10.1038/s41592-024-02499-w},
	language = {en},
	urldate = {2024-11-19},
	journal = {Nature Methods},
	author = {Zhao, Theodore and Gu, Yu and Yang, Jianwei and Usuyama, Naoto and Lee, Ho Hin and Kiblawi, Sid and Naumann, Tristan and Gao, Jianfeng and Crabtree, Angela and Abel, Jacob and Moung-Wen, Christine and Piening, Brian and Bifulco, Carlo and Wei, Mu and Poon, Hoifung and Wang, Sheng},
	month = nov,
	year = {2024},
	keywords = {ai, airdoc, medical, healthtech},
	file = {Zhao et al. - 2024 - A foundation model for joint segmentation, detecti.pdf:/Users/sprague/Zotero/storage/LQ6GC6MY/Zhao et al. - 2024 - A foundation model for joint segmentation, detecti.pdf:application/pdf},
}

@book{larson_myth_2021,
	address = {Cambridge, Massachusetts London, England},
	title = {The myth of artificial intelligence: why computers can't think the way we do},
	isbn = {978-0-674-27866-0 978-0-674-98351-9},
	shorttitle = {The myth of artificial intelligence},
	abstract = {Part One. The simplified world: The intelligence errors -- Turing at Bletchley -- The superintelligence error -- The singularity, then and now -- Natural language understanding -- AI as technological kitsch -- Simplifications and mysteries -- Part Two. The problem of inference: Don't calculate, analyze -- The puzzle of Peirce (and Peirce's Puzzle) -- Problems with deduction and induction -- Machine learning and big data -- Abductive inference -- Inference and language I -- Inference and language II -- Part Three. The future of the myth: Myths and heroes -- AI mythology invades neuroscience -- Neocortical theories of human intelligence -- The end of science?},
	language = {eng},
	publisher = {The Belknap Press of Harvard University Press},
	author = {Larson, Erik J.},
	year = {2021},
	file = {Table of Contents PDF:/Users/sprague/Zotero/storage/XHXKYJAC/Larson - 2021 - The myth of artificial intelligence why computers.pdf:application/pdf},
}

@book{narayanan_ai_2024,
	address = {Princeton Oxford},
	title = {{AI} snake oil: what artificial intelligence can do, what it can't, and how to tell the difference},
	isbn = {978-0-691-24913-1 978-0-691-24964-3},
	shorttitle = {{AI} snake oil},
	abstract = {"A trade book that argues that predictive AI is snake oil: it cannot and will never work. Artificial Intelligence is an umbrella term for a set of loosely related technologies. For instance, ChatGPT has little in common with algorithms that banks use to evaluate loan applicants. Both of these are referred to as AI, but in all of the salient ways - how they work, what they're used for and by whom, and how they fail - they couldn't be more different. Understanding the fundamental differences between AI technologies is critical for a technologically literate public to evaluate how AI is being used all around us. In this book, Arvind Narayanan and Sayash Kapoor explain the major strains of AI in use today: generative AI, predictive AI, and AI for content moderation. They show readers how to differentiate between them and, importantly, make a cogent argument for which types of AI can work well and which can never work, because of their inherent limitations. AI in this latter category, the authors argue, is AI snake oil: it does not and cannot work. More precisely, generative AI is imperfect but can be used for good once we learn how to apply it appropriately, whereas predictive AI can never work - in spite of the fact that it's being sold and marketed today in products - because we have never been able to accurately predict human behavior."--},
	language = {eng},
	publisher = {Princeton University Press},
	author = {Narayanan, Arvind and Kapoor, Sayash},
	year = {2024},
	file = {Table of Contents PDF:/Users/sprague/Zotero/storage/DI9L9M7A/Narayanan and Kapoor - 2024 - AI snake oil what artificial intelligence can do,.pdf:application/pdf},
}

@book{larson_myth_2021-1,
	address = {Cambridge, Massachusetts},
	title = {The myth of artificial intelligence: why computers can't think the way we do},
	isbn = {978-0-674-98351-9},
	shorttitle = {The myth of artificial intelligence},
	abstract = {"Futurists are certain that humanlike AI is on the horizon, but in fact engineers have no idea how to program human reasoning. AI reasons from statistical correlations across data sets, while common sense is based heavily on conjecture. Erik Larson argues that hyping existing methods will only hold us back from developing truly humanlike AI"--},
	publisher = {The Belknap Press of Harvard University Press},
	author = {Larson, Erik J.},
	year = {2021},
	keywords = {Artificial intelligence, Inference, Intellect, Logic, Natural language processing (Computer science), Neurosciences},
}

@misc{liu_monolith_2022,
	title = {Monolith: {Real} {Time} {Recommendation} {System} {With} {Collisionless} {Embedding} {Table}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Monolith},
	url = {https://arxiv.org/abs/2209.07663},
	doi = {10.48550/ARXIV.2209.07663},
	abstract = {Building a scalable and real-time recommendation system is vital for many businesses driven by time-sensitive customer feedback, such as short-videos ranking or online ads. Despite the ubiquitous adoption of production-scale deep learning frameworks like TensorFlow or PyTorch, these general-purpose frameworks fall short of business demands in recommendation scenarios for various reasons: on one hand, tweaking systems based on static parameters and dense computations for recommendation with dynamic and sparse features is detrimental to model quality; on the other hand, such frameworks are designed with batch-training stage and serving stage completely separated, preventing the model from interacting with customer feedback in real-time. These issues led us to reexamine traditional approaches and explore radically different design choices. In this paper, we present Monolith, a system tailored for online training. Our design has been driven by observations of our application workloads and production environment that reflects a marked departure from other recommendations systems. Our contributions are manifold: first, we crafted a collisionless embedding table with optimizations such as expirable embeddings and frequency filtering to reduce its memory footprint; second, we provide an production-ready online training architecture with high fault-tolerance; finally, we proved that system reliability could be traded-off for real-time learning. Monolith has successfully landed in the BytePlus Recommend product.},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Liu, Zhuoran and Zou, Leqi and Zou, Xuan and Wang, Caihua and Zhang, Biao and Tang, Da and Zhu, Bolin and Zhu, Yijie and Wu, Peng and Wang, Ke and Cheng, Youlong},
	year = {2022},
	note = {Version Number: 2},
	keywords = {FOS: Computer and information sciences, Information Retrieval (cs.IR)},
	file = {Full Text:/Users/sprague/Zotero/storage/WCX6I37G/Liu et al. - 2022 - Monolith Real Time Recommendation System With Col.pdf:application/pdf},
}
